{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Using the notebook will allow you to load the model once and use it as many times as you\n",
    "want. Also makes the code more resilient to errors, such as a bad path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INFO_TAG = \"persuasive_essays_paragraph\"\n",
    "PROCESS_TAG = \"granma_letters\"\n",
    "BASE_DATA = Path(\"data\")\n",
    "\n",
    "SOURCE_LANGUAGE = \"english\"\n",
    "TARGET_LANGUAGE = \"spanish\"\n",
    "\n",
    "# Corpus Projection\n",
    "CORPUS = BASE_DATA / \"corpus\" / INFO_TAG\n",
    "PROCESSED_CORPUS = BASE_DATA / \"parsed_to_conll\" / INFO_TAG\n",
    "SENTENCE_ALIGN = BASE_DATA / 'sentence_alignment' / INFO_TAG\n",
    "BIDIRECTIONAL_ALIGN = BASE_DATA / 'bidirectional_alignment' / INFO_TAG\n",
    "PROJECTION = BASE_DATA / 'projection' / INFO_TAG\n",
    "\n",
    "# Link Prediction\n",
    "TO_PROCESS = BASE_DATA / \"to_process\" / PROCESS_TAG\n",
    "SEGMENTER = BASE_DATA / \"segmenter_processed\" / INFO_TAG / PROCESS_TAG\n",
    "LINK_PREDICTION = BASE_DATA / 'link_prediction_processed' / INFO_TAG / PROCESS_TAG\n",
    "\n",
    "# Export to Brat\n",
    "BRAT = Path(\"brat\", \"data\", PROCESS_TAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.corpus_pipelines import full_corpus_processing_pipeline\n",
    "\n",
    "from aligner.aligner import AwesomeAlignAligner as Aligner\n",
    "from corpus_parser.unified_parser import UnifiedParser as Parser\n",
    "from projector.projector import CrossLingualAnnotationProjector as Projector\n",
    "from sentence_aligner.sentence_aligner import SentenceAligner\n",
    "from sentence_aligner.translator import GoogleDeepTranslator as Translator\n",
    "\n",
    "full_corpus_processing_pipeline(\n",
    "    corpus_dir=CORPUS,\n",
    "    standard_corpus_dest_dir=PROCESSED_CORPUS,\n",
    "    sentence_alignment_dest_dir=SENTENCE_ALIGN,\n",
    "    bidirectional_alignment_dest_dir=BIDIRECTIONAL_ALIGN,\n",
    "    projection_dest_dir=PROJECTION,\n",
    "    corpus_parser=Parser(),\n",
    "    sentence_aligner=SentenceAligner(Translator()),\n",
    "    aligner=Aligner(),\n",
    "    projector=Projector(),\n",
    "    source_language=SOURCE_LANGUAGE,\n",
    "    target_language=TARGET_LANGUAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.segmenter_pipelines import perform_full_inference_pipeline, perform_segmentation_pipeline, perform_link_prediction_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_amount 11\n",
      "char_amount 89\n",
      "word_amount 9007\n",
      "max_word_size 19\n",
      "max_seq_size 579\n",
      "\n",
      "word_to_index\n",
      "Length: 9009\n",
      "First 20: [('', 0), ('[UNK]', 1), (',', 2), ('de', 3), ('.', 4), ('la', 5), ('que', 6), ('los', 7), ('en', 8), ('y', 9), ('a', 10), ('el', 11), ('las', 12), ('para', 13), ('un', 14), ('es', 15), ('una', 16), ('m√°s', 17), ('no', 18), ('se', 19)]\n",
      "\n",
      "tag_to_index\n",
      "Length: 13\n",
      "First 20: [('', 0), ('[UNK]', 1), ('I-Premise', 2), ('O', 3), ('I-Claim', 4), ('I-MajorClaim', 5), ('E-Premise', 6), ('B-Premise', 7), ('E-Claim', 8), ('B-Claim', 9), ('E-MajorClaim', 10), ('B-MajorClaim', 11), ('S-MajorClaim', 12)]\n",
      "\n",
      "char_to_index\n",
      "Length: 91\n",
      "First 20: [('', 0), ('[UNK]', 1), (' ', 2), ('e', 3), ('a', 4), ('s', 5), ('o', 6), ('n', 7), ('r', 8), ('i', 9), ('d', 10), ('l', 11), ('t', 12), ('u', 13), ('c', 14), ('m', 15), ('p', 16), ('b', 17), (',', 18), ('g', 19)]\n",
      "tf.Tensor(\n",
      "[[b'I-Premise' b'I-Premise' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O'\n",
      "  b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'B-MajorClaim'\n",
      "  b'I-MajorClaim' b'I-MajorClaim' b'I-MajorClaim' b'I-MajorClaim'\n",
      "  b'I-MajorClaim' b'I-MajorClaim' b'I-MajorClaim' b'E-MajorClaim' b'O'\n",
      "  b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'B-Claim' b'I-Claim'\n",
      "  b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim'\n",
      "  b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'E-Premise' b'O' b'B-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'E-Premise' b'O' b'B-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'E-Premise' b'O' b'B-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'E-Premise' b'O' b'B-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'E-Premise' b'O' b'B-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'E-Premise' b'O' b'O' b'O' b'B-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'E-Premise' b'O' b'O' b'O' b'I-Premise' b'I-Premise' b'I-Claim'\n",
      "  b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'E-Claim' b'O'\n",
      "  b'O' b'O' b'O' b'O' b'I-Premise' b'I-Premise' b'I-Premise' b'E-Premise'\n",
      "  b'O' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'']], shape=(1, 579), dtype=string)\n",
      "[['B-Premise', 'E-Premise', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'E-MajorClaim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'O', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'O', 'O', 'B-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'E-Claim', 'O', 'O', 'O', 'O', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O']]\n"
     ]
    }
   ],
   "source": [
    "from segmenter.tf_segmenter import TensorflowArgumentSegmenter as Segmenter\n",
    "\n",
    "segmenter = Segmenter(INFO_TAG, TARGET_LANGUAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev relations 652\n",
      "dev source argumentative units 326\n",
      "dev target argumentative units 144\n",
      "test relations 1618\n",
      "test source argumentative units 809\n",
      "test target argumentative units 365\n",
      "train relations 5392\n",
      "train source argumentative units 2696\n",
      "train target argumentative units 1198\n",
      "Vocab size 8958\n",
      "Relation tags ['supports_Inverse', 'attacks', 'attacks_Inverse', 'supports']\n",
      "Proposition tags ['Claim', 'Premise', 'MajorClaim']\n",
      "max_size_prop 70\n",
      "max_amount_doc 20\n",
      "[('supports', 'Premise', 'Premise'), ('', 'Premise', 'Premise'), ('supports_Inverse', 'Premise', 'Premise')]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from link_prediction.tf_link_predictor import TensorflowLinkPredictor as LinkPredictor\n",
    "\n",
    "link_predictor = LinkPredictor(INFO_TAG, TARGET_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# perform_segmentation_pipeline(\n",
    "#     segmenter=segmenter,\n",
    "#     source_dir=TO_PROCESS,\n",
    "#     destination_dir=SEGMENTER,\n",
    "#     language=TARGET_LANGUAGE,\n",
    "# )\n",
    "\n",
    "perform_link_prediction_pipeline(\n",
    "    link_predictor=link_predictor,\n",
    "    source_dir=SEGMENTER,\n",
    "    destination_dir=LINK_PREDICTION,\n",
    "    source_language=TARGET_LANGUAGE\n",
    ")\n",
    "\n",
    "# perform_full_inference_pipeline(\n",
    "#     segmenter=segmenter,\n",
    "#     link_predictor=link_predictor,\n",
    "#     source_dir=TO_PROCESS,\n",
    "#     segmenter_destination_dir=SEGMENTER,\n",
    "#     destination_dir=LINK_PREDICTION,\n",
    "#     source_language=TARGET_LANGUAGE\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Brat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus_parser.brat_parser import BratParser\n",
    "from corpus_parser.conll_parser import ConllParser\n",
    "\n",
    "dataframes_dict = ConllParser(bioes=True).parse_dir(LINK_PREDICTION)\n",
    "\n",
    "BratParser().export_from_dataframes(BRAT, dataframes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
