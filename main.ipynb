{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Using the notebook will allow you to load the model once and use it as many times as you\n",
    "want. Also makes the code more resilient to errors, such as a bad path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INFO_TAG = \"persuasive_essays_paragraph_all_linked\"\n",
    "# INFO_TAG = \"cdcp\"\n",
    "# INFO_TAG = \"abstrct\"\n",
    "\n",
    "PROCESS_TAG = \"granma_letters\"\n",
    "BASE_DATA = Path(\"data\")\n",
    "\n",
    "SOURCE_LANGUAGE = \"english\"\n",
    "TARGET_LANGUAGE = \"spanish\"\n",
    "\n",
    "# Corpus Projection\n",
    "CORPUS = BASE_DATA / \"corpus\" / INFO_TAG\n",
    "PROCESSED_CORPUS = BASE_DATA / \"parsed_to_conll\" / INFO_TAG\n",
    "SENTENCE_ALIGN = BASE_DATA / 'sentence_alignment' / INFO_TAG\n",
    "BIDIRECTIONAL_ALIGN = BASE_DATA / 'bidirectional_alignment' / INFO_TAG\n",
    "PROJECTION = BASE_DATA / 'projection' / INFO_TAG\n",
    "\n",
    "# Link Prediction\n",
    "TO_PROCESS = BASE_DATA / \"to_process\" / PROCESS_TAG\n",
    "SEGMENTER = BASE_DATA / \"segmenter_processed\" / INFO_TAG / PROCESS_TAG\n",
    "LINK_PREDICTION = BASE_DATA / 'link_prediction_processed' / INFO_TAG / PROCESS_TAG\n",
    "\n",
    "# Export to Brat\n",
    "BRAT = Path(\"brat\", \"data\", PROCESS_TAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deep_translator\n",
    "\n",
    "from pipelines.corpus_pipelines import full_corpus_processing_pipeline, make_alignemnts_pipeline\n",
    "\n",
    "from aligner.aligner import AwesomeAlignAligner as Aligner\n",
    "from corpus_parser.unified_parser import UnifiedParser as Parser\n",
    "from projector.projector import CrossLingualAnnotationProjector as Projector\n",
    "from sentence_aligner.sentence_aligner import SentenceAligner\n",
    "from sentence_aligner.translator import GoogleDeepTranslator as Translator\n",
    "from data_augmentation.translation_augmentation import TranslateDataAugmentator as DataAugmentator\n",
    "\n",
    "for split in ['dev', 'test', 'train']:\n",
    "    \n",
    "    print(split)\n",
    "    print()\n",
    "    \n",
    "    full_corpus_processing_pipeline(\n",
    "        corpus_dir=CORPUS / split,\n",
    "        standard_corpus_dest_dir=PROCESSED_CORPUS / split,\n",
    "        sentence_alignment_dest_dir=SENTENCE_ALIGN / split,\n",
    "        bidirectional_alignment_dest_dir=BIDIRECTIONAL_ALIGN / split,\n",
    "        projection_dest_dir=PROJECTION / split,\n",
    "        corpus_parser=Parser(),\n",
    "        sentence_aligner=SentenceAligner(Translator()),\n",
    "        aligner=Aligner(),\n",
    "        projector=Projector(),\n",
    "        data_augmentator=DataAugmentator(),\n",
    "        source_language=SOURCE_LANGUAGE,\n",
    "        target_language=TARGET_LANGUAGE,\n",
    "        middle_language=TARGET_LANGUAGE,\n",
    "        use_spacy=True,\n",
    "    )\n",
    "    \n",
    "#     make_alignemnts_pipeline(\n",
    "#         standard_corpus_dir=PROCESSED_CORPUS/ split,\n",
    "#         sentence_alignment_dest_dir=SENTENCE_ALIGN/ split,\n",
    "#         bidirectional_alignment_dest_dir=BIDIRECTIONAL_ALIGN/ split,\n",
    "#         projection_dest_dir=PROJECTION/ split,\n",
    "#         sentence_aligner=SentenceAligner(Translator()),\n",
    "#         aligner=Aligner(),\n",
    "#         projector=Projector(),\n",
    "#         data_augmentator=DataAugmentator(),\n",
    "#         source_language=SOURCE_LANGUAGE,\n",
    "#         target_language=TARGET_LANGUAGE,\n",
    "#         use_spacy=True,\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.segmenter_pipelines import perform_full_inference_pipeline, perform_segmentation_pipeline, perform_link_prediction_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_amount 12\n",
      "tag_amount 11\n",
      "char_amount 90\n",
      "word_amount 9280\n",
      "max_word_size 19\n",
      "max_seq_size 579\n",
      "\n",
      "word_to_index\n",
      "Length: 9282\n",
      "First 20: [('', 0), ('[UNK]', 1), ('noun', 2), ('det', 3), ('verb', 4), ('adp', 5), ('.', 6), ('adj', 7), ('conj', 8), (',', 9), ('de', 10), ('adv', 11), ('pron', 12), ('la', 13), ('que', 14), ('en', 15), ('los', 16), ('y', 17), ('a', 18), ('el', 19)]\n",
      "\n",
      "tag_to_index\n",
      "Length: 13\n",
      "First 20: [('', 0), ('[UNK]', 1), ('I-Premise', 2), ('O', 3), ('I-Claim', 4), ('I-MajorClaim', 5), ('E-Premise', 6), ('B-Premise', 7), ('E-Claim', 8), ('B-Claim', 9), ('E-MajorClaim', 10), ('B-MajorClaim', 11), ('S-MajorClaim', 12)]\n",
      "\n",
      "char_to_index\n",
      "Length: 92\n",
      "First 20: [('', 0), ('[UNK]', 1), (' ', 2), ('e', 3), ('N', 4), ('a', 5), ('D', 6), ('s', 7), ('o', 8), ('n', 9), ('O', 10), ('E', 11), ('r', 12), ('i', 13), ('A', 14), ('U', 15), ('d', 16), ('l', 17), ('t', 18), ('V', 19)]\n",
      "\n",
      "pos_to_index\n",
      "Length: 14\n",
      "First 20: [('', 0), ('[UNK]', 1), ('NOUN', 2), ('DET', 3), ('VERB', 4), ('ADP', 5), ('.', 6), ('ADJ', 7), ('CONJ', 8), ('ADV', 9), ('PRON', 10), ('NUM', 11), ('PRT', 12), ('X', 13)]\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'E-MajorClaim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'E-Claim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'E-Claim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'O', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Premise', 'E-Premise', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'E-Claim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'E-Claim', 'O', 'B-Claim', 'E-Claim', 'O', 'O', 'O', 'O', 'O', 'B-MajorClaim', 'E-MajorClaim', 'O', 'B-Claim', 'E-Claim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "from segmenter.tf_segmenter import TensorflowArgumentSegmenter as Segmenter\n",
    "\n",
    "segmenter = Segmenter(INFO_TAG, TARGET_LANGUAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from link_prediction.tf_link_predictor import TensorflowLinkPredictor as LinkPredictor\n",
    "\n",
    "link_predictor = LinkPredictor(INFO_TAG, TARGET_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Only segmentation\n",
    "\n",
    "perform_segmentation_pipeline(\n",
    "    segmenter=segmenter,\n",
    "    source_dir=TO_PROCESS,\n",
    "    destination_dir=SEGMENTER,\n",
    "    language=TARGET_LANGUAGE,\n",
    ")\n",
    "\n",
    "# Only link prediction (Segmentation must be done first)\n",
    "\n",
    "# perform_link_prediction_pipeline(\n",
    "#     link_predictor=link_predictor,\n",
    "#     source_dir=SEGMENTER,\n",
    "#     destination_dir=LINK_PREDICTION,\n",
    "#     source_language=TARGET_LANGUAGE\n",
    "# )\n",
    "\n",
    "# Both processes Segmentation and Link prediction\n",
    "\n",
    "# perform_full_inference_pipeline(\n",
    "#     segmenter=segmenter,\n",
    "#     link_predictor=link_predictor,\n",
    "#     source_dir=TO_PROCESS,\n",
    "#     segmenter_destination_dir=SEGMENTER,\n",
    "#     destination_dir=LINK_PREDICTION,\n",
    "#     source_language=TARGET_LANGUAGE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev relations 2236\n",
      "dev source argumentative units 912\n",
      "dev target argumentative units 418\n",
      "test relations 5580\n",
      "test source argumentative units 2226\n",
      "test target argumentative units 1036\n",
      "train relations 18874\n",
      "train source argumentative units 7537\n",
      "train target argumentative units 3460\n",
      "Vocab size 10305\n",
      "Relation tags ['attacks_Inverse', 'supports', 'supports_Inverse', 'attacks']\n",
      "Proposition tags ['MajorClaim', 'Claim', 'Premise']\n",
      "max_size_prop 70\n",
      "max_amount_doc 26\n",
      "[('supports', 'Premise', 'Claim'), ('supports', 'Premise', 'Claim'), ('supports', 'Premise', 'Claim')]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from link_prediction.tf_link_predictor import TensorflowLinkPredictor as LinkPredictor\n",
    "\n",
    "link_predictor = LinkPredictor(INFO_TAG, TARGET_LANGUAGE)\n",
    "\n",
    "perform_link_prediction_pipeline(\n",
    "    link_predictor=link_predictor,\n",
    "    source_dir=SEGMENTER,\n",
    "    destination_dir=LINK_PREDICTION,\n",
    "    source_language=TARGET_LANGUAGE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Brat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus_parser.brat_parser import BratParser\n",
    "from corpus_parser.conll_parser import ConllParser\n",
    "\n",
    "dataframes_dict = ConllParser(bioes=True).parse_dir(LINK_PREDICTION)\n",
    "\n",
    "BratParser().export_from_dataframes(BRAT, dataframes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
