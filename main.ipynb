{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Using the notebook will allow you to load the model once and use it as many times as you\n",
    "want. Also makes the code more resilient to errors, such as a bad path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INFO_TAG = \"persuasive_essays_paragraph\"\n",
    "PROCESS_TAG = \"granma_letters\"\n",
    "BASE_DATA = Path(\"data\")\n",
    "\n",
    "SOURCE_LANGUAGE = \"english\"\n",
    "TARGET_LANGUAGE = \"spanish\"\n",
    "\n",
    "# Corpus Projection\n",
    "CORPUS = BASE_DATA / \"corpus\" / INFO_TAG\n",
    "PROCESSED_CORPUS = BASE_DATA / \"parsed_to_conll\" / INFO_TAG\n",
    "SENTENCE_ALIGN = BASE_DATA / 'sentence_alignment' / INFO_TAG\n",
    "BIDIRECTIONAL_ALIGN = BASE_DATA / 'bidirectional_alignment' / INFO_TAG\n",
    "PROJECTION = BASE_DATA / 'projection' / INFO_TAG\n",
    "\n",
    "# Link Prediction\n",
    "TO_PROCESS = BASE_DATA / \"to_process\" / PROCESS_TAG\n",
    "SEGMENTER = BASE_DATA / \"segmenter_processed\" / INFO_TAG / PROCESS_TAG\n",
    "LINK_PREDICTION = BASE_DATA / 'link_prediction_processed' / INFO_TAG / PROCESS_TAG\n",
    "\n",
    "# Export to Brat\n",
    "BRAT = Path(\"brat\", \"data\", PROCESS_TAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Line 2 file essay024.ann. Match not found: A1\tStance T2 Against\n",
      "WARNING:root:Line 1 file essay012.ann. Match not found: A2\tStance T3 For\n",
      "WARNING:root:Line 3 file essay054.ann. Match not found: A1\tStance T3 Against\n",
      "WARNING:root:Line 4 file essay024.ann. Match not found: A2\tStance T3 For\n",
      "WARNING:root:Line 2 file essay045.ann. Match not found: A1\tStance T2 Against\n",
      "WARNING:root:Line 2 file essay102.ann. Match not found: A1\tStance T2 Against\n",
      "WARNING:root:Line 2 file essay190.ann. Match not found: A1\tStance T2 For\n",
      "WARNING:root:Line 3 file essay288.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 3 file essay177.ann. Match not found: A1\tStance T3 Against\n",
      "WARNING:root:Line 9 file essay012.ann. Match not found: A1\tStance T1 For\n",
      "WARNING:root:Line 2 file essay230.ann. Match not found: A1\tStance T2 For\n",
      "WARNING:root:Line 5 file essay210.ann. Match not found: A2\tStance T5 For\n",
      "WARNING:root:Line 5 file essay045.ann. Match not found: A2\tStance T4 For\n",
      "WARNING:root:Line 2 file essay362.ann. Match not found: A1\tStance T2 Against\n",
      "WARNING:root:Line 3 file essay346.ann. Match not found: A1\tStance T3 Against\n",
      "WARNING:root:Line 3 file essay283.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 3 file essay329.ann. Match not found: A1\tStance T3 Against\n",
      "WARNING:root:Line 3 file essay377.ann. Match not found: A2\tStance T3 Against\n",
      "WARNING:root:Line 10 file essay054.ann. Match not found: A3\tStance T8 For\n",
      "WARNING:root:Line 8 file essay102.ann. Match not found: A2\tStance T7 Against\n",
      "WARNING:root:Line 4 file essay309.ann. Match not found: A1\tStance T4 For\n",
      "WARNING:root:Line 12 file essay012.ann. Match not found: A4\tStance T8 For\n",
      "WARNING:root:Line 8 file essay118.ann. Match not found: A1\tStance T8 Against\n",
      "WARNING:root:Line 13 file essay288.ann. Match not found: A3\tStance T11 For\n",
      "WARNING:root:Line 12 file essay024.ann. Match not found: A4\tStance T9 For\n",
      "WARNING:root:Line 3 file essay388.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 8 file essay210.ann. Match not found: A3\tStance T6 For\n",
      "WARNING:root:Line 11 file essay190.ann. Match not found: A2\tStance T8 For\n",
      "WARNING:root:Line 5 file essay283.ann. Match not found: A2\tStance T4 For\n",
      "WARNING:root:Line 7 file essay362.ann. Match not found: A2\tStance T6 For\n",
      "WARNING:root:Line 16 file essay177.ann. Match not found: A3\tStance T11 For\n",
      "WARNING:root:Line 5 file essay329.ann. Match not found: A2\tStance T4 For\n",
      "WARNING:root:Line 15 file essay045.ann. Match not found: A3\tStance T9 For\n",
      "WARNING:root:Line 12 file essay054.ann. Match not found: A4\tStance T9 For\n",
      "WARNING:root:Line 14 file essay230.ann. Match not found: A2\tStance T8 For\n",
      "WARNING:root:Line 8 file essay377.ann. Match not found: A1\tStance T6 For\n",
      "WARNING:root:Line 9 file essay346.ann. Match not found: A3\tStance T7 For\n",
      "WARNING:root:Line 11 file essay102.ann. Match not found: A3\tStance T9 Against\n",
      "WARNING:root:Line 15 file essay288.ann. Match not found: A4\tStance T12 For\n",
      "WARNING:root:Line 5 file essay388.ann. Match not found: A2\tStance T4 Against\n",
      "WARNING:root:Line 12 file essay309.ann. Match not found: A3\tStance T10 For\n",
      "WARNING:root:Line 19 file essay024.ann. Match not found: A3\tStance T6 For\n",
      "WARNING:root:Line 7 file essay283.ann. Match not found: A3\tStance T5 For\n",
      "WARNING:root:Line 16 file essay190.ann. Match not found: A3\tStance T9 Against\n",
      "WARNING:root:Line 20 file essay118.ann. Match not found: A2\tStance T14 For\n",
      "WARNING:root:Line 22 file essay012.ann. Match not found: A3\tStance T14 For\n",
      "WARNING:root:Line 21 file essay177.ann. Match not found: A2\tStance T12 For\n",
      "WARNING:root:Line 11 file essay362.ann. Match not found: A3\tStance T9 For\n",
      "WARNING:root:Line 11 file essay329.ann. Match not found: A3\tStance T8 For\n",
      "WARNING:root:Line 20 file essay045.ann. Match not found: A5\tStance T12 For\n",
      "WARNING:root:Line 22 file essay230.ann. Match not found: A3\tStance T12 For\n",
      "WARNING:root:Line 15 file essay377.ann. Match not found: A3\tStance T8 For\n",
      "WARNING:root:Line 19 file essay054.ann. Match not found: A5\tStance T5 Against\n",
      "WARNING:root:Line 15 file essay346.ann. Match not found: A2\tStance T9 For\n",
      "WARNING:root:Line 2 file essay225.ann. Match not found: A1\tStance T2 For\n",
      "WARNING:root:Line 11 file essay388.ann. Match not found: A3\tStance T7 For\n",
      "WARNING:root:Line 23 file essay288.ann. Match not found: A2\tStance T13 For\n",
      "WARNING:root:Line 21 file essay102.ann. Match not found: A4\tStance T12 For\n",
      "WARNING:root:Line 23 file essay309.ann. Match not found: A4\tStance T16 For\n",
      "WARNING:root:Line 29 file essay177.ann. Match not found: A4\tStance T16 For\n",
      "WARNING:root:Line 18 file essay377.ann. Match not found: A4\tStance T10 For\n",
      "WARNING:root:Line 18 file essay362.ann. Match not found: A4\tStance T10 For\n",
      "WARNING:root:Line 14 file essay283.ann. Match not found: A4\tStance T11 For\n",
      "WARNING:root:Line 19 file essay329.ann. Match not found: A4\tStance T11 For\n",
      "WARNING:root:Line 4 file essay225.ann. Match not found: A2\tStance T3 For\n",
      "WARNING:root:Line 17 file essay388.ann. Match not found: A4\tStance T10 For\n",
      "WARNING:root:Line 29 file essay102.ann. Match not found: A5\tStance T16 For\n",
      "WARNING:root:Line 2 file essay264.ann. Match not found: A1\tStance T2 For\n",
      "WARNING:root:Line 35 file essay288.ann. Match not found: A6\tStance T19 For\n",
      "WARNING:root:Line 3 file essay209.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 2 file essay080.ann. Match not found: A1\tStance T2 Against\n",
      "WARNING:root:Line 3 file essay043.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 3 file essay171.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 2 file essay053.ann. Match not found: A1\tStance T2 For\n",
      "WARNING:root:Line 4 file essay264.ann. Match not found: A2\tStance T1 For\n",
      "WARNING:root:Line 27 file essay283.ann. Match not found: A5\tStance T18 For\n",
      "WARNING:root:Line 14 file essay225.ann. Match not found: A3\tStance T8 For\n",
      "WARNING:root:Line 2 file essay381.ann. Match not found: A1\tStance T2 Against\n",
      "WARNING:root:Line 6 file essay209.ann. Match not found: A2\tStance T5 For\n",
      "WARNING:root:Line 1 file essay351.ann. Match not found: A1\tStance T1 Against\n",
      "WARNING:root:Line 4 file essay080.ann. Match not found: A2\tStance T3 Against\n",
      "WARNING:root:Line 4 file essay053.ann. Match not found: A2\tStance T3 For\n",
      "WARNING:root:Line 9 file essay264.ann. Match not found: A3\tStance T7 For\n",
      "WARNING:root:Line 2 file essay186.ann. Match not found: A1\tStance T2 Against\n",
      "WARNING:root:Line 3 file essay372.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 2 file essay326.ann. Match not found: A1\tStance T2 For\n",
      "WARNING:root:Line 11 file essay043.ann. Match not found: A2\tStance T7 For\n",
      "WARNING:root:Line 3 file essay400.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 5 file essay381.ann. Match not found: A2\tStance T4 For\n",
      "WARNING:root:Line 9 file essay171.ann. Match not found: A2\tStance T6 For\n",
      "WARNING:root:Line 11 file essay209.ann. Match not found: A3\tStance T7 For\n",
      "WARNING:root:Line 5 file essay351.ann. Match not found: A2\tStance T4 For\n",
      "WARNING:root:Line 2 file essay337.ann. Match not found: A1\tStance T2 For\n",
      "WARNING:root:Line 8 file essay297.ann. Match not found: A1\tStance T7 For\n",
      "WARNING:root:Line 38 file essay283.ann. Match not found: A6\tStance T8 For\n",
      "WARNING:root:Line 5 file essay372.ann. Match not found: A2\tStance T4 For\n",
      "WARNING:root:Line 5 file essay326.ann. Match not found: A2\tStance T4 For\n",
      "WARNING:root:Line 26 file essay225.ann. Match not found: A4\tStance T15 For\n",
      "WARNING:root:Line 12 file essay053.ann. Match not found: A3\tStance T7 For\n",
      "WARNING:root:Line 16 file essay080.ann. Match not found: A3\tStance T9 For\n",
      "WARNING:root:Line 19 file essay264.ann. Match not found: A5\tStance T12 For\n",
      "WARNING:root:Line 3 file essay106.ann. Match not found: A1\tStance T3 For\n",
      "WARNING:root:Line 12 file essay186.ann. Match not found: A3\tStance T9 For\n",
      "WARNING:root:Line 21 file essay043.ann. Match not found: A3\tStance T12 For\n",
      "WARNING:root:Line 14 file essay400.ann. Match not found: A3\tStance T10 For\n",
      "WARNING:root:Line 15 file essay381.ann. Match not found: A3\tStance T9 For\n",
      "WARNING:root:Line 8 file essay351.ann. Match not found: A3\tStance T6 Against\n",
      "WARNING:root:Line 13 file essay297.ann. Match not found: A2\tStance T8 For\n",
      "WARNING:root:Line 13 file essay372.ann. Match not found: A3\tStance T9 For\n",
      "WARNING:root:Line 13 file essay337.ann. Match not found: A2\tStance T9 For\n",
      "WARNING:root:Line 15 file essay326.ann. Match not found: A4\tStance T12 Against\n",
      "WARNING:root:Line 31 file essay225.ann. Match not found: A5\tStance T17 For\n",
      "WARNING:root:Line 16 file essay053.ann. Match not found: A4\tStance T10 For\n",
      "WARNING:root:Line 15 file essay186.ann. Match not found: A4\tStance T10 For\n",
      "WARNING:root:Line 11 file essay106.ann. Match not found: A2\tStance T7 For\n",
      "WARNING:root:Line 4 file essay284.ann. Match not found: A2\tStance T4 For\n",
      "WARNING:root:Line 13 file essay351.ann. Match not found: A4\tStance T8 For\n",
      "WARNING:root:Line 21 file essay297.ann. Match not found: A3\tStance T12 For\n",
      "WARNING:root:Line 19 file essay337.ann. Match not found: A3\tStance T11 Against\n",
      "WARNING:root:Line 27 file essay326.ann. Match not found: A3\tStance T17 Against\n",
      "WARNING:root:Line 2 file essay281.ann. Match not found: A1\tStance T2 For\n",
      "WARNING:root:Line 23 file essay186.ann. Match not found: A5\tStance T14 For\n",
      "WARNING:root:Line 24 file essay337.ann. Match not found: A4\tStance T13 Against\n",
      "WARNING:root:Line 29 file essay297.ann. Match not found: A4\tStance T16 Against\n",
      "WARNING:root:Line 33 file essay326.ann. Match not found: A5\tStance T20 For\n",
      "WARNING:root:Line 32 file essay284.ann. Match not found: A1\tStance T10 For\n",
      "WARNING:root:Line 22 file essay281.ann. Match not found: A3\tStance T14 For\n",
      "WARNING:root:Line 39 file essay284.ann. Match not found: A4\tStance T22 For\n",
      "WARNING:root:Line 32 file essay281.ann. Match not found: A2\tStance T17 For\n",
      "WARNING:root:Line 45 file essay281.ann. Match not found: A4\tStance T24 For\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "ename": "NotValidPayload",
     "evalue": "None --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotValidPayload\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_augmentation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslation_augmentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TranslateDataAugmentator \u001b[38;5;28;01mas\u001b[39;00m DataAugmentator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mfull_corpus_processing_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCORPUS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstandard_corpus_dest_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROCESSED_CORPUS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence_alignment_dest_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSENTENCE_ALIGN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbidirectional_alignment_dest_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBIDIRECTIONAL_ALIGN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprojection_dest_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROJECTION\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence_aligner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSentenceAligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43maligner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprojector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mProjector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_augmentator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataAugmentator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSOURCE_LANGUAGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_LANGUAGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmiddle_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_LANGUAGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_spacy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/argument-mining/pipelines/corpus_pipelines.py:82\u001b[0m, in \u001b[0;36mfull_corpus_processing_pipeline\u001b[0;34m(corpus_dir, standard_corpus_dest_dir, sentence_alignment_dest_dir, bidirectional_alignment_dest_dir, projection_dest_dir, corpus_parser, sentence_aligner, aligner, projector, data_augmentator, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         middle_language \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_language\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspanish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Augmentator was given without a middle_language key arg. Defaulting to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmiddle_language\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mdata_augmentator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_data_by_translation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstandard_corpus_dest_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence_aligner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43maligner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmiddle_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmiddle_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m parse_corpus_pipeline(corpus_dir, standard_corpus_dest_dir, corpus_parser, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     95\u001b[0m make_alignemnts_pipeline(standard_corpus_dest_dir, sentence_alignment_dest_dir, bidirectional_alignment_dest_dir, \n\u001b[1;32m     96\u001b[0m                 projection_dest_dir, sentence_aligner, aligner, projector, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspaces/argument-mining/data_augmentation/translation_augmentation.py:74\u001b[0m, in \u001b[0;36mTranslateDataAugmentator.augment_data_by_translation\u001b[0;34m(self, corpus_path, destination_path, parser, sentence_aligner, aligner, projector, source_language, middle_language, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_language\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_language\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mfull_corpus_processing_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstandard_corpus_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence_alignment_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbidirectional_alignment_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprojection_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence_aligner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43maligner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmiddle_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmiddle_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     destination_path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m projection_dir\u001b[38;5;241m.\u001b[39miterdir():\n",
      "File \u001b[0;32m/workspaces/argument-mining/pipelines/corpus_pipelines.py:95\u001b[0m, in \u001b[0;36mfull_corpus_processing_pipeline\u001b[0;34m(corpus_dir, standard_corpus_dest_dir, sentence_alignment_dest_dir, bidirectional_alignment_dest_dir, projection_dest_dir, corpus_parser, sentence_aligner, aligner, projector, data_augmentator, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     data_augmentator\u001b[38;5;241m.\u001b[39maugment_data_by_translation(\n\u001b[1;32m     83\u001b[0m         corpus_dir,\n\u001b[1;32m     84\u001b[0m         standard_corpus_dest_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     93\u001b[0m parse_corpus_pipeline(corpus_dir, standard_corpus_dest_dir, corpus_parser, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 95\u001b[0m \u001b[43mmake_alignemnts_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandard_corpus_dest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_alignment_dest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbidirectional_alignment_dest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprojection_dest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_aligner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/argument-mining/pipelines/corpus_pipelines.py:54\u001b[0m, in \u001b[0;36mmake_alignemnts_pipeline\u001b[0;34m(standard_corpus_dir, sentence_alignment_dest_dir, bidirectional_alignment_dest_dir, projection_dest_dir, sentence_aligner, aligner, projector, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03mMake the sentences alignment with `aligner` saving the results in `sentence_alignment_dest_dir`.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mMake the bidirectional alignment with `aligner` saving the results in `bidirectional_alignment_dest_dir`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mprojector: Projector used to make the projections\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating sentence alignment \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, standard_corpus_dir)\n\u001b[0;32m---> 54\u001b[0m \u001b[43msentence_aligner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_alignment_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandard_corpus_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_alignment_dest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating bidirectional alignment \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sentence_alignment_dest_dir)\n\u001b[1;32m     57\u001b[0m aligner\u001b[38;5;241m.\u001b[39mbidirectional_align_dir(sentence_alignment_dest_dir, bidirectional_alignment_dest_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspaces/argument-mining/sentence_aligner/sentence_aligner.py:42\u001b[0m, in \u001b[0;36mSentenceAligner.sentence_alignment_dir\u001b[0;34m(self, corpus_address, sentence_dest, sentences_splitted, source_language, target_language, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, annotated_tags_info \u001b[38;5;129;01min\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     41\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tok[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m annotated_tags_info)\n\u001b[0;32m---> 42\u001b[0m     sentence_sentence_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_sentence_sentence_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentences_splitted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentences_splitted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_word_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_language\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_language\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     dest_file \u001b[38;5;241m=\u001b[39m sentence_dest \u001b[38;5;241m/\u001b[39m (Path(key)\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.align\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     dest_file\u001b[38;5;241m.\u001b[39mwrite_text(sentence_sentence_text)\n",
      "File \u001b[0;32m/workspaces/argument-mining/sentence_aligner/sentence_aligner.py:103\u001b[0m, in \u001b[0;36mSentenceAligner.make_sentence_sentence_text\u001b[0;34m(self, text, sentences_splitted, source_word_tokenizer, target_word_tokenizer, sent_tokenizer, source_language, target_language, separator, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_worker) \u001b[38;5;28;01mas\u001b[39;00m exe:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_worker):\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# futures.append(exe.submit(batch_work, i))\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[43mbatch_work\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m wait(futures)\n\u001b[1;32m    105\u001b[0m exceptions \u001b[38;5;241m=\u001b[39m [future\u001b[38;5;241m.\u001b[39mexception() \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures \u001b[38;5;28;01mif\u001b[39;00m future\u001b[38;5;241m.\u001b[39mexception()]\n",
      "File \u001b[0;32m/workspaces/argument-mining/sentence_aligner/sentence_aligner.py:90\u001b[0m, in \u001b[0;36mSentenceAligner.make_sentence_sentence_text.<locals>.batch_work\u001b[0;34m(slice)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences[batch\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mslice\u001b[39m:batch\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Result is the sentence's tokens separated by spaces\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     source_sentence_with_spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(source_word_tokenizer(sentence, language\u001b[38;5;241m=\u001b[39msource_language))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 90\u001b[0m     target_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_sentence_with_spaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_language\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_language\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target_sentence \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m         target_sentence_with_spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(target_word_tokenizer(target_sentence, language\u001b[38;5;241m=\u001b[39mtarget_language))\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/workspaces/argument-mining/sentence_aligner/translator.py:204\u001b[0m, in \u001b[0;36mBaseDeepTranslator.translate\u001b[0;34m(self, source_sentence, source_language, target_language, middle_language, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_translated\n\u001b[1;32m    203\u001b[0m translator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_translator(source_language, target_language)\n\u001b[0;32m--> 204\u001b[0m target_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__save_cache(source_sentence, target_sentence, source_language, target_language)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_sentence\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/deep_translator/google.py:55\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    function to translate a text\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    @param text: desired text to translate\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    @return: str: translated text\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_input_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     56\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_same_source_target() \u001b[38;5;129;01mor\u001b[39;00m is_empty(text):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/deep_translator/validate.py:18\u001b[0m, in \u001b[0;36mis_input_valid\u001b[0;34m(text, min_chars, max_chars)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mvalidate the target text to translate\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m@param min_chars: min characters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m@return: bool\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m text\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotValidPayload(text)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m min_chars \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m<\u001b[39m max_chars:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotValidLength(text, min_chars, max_chars)\n",
      "\u001b[0;31mNotValidPayload\u001b[0m: None --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated"
     ]
    }
   ],
   "source": [
    "from pipelines.corpus_pipelines import full_corpus_processing_pipeline\n",
    "\n",
    "from aligner.aligner import AwesomeAlignAligner as Aligner\n",
    "from corpus_parser.unified_parser import UnifiedParser as Parser\n",
    "from projector.projector import CrossLingualAnnotationProjector as Projector\n",
    "from sentence_aligner.sentence_aligner import SentenceAligner\n",
    "from sentence_aligner.translator import GoogleDeepTranslator as Translator\n",
    "from data_augmentation.translation_augmentation import TranslateDataAugmentator as DataAugmentator\n",
    "\n",
    "for split in ['dev', 'test', 'train']:\n",
    "\n",
    "    full_corpus_processing_pipeline(\n",
    "        corpus_dir=CORPUS / split,\n",
    "        standard_corpus_dest_dir=PROCESSED_CORPUS / split,\n",
    "        sentence_alignment_dest_dir=SENTENCE_ALIGN / split,\n",
    "        bidirectional_alignment_dest_dir=BIDIRECTIONAL_ALIGN / split,\n",
    "        projection_dest_dir=PROJECTION / split,\n",
    "        corpus_parser=Parser(),\n",
    "        sentence_aligner=SentenceAligner(Translator()),\n",
    "        aligner=Aligner(),\n",
    "        projector=Projector(),\n",
    "        data_augmentator=DataAugmentator(),\n",
    "        source_language=SOURCE_LANGUAGE,\n",
    "        target_language=TARGET_LANGUAGE,\n",
    "        middle_language=TARGET_LANGUAGE,\n",
    "        # use_spacy=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.segmenter_pipelines import perform_full_inference_pipeline, perform_segmentation_pipeline, perform_link_prediction_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_amount 11\n",
      "char_amount 89\n",
      "word_amount 9007\n",
      "max_word_size 19\n",
      "max_seq_size 579\n",
      "\n",
      "word_to_index\n",
      "Length: 9009\n",
      "First 20: [('', 0), ('[UNK]', 1), (',', 2), ('de', 3), ('.', 4), ('la', 5), ('que', 6), ('los', 7), ('en', 8), ('y', 9), ('a', 10), ('el', 11), ('las', 12), ('para', 13), ('un', 14), ('es', 15), ('una', 16), ('ms', 17), ('no', 18), ('se', 19)]\n",
      "\n",
      "tag_to_index\n",
      "Length: 13\n",
      "First 20: [('', 0), ('[UNK]', 1), ('I-Premise', 2), ('O', 3), ('I-Claim', 4), ('I-MajorClaim', 5), ('E-Premise', 6), ('B-Premise', 7), ('E-Claim', 8), ('B-Claim', 9), ('E-MajorClaim', 10), ('B-MajorClaim', 11), ('S-MajorClaim', 12)]\n",
      "\n",
      "char_to_index\n",
      "Length: 91\n",
      "First 20: [('', 0), ('[UNK]', 1), (' ', 2), ('e', 3), ('a', 4), ('s', 5), ('o', 6), ('n', 7), ('r', 8), ('i', 9), ('d', 10), ('l', 11), ('t', 12), ('u', 13), ('c', 14), ('m', 15), ('p', 16), ('b', 17), (',', 18), ('g', 19)]\n",
      "tf.Tensor(\n",
      "[[b'I-Premise' b'I-Premise' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O'\n",
      "  b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'B-MajorClaim'\n",
      "  b'I-MajorClaim' b'I-MajorClaim' b'I-MajorClaim' b'I-MajorClaim'\n",
      "  b'I-MajorClaim' b'I-MajorClaim' b'I-MajorClaim' b'E-MajorClaim' b'O'\n",
      "  b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'O' b'B-Claim' b'I-Claim'\n",
      "  b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim'\n",
      "  b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'E-Premise' b'O' b'B-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'E-Premise' b'O' b'B-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'E-Premise' b'O' b'B-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'E-Premise' b'O' b'B-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'E-Premise' b'O' b'B-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'E-Premise' b'O' b'O' b'O' b'B-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise' b'I-Premise'\n",
      "  b'E-Premise' b'O' b'O' b'O' b'I-Premise' b'I-Premise' b'I-Claim'\n",
      "  b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'I-Claim' b'E-Claim' b'O'\n",
      "  b'O' b'O' b'O' b'O' b'I-Premise' b'I-Premise' b'I-Premise' b'E-Premise'\n",
      "  b'O' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      "  b'' b'' b'' b'' b'']], shape=(1, 579), dtype=string)\n",
      "[['B-Premise', 'E-Premise', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'I-MajorClaim', 'E-MajorClaim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'O', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O', 'O', 'O', 'B-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'I-Claim', 'E-Claim', 'O', 'O', 'O', 'O', 'O', 'B-Premise', 'I-Premise', 'I-Premise', 'E-Premise', 'O']]\n"
     ]
    }
   ],
   "source": [
    "from segmenter.tf_segmenter import TensorflowArgumentSegmenter as Segmenter\n",
    "\n",
    "segmenter = Segmenter(INFO_TAG, TARGET_LANGUAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev relations 652\n",
      "dev source argumentative units 326\n",
      "dev target argumentative units 144\n",
      "test relations 1618\n",
      "test source argumentative units 809\n",
      "test target argumentative units 365\n",
      "train relations 5392\n",
      "train source argumentative units 2696\n",
      "train target argumentative units 1198\n",
      "Vocab size 8958\n",
      "Relation tags ['supports_Inverse', 'attacks', 'attacks_Inverse', 'supports']\n",
      "Proposition tags ['Claim', 'Premise', 'MajorClaim']\n",
      "max_size_prop 70\n",
      "max_amount_doc 20\n",
      "[('supports', 'Premise', 'Premise'), ('', 'Premise', 'Premise'), ('supports_Inverse', 'Premise', 'Premise')]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from link_prediction.tf_link_predictor import TensorflowLinkPredictor as LinkPredictor\n",
    "\n",
    "link_predictor = LinkPredictor(INFO_TAG, TARGET_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# perform_segmentation_pipeline(\n",
    "#     segmenter=segmenter,\n",
    "#     source_dir=TO_PROCESS,\n",
    "#     destination_dir=SEGMENTER,\n",
    "#     language=TARGET_LANGUAGE,\n",
    "# )\n",
    "\n",
    "perform_link_prediction_pipeline(\n",
    "    link_predictor=link_predictor,\n",
    "    source_dir=SEGMENTER,\n",
    "    destination_dir=LINK_PREDICTION,\n",
    "    source_language=TARGET_LANGUAGE\n",
    ")\n",
    "\n",
    "# perform_full_inference_pipeline(\n",
    "#     segmenter=segmenter,\n",
    "#     link_predictor=link_predictor,\n",
    "#     source_dir=TO_PROCESS,\n",
    "#     segmenter_destination_dir=SEGMENTER,\n",
    "#     destination_dir=LINK_PREDICTION,\n",
    "#     source_language=TARGET_LANGUAGE\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Brat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus_parser.brat_parser import BratParser\n",
    "from corpus_parser.conll_parser import ConllParser\n",
    "\n",
    "dataframes_dict = ConllParser(bioes=True).parse_dir(LINK_PREDICTION)\n",
    "\n",
    "BratParser().export_from_dataframes(BRAT, dataframes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}