{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "try:\n",
    "    BASE_PATH = str(Path(__file__, \"..\", \"..\", \"..\").resolve())\n",
    "    import sys\n",
    "except NameError:\n",
    "    import sys\n",
    "    BASE_PATH = str(Path(\"..\", \"..\").resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if BASE_PATH not in sys.path:\n",
    "        sys.path.insert(0, BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpus_parser.conll_parser\n",
    "# import importlib\n",
    "# importlib.reload(corpus_parser.conll_parser)\n",
    "import os\n",
    "\n",
    "from corpus_parser.unified_parser import UnifiedParser\n",
    "from corpus_parser.conll_parser import ConllParser\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "\n",
    "import json\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "from link_prediction.models.link_utils import create_lr_annealing_function\n",
    "from link_prediction.models.attention import apply_attention, apply_multihead_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model initial params\n",
    "\n",
    "INFO_TAG = \"persuasive_essays_paragraph_all_linked\"\n",
    "# INFO_TAG = \"cdcp\"\n",
    "# INFO_TAG = \"abstrct\"\n",
    "\n",
    "DATA_PATH = Path(BASE_PATH, \"data/\")\n",
    "\n",
    "# LANGUAGE = \"english\"\n",
    "LANGUAGE = \"spanish\"\n",
    "if LANGUAGE == \"english\":\n",
    "    GLOVE_PATH = Path(DATA_PATH, 'glove.840B.300d.txt')\n",
    "elif LANGUAGE == \"spanish\":\n",
    "    GLOVE_PATH = Path(DATA_PATH, 'glove-sbwc.i25.vec')\n",
    "else:\n",
    "    raise Exception(\"Not supported language\")\n",
    "\n",
    "EXPORT_PATH = Path(DATA_PATH, 'link_prediction', INFO_TAG)\n",
    "TO_PROCESS_DATADIR = Path(DATA_PATH, 'segmenter_processed', INFO_TAG)\n",
    "PROCESSED_DATADIR = Path(DATA_PATH, 'link_prediction_processed', INFO_TAG)\n",
    "DIM = 300\n",
    "\n",
    "params = {\n",
    "    'in_production': True,\n",
    "    \n",
    "    # Model Training Hyperparameters\n",
    "    'epochs': 70,\n",
    "    'batch_size': 20,\n",
    "    'metrics': ['acc'],\n",
    "    \n",
    "    # Ensemble Hyperparameters\n",
    "    'ensemble_amount': 3,\n",
    "    \n",
    "    # Model Hyperparameters\n",
    "    'dim': DIM,\n",
    "    'dropout': 0.1,\n",
    "    'lstm_size': 200,\n",
    "    'max_distance_encoded': 5,\n",
    "    'linear_embedders_dims': [50, 50, 50, DIM],\n",
    "    'regularizer_weight': 0.001,\n",
    "    'encoder_dense_units': 50,\n",
    "    'encoder_pool_size': 1, # If 1 no tranformation is made to the input.\n",
    "    'lstm_units': 50,\n",
    "    'final_size': 20,\n",
    "    'residual_size': 50,\n",
    "    'with_attention': False, # If the attention block is used\n",
    "    'with_multi_head_attention': False, # If the attention block is used\n",
    "    'head_dim': 10,\n",
    "    'key_dim': 20,\n",
    "    'forward_dim': 50,\n",
    "    'loss_weights': {\n",
    "        'relation': 10,\n",
    "        'source': 1,\n",
    "        'target': 1,\n",
    "    },\n",
    "    \n",
    "    # Adam Optimizer Hyperparameters\n",
    "    'lr_alpha': 0.003,\n",
    "    'lr_kappa': 0.001,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    \n",
    "    # Early Stopping Hyperparameters\n",
    "    'min_delta': 0,\n",
    "    'patience': 5,\n",
    "    'return_best': False,\n",
    "    \n",
    "    # Corpus Info\n",
    "    'corpus_path': str(Path(DATA_PATH, 'projection', INFO_TAG)),\n",
    "    'glove_path': str(Path(EXPORT_PATH, 'glove.npz')),\n",
    "    'export_path': str(EXPORT_PATH),\n",
    "    'data_path': str(DATA_PATH),\n",
    "    'glove_raw_path': str(Path(GLOVE_PATH)),\n",
    "    'language': LANGUAGE,\n",
    "    \n",
    "    # Vectorizer Hyperparameters\n",
    "    'sequence_standardize': 'lower',\n",
    "    'sequence_split': 'whitespace',\n",
    "    \n",
    "    # Corpus Hyperparameters\n",
    "    'max_proposition_distance': 10, # Max distance allowed between argumentation\n",
    "    'non_related_max_proportion': 0.5, # Max proportion allowed between non related links and the total of links\n",
    "    \n",
    "    # Data info\n",
    "    'to_process_data_path': str(Path(TO_PROCESS_DATADIR)), # Directory with text to be processed\n",
    "    'processed_data_path': str(Path(PROCESSED_DATADIR)), # Directory to save the processed data\n",
    "}\n",
    "\n",
    "MODEL_NAME = \"model\" + (\"_attention\" if params['with_attention'] else \"\") \\\n",
    "                     + (\"_multihead\" if params['with_multi_head_attention'] else \"\") \\\n",
    "                     + f\"_{params['lr_alpha']}\" \\\n",
    "                     + f\"_{params['encoder_pool_size']}\" \\\n",
    "                     + f\"_{params['dropout']}\" \\\n",
    "                     + f\"_{params['patience']}\" \\\n",
    "                     + (f\"_{params['final_size']}\" if params['final_size'] != 20 else \"\") \\\n",
    "                     + f\"_{params['return_best']}\"\n",
    "params['model_name'] = MODEL_NAME\n",
    "\n",
    "print(MODEL_NAME)\n",
    "\n",
    "params.update({\n",
    "    'model_path': str(EXPORT_PATH / MODEL_NAME)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_load_params(params: dict, load_params=False, save_params=True):\n",
    "    Path(params['model_path']).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    params_path = Path(params['model_path'], \"params.json\")\n",
    "\n",
    "    assert not (load_params and save_params), \"Only one action can be performed, load or save but not both\"\n",
    "    \n",
    "    if load_params:\n",
    "        params = json.load(params_path.open('r'))\n",
    "        for key, value in params.items():\n",
    "            print(key, \"-->\", value)\n",
    "    \n",
    "    if save_params:\n",
    "        json.dump(params, params_path.open('w'))\n",
    "    \n",
    "    return params\n",
    "\n",
    "params = save_load_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "\n",
    "def find_duplicates(dataframe, first, second, group):\n",
    "\n",
    "    for g, dataframe in dataframe.groupby(by=group):\n",
    "        rows = [row for _, row in dataframe.iterrows()]\n",
    "        for i, row in enumerate(rows):\n",
    "            for row2 in rows[i+1:]:\n",
    "                if row[first] == row2[second] and row[second] == row2[first]:\n",
    "                    print(\"DUPLICATED ROW\")\n",
    "                    print(g)\n",
    "                    print(row)\n",
    "                    print(row2)\n",
    "\n",
    "                \n",
    "def extract_propositions(params: dict):\n",
    "    corpus_path = Path(params['corpus_path'])\n",
    "    \n",
    "    parser = UnifiedParser()\n",
    "    \n",
    "    names = [\n",
    "        \"dev\", \n",
    "        \"test\",\n",
    "        \"train\",\n",
    "    ]\n",
    "    \n",
    "    relation_tags = set()\n",
    "    proposition_tags = set()\n",
    "    \n",
    "    source_vocabulary = set()\n",
    "    target_vocabulary = set()\n",
    "    \n",
    "    # Max amount of propositions in a document\n",
    "    max_amount_source_in_doc = 0\n",
    "    max_amount_target_in_doc = 0\n",
    "    \n",
    "    # Max amount of tokens in a proposition\n",
    "    max_size_in_source_prop = 0\n",
    "    max_size_in_target_prop = 0\n",
    "    \n",
    "    for name in names:\n",
    "        \n",
    "        proposition_dict = parser.parse_dir(corpus_path / name)\n",
    "        \n",
    "        current_source_arg_units = {\n",
    "            'prop_id': [], \n",
    "            'prop_type': [], \n",
    "            'prop_text': [], \n",
    "            'file_key': []\n",
    "        }\n",
    "        current_target_arg_units = {\n",
    "            'prop_id': [], \n",
    "            'prop_type': [], \n",
    "            'prop_text': [], \n",
    "            'file_key': []\n",
    "        }\n",
    "        current_relations = {\n",
    "            'prop_id_source': [], \n",
    "            'prop_id_target': [], \n",
    "            'relation_type': [], \n",
    "            'distance': [], \n",
    "            'file_key': []\n",
    "        }\n",
    "\n",
    "        for key, (args_unit, relations, _) in proposition_dict.items():\n",
    "            if relations.empty: \n",
    "                continue\n",
    "            \n",
    "            args_unit['file_key'] = [key for _ in range(len(args_unit))]\n",
    "            args_unit = args_unit[['prop_id', 'prop_type', 'prop_text', 'file_key']]\n",
    "            \n",
    "            relations = relations[['prop_id_source', 'prop_id_target', 'relation_type']]\n",
    "            relations['distance'] = relations.aggregate(lambda x: x['prop_id_target']-x['prop_id_source'], axis=1)\n",
    "            relations['file_key'] = relations.aggregate(lambda x: key, axis=1)\n",
    "            \n",
    "            source_prop = args_unit[args_unit['prop_id'].isin(relations['prop_id_source'])]\n",
    "            target_prop = args_unit[args_unit['prop_id'].isin(relations['prop_id_target'])]\n",
    "            \n",
    "            source_vocabulary.update([t for s in source_prop['prop_text'] for t in s.split()])\n",
    "            target_vocabulary.update([t for s in target_prop['prop_text'] for t in s.split()])\n",
    "            \n",
    "            max_size_in_source_prop = max(max_size_in_source_prop, source_prop.aggregate(lambda x: len(x['prop_text'].split()), axis=1).max())\n",
    "            max_size_in_target_prop = max(max_size_in_target_prop, target_prop.aggregate(lambda x: len(x['prop_text'].split()), axis=1).max())\n",
    "            \n",
    "            \n",
    "            max_amount_source_in_doc = max(max_amount_source_in_doc, len(relations['prop_id_source'].drop_duplicates()))\n",
    "            max_amount_target_in_doc = max(max_amount_target_in_doc, len(relations['prop_id_target'].drop_duplicates()))\n",
    "            \n",
    "            current_source_arg_units['prop_id'].extend(source_prop['prop_id'])\n",
    "            current_source_arg_units['prop_type'].extend(source_prop['prop_type'])\n",
    "            current_source_arg_units['prop_text'].extend(source_prop['prop_text'])\n",
    "            current_source_arg_units['file_key'].extend(source_prop['file_key'])\n",
    "            \n",
    "            current_target_arg_units['prop_id'].extend(target_prop['prop_id'])\n",
    "            current_target_arg_units['prop_type'].extend(target_prop['prop_type'])\n",
    "            current_target_arg_units['prop_text'].extend(target_prop['prop_text'])\n",
    "            current_target_arg_units['file_key'].extend(target_prop['file_key'])\n",
    "            \n",
    "            current_relations['prop_id_source'].extend(relations['prop_id_source'])\n",
    "            current_relations['prop_id_target'].extend(relations['prop_id_target'])\n",
    "            current_relations['relation_type'].extend(relations['relation_type'])\n",
    "            current_relations['distance'].extend(relations['distance'])\n",
    "            current_relations['file_key'].extend(relations['file_key'])\n",
    "\n",
    "            \n",
    "        # Add Inverse Relations\n",
    "        inverse_relations = {\n",
    "            'prop_id_source': current_relations['prop_id_target'].copy(),\n",
    "            'prop_id_target': current_relations['prop_id_source'].copy(),\n",
    "            'relation_type': [relation_type + \"_Inverse\" for relation_type in current_relations['relation_type']],\n",
    "            'distance': [-distance for distance in current_relations['distance']],\n",
    "            'file_key': current_relations['file_key'].copy(),\n",
    "        }\n",
    "        current_relations['prop_id_source'].extend(inverse_relations['prop_id_source'])\n",
    "        current_relations['prop_id_target'].extend(inverse_relations['prop_id_target'])\n",
    "        current_relations['relation_type'].extend(inverse_relations['relation_type'])\n",
    "        current_relations['distance'].extend(inverse_relations['distance'])\n",
    "        current_relations['file_key'].extend(inverse_relations['file_key'])\n",
    "        \n",
    "        \n",
    "        current_relations = pandas.DataFrame(current_relations)\n",
    "        current_source_arg_units = pandas.DataFrame(current_source_arg_units)\n",
    "        current_target_arg_units = pandas.DataFrame(current_target_arg_units)\n",
    "        \n",
    "        # Sanity checks\n",
    "#         print(\"NEGATIVE PROP IDs\")\n",
    "#         print(\"TARGET < 0\", current_target_arg_units[current_target_arg_units['prop_id'] < 0])\n",
    "#         print(\"SOURCE < 0\", current_source_arg_units[current_source_arg_units['prop_id'] < 0])\n",
    "#         print(\"RELATION TARGET < 0:\", list(current_relations[current_relations['prop_id_target'] < 0]['file_key']))\n",
    "#         print(\"RELATION SOURCE < 0:\", list(current_relations[current_relations['prop_id_source'] < 0]['file_key']))\n",
    "#         def check_max(s_t_data, data, max_column, compare_to, title):\n",
    "#             for file, df in data.groupby(by='file_key'):\n",
    "#                 maxim = s_t_data[s_t_data['file_key'] == file][max_column].max()\n",
    "#                 print(title, maxim, file)\n",
    "#                 print(df[df[compare_to] > maxim])\n",
    "#         check_max(current_target_arg_units, current_relations, 'prop_id', 'prop_id_target', \"RELATION TARGET > max\")\n",
    "#         check_max(current_source_arg_units, current_relations, 'prop_id', 'prop_id_source', \"RELATION SOURCE > max\")\n",
    "#         print(\"BEFORE\")\n",
    "#         find_duplicates(current_relations, 'prop_id_source', 'prop_id_target', 'file_key')\n",
    "\n",
    "        params[f'{name}_source_propositions'] = current_source_arg_units\n",
    "        params[f'{name}_target_propositions'] = current_target_arg_units\n",
    "        params[f'{name}_relations'] = current_relations\n",
    "\n",
    "        print(name, \"relations\", len(current_relations))\n",
    "        print(name, \"source argumentative units\", len(current_source_arg_units))\n",
    "        print(name, \"target argumentative units\", len(current_target_arg_units))\n",
    "\n",
    "        relation_tags.update(current_relations['relation_type'])\n",
    "        proposition_tags.update(current_source_arg_units['prop_type'])\n",
    "        proposition_tags.update(current_target_arg_units['prop_type'])\n",
    "    \n",
    "\n",
    "    vocabulary = source_vocabulary.union(target_vocabulary)\n",
    "    params['vocabulary'] = vocabulary\n",
    "    print(\"Vocab size\", len(vocabulary))\n",
    "    \n",
    "    relation_tags = sorted(relation_tags)\n",
    "    proposition_tags = sorted(proposition_tags)\n",
    "    print(\"Relation tags\", relation_tags)\n",
    "    print(\"Proposition tags\", proposition_tags)\n",
    "    params['relation_tags'] = relation_tags\n",
    "    params['proposition_tags'] = proposition_tags\n",
    "    \n",
    "    max_size_prop = max(max_size_in_source_prop, max_size_in_target_prop)\n",
    "    max_amount_doc = max(max_amount_source_in_doc, max_amount_target_in_doc)\n",
    "    params['max_size_prop'] = max_size_prop\n",
    "    params['max_amount_doc'] = max_amount_doc\n",
    "    \n",
    "    print('max_size_prop', max_size_prop)\n",
    "    print('max_amount_doc', max_amount_doc)\n",
    "\n",
    "    # Vectorizers\n",
    "    sequence_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = len(vocabulary) + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = int(max_size_prop),\n",
    "        standardize = params['sequence_standardize'],\n",
    "        split = params['sequence_split']\n",
    "    )\n",
    "    sequence_vectorizer.adapt(pandas.concat([\n",
    "        params['train_source_propositions'],\n",
    "        params['train_target_propositions'],\n",
    "    ], ignore_index=True)['prop_text'])\n",
    "    params['sequence_vectorizer'] = sequence_vectorizer\n",
    "    \n",
    "    relation_tag_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = len(relation_tags) + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = 1,\n",
    "        standardize = None,\n",
    "        split = None\n",
    "    )\n",
    "    relation_tag_vectorizer.adapt(relation_tags)\n",
    "    params['relation_tag_vectorizer'] = relation_tag_vectorizer\n",
    "    \n",
    "    proposition_tag_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = len(proposition_tags) + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = 1,\n",
    "        standardize = None,\n",
    "        split = None\n",
    "    )\n",
    "    proposition_tag_vectorizer.adapt(proposition_tags)\n",
    "    params['proposition_tag_vectorizer'] = proposition_tag_vectorizer\n",
    "    \n",
    "    # One-Hot Encoders\n",
    "    relation_encoder = layers.CategoryEncoding(\n",
    "        num_tokens=len(relation_tag_vectorizer.get_vocabulary()), # Plus PAD and UNK\n",
    "        output_mode=\"one_hot\",\n",
    "    )\n",
    "    params['relation_encoder'] = relation_encoder\n",
    "    \n",
    "    proposition_encoder = layers.CategoryEncoding(\n",
    "        num_tokens=len(proposition_tag_vectorizer.get_vocabulary()), # Plus PAD and UNK\n",
    "        output_mode=\"one_hot\",\n",
    "    )\n",
    "    params['proposition_encoder'] = proposition_encoder\n",
    "    \n",
    "    \n",
    "extract_propositions(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_statistics(params: dict):\n",
    "    \n",
    "    def plot_relation_info(relation_df: pandas.DataFrame):\n",
    "        relation_df = relation_df[~relation_df['relation_type'].str.endswith(\"_Inverse\")]\n",
    "        bins = set(relation_df['distance'])\n",
    "        relation_df['distance'].hist(bins=len(bins))\n",
    "        plt.title(\"Relation distances\")\n",
    "        plt.show()\n",
    "    \n",
    "    for name in ['train']:\n",
    "        current_relations = params[f'{name}_relations']\n",
    "\n",
    "        plot_relation_info(current_relations)\n",
    "    \n",
    "dataset_statistics(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def creating_glove_embeddings(params: dict):\n",
    "    \n",
    "    if Path(params[\"glove_path\"]).exists():\n",
    "        print(\"Glove Embedding Matrix Found\")\n",
    "        embedding_matrix = np.load(params[\"glove_path\"])[\"embeddings\"]\n",
    "        params['embedding_matrix'] = embedding_matrix\n",
    "        return\n",
    "    \n",
    "    # Loading Glove\n",
    "    hits = 0\n",
    "    embedding_dim = params['dim']\n",
    "    word_to_index = dict(map(lambda x: (x[1], x[0]), enumerate(params['sequence_vectorizer'].get_vocabulary())))\n",
    "    num_tokens = len(word_to_index) # Plus padding and unknown \n",
    "\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    with Path(params[\"glove_raw_path\"]).open() as f:\n",
    "        for line_idx, line in enumerate(f):\n",
    "            if line_idx % 100000 == 0:\n",
    "                print('- At line {}'.format(line_idx))\n",
    "            line = line.strip().split()\n",
    "            if len(line) != 300 + 1:\n",
    "                continue\n",
    "            word = line[0]\n",
    "            embedding = line[1:]\n",
    "            if word in word_to_index:\n",
    "                hits += 1\n",
    "                word_idx = word_to_index[word]\n",
    "                embedding_matrix[word_idx] = embedding\n",
    "                \n",
    "    print('- Done. Found {} vectors for {} words'.format(hits, num_tokens - 2))\n",
    "    \n",
    "    params['embedding_matrix'] = embedding_matrix\n",
    "    Path(params[\"glove_path\"], \"..\").resolve().mkdir(exist_ok=True, parents=True)\n",
    "    np.savez_compressed(params[\"glove_path\"], embeddings=embedding_matrix)\n",
    "\n",
    "creating_glove_embeddings(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Dataset\n",
    "\n",
    "def encode_distance(distance, encode_size):\n",
    "    \"\"\"\n",
    "    return: Tensor with the encoded distance\n",
    "    \"\"\"\n",
    "    middle = encode_size // 2\n",
    "\n",
    "    abs_distance = tf.cast(tf.abs(distance), dtype=\"int32\") \n",
    "    zeros = tf.zeros((tf.maximum(1, abs_distance), middle))\n",
    "\n",
    "    to_sum = tf.concat([zeros, tf.eye(abs_distance, num_columns=middle)], axis=0)\n",
    "    distance_vec = tf.foldl(lambda x, y: tf.add(x, y), to_sum)\n",
    "\n",
    "    if distance < 0:\n",
    "        first_vec = tf.reverse(distance_vec, axis=[0])\n",
    "        second_vec = zeros[0]\n",
    "    else:\n",
    "        first_vec = zeros[0]\n",
    "        second_vec = distance_vec\n",
    "\n",
    "    return tf.concat([first_vec, second_vec], axis=0)\n",
    "\n",
    "\n",
    "def encode_datasets(params: dict):\n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    proposition_tag_vectorizer = params['proposition_tag_vectorizer']\n",
    "    relation_tag_vectorizer = params['relation_tag_vectorizer']\n",
    "    proposition_encoder = params['proposition_encoder']\n",
    "    relation_encoder = params['relation_encoder']\n",
    "    distance_encoding_bits = params['max_distance_encoded'] * 2\n",
    "    max_proposition_distance = params['max_proposition_distance']\n",
    "    non_related_max_proportion = params['non_related_max_proportion']\n",
    "\n",
    "    df_path = Path(params['export_path'], f'data_df_{max_proposition_distance}.pkl')\n",
    "    if df_path.exists():\n",
    "        data_dataframe = pandas.read_pickle(df_path)\n",
    "        print(\"INFO: Data Dataframe Found\")\n",
    "    else:\n",
    "        data_dataframe = pandas.DataFrame(\n",
    "            columns = [\n",
    "                'file_key', \n",
    "                'source_prop_id', \n",
    "                'target_prop_id', \n",
    "                'source_prop_text',\n",
    "                'target_prop_text',\n",
    "                'source_prop_type',\n",
    "                'target_prop_type',\n",
    "                'relation_type', \n",
    "                'distance',\n",
    "                'split',\n",
    "            ])\n",
    "\n",
    "        for split in ['dev', 'test', 'train']:\n",
    "\n",
    "            source_arg_units = params[f'{split}_source_propositions']\n",
    "            target_arg_units = params[f'{split}_target_propositions']\n",
    "            relations = params[f'{split}_relations']\n",
    "\n",
    "            all_arg_units = pandas.concat([source_arg_units, target_arg_units], ignore_index=True)\n",
    "            all_arg_units = all_arg_units.drop_duplicates()\n",
    "            all_arg_units = [(file, df) for file, df in all_arg_units.groupby(by='file_key')]\n",
    "            \n",
    "            for file_key, file_source_df in all_arg_units:\n",
    "                file_target_df = file_source_df.copy()\n",
    "                file_relations = relations[relations['file_key'] == file_key]\n",
    "\n",
    "                current_file_info = {\n",
    "                    'file_key': [], \n",
    "                    'source_prop_id': [],\n",
    "                    'target_prop_id': [],\n",
    "                    'source_prop_text': [],\n",
    "                    'target_prop_text': [],\n",
    "                    'source_prop_type': [],\n",
    "                    'target_prop_type': [],\n",
    "                    'relation_type': [],\n",
    "                    'distance': [],\n",
    "                    'split': [],\n",
    "                }\n",
    "                \n",
    "                for _, source_row in file_source_df.iterrows():\n",
    "                    source_id = source_row['prop_id']\n",
    "                    for _, target_row in file_target_df.iterrows():\n",
    "                        target_id = target_row['prop_id']\n",
    "\n",
    "                        # Same relations not allowed\n",
    "                        if source_id == target_id:\n",
    "                            continue\n",
    "\n",
    "                        distance = target_id - source_id\n",
    "                        # Distance is greater than the max alowed distance between propositions\n",
    "                        if abs(distance) > max_proposition_distance:\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        source_target_relation = file_relations[(file_relations['prop_id_target'] == target_id) & (file_relations['prop_id_source'] == source_id)]\n",
    "                        \n",
    "                        if len(source_target_relation) == 0:\n",
    "                            # No related propositions\n",
    "                            relation_type = '' # No Relation\n",
    "                            distance = target_id - source_id\n",
    "                            source_target_relation = pandas.concat([source_target_relation, pandas.DataFrame({\n",
    "                                'prop_id_source': [source_id],\n",
    "                                'prop_id_target': [target_id],\n",
    "                                'relation_type': [relation_type],\n",
    "                                'distance': [distance],\n",
    "                                'file_key': [file_key]\n",
    "                            })])\n",
    "                            \n",
    "                        if len(source_target_relation) > 1:\n",
    "                            print(\"WARNING: Multiple relation with single source-target pair\")\n",
    "                            print(source_target_relation)\n",
    "\n",
    "                        for _, relation_row in source_target_relation.iterrows():\n",
    "\n",
    "                            assert relation_row['distance'] == distance, f\"{relation_row['distance']} != {distance}\"\n",
    "\n",
    "                            # Adding data\n",
    "                            current_file_info['file_key'].append(file_key)\n",
    "                            current_file_info['source_prop_id'].append(source_id)\n",
    "                            current_file_info['target_prop_id'].append(target_id)\n",
    "                            current_file_info['source_prop_text'].append(source_row['prop_text'])\n",
    "                            current_file_info['target_prop_text'].append(target_row['prop_text'])\n",
    "                            current_file_info['source_prop_type'].append(source_row['prop_type'])\n",
    "                            current_file_info['target_prop_type'].append(target_row['prop_type'])\n",
    "                            current_file_info['relation_type'].append(relation_row['relation_type'])\n",
    "                            current_file_info['distance'].append(distance)\n",
    "                            current_file_info['split'].append(split)\n",
    "                    \n",
    "                current_file_info = pandas.DataFrame(current_file_info)\n",
    "                \n",
    "\n",
    "                data_dataframe = pandas.concat([data_dataframe, current_file_info], ignore_index=True)\n",
    "        data_dataframe.to_pickle(df_path)\n",
    "\n",
    "    params['raw_data_dataframe'] = data_dataframe\n",
    "    print(len(data_dataframe))\n",
    "    \n",
    "    # Encoding\n",
    "    for split, data_dataframe in data_dataframe.groupby(by=\"split\"):\n",
    "        \n",
    "        relation_counter = Counter(data_dataframe['relation_type'])\n",
    "        \n",
    "        non_related_proportion = relation_counter[''] / len(data_dataframe)\n",
    "        if non_related_proportion > non_related_max_proportion:\n",
    "            amount_to_drop = int((relation_counter[''] - non_related_max_proportion * len(data_dataframe)) / (1 - non_related_max_proportion))\n",
    "            index = list(data_dataframe[data_dataframe['relation_type'] == ''].index)\n",
    "            rand.shuffle(index)\n",
    "            \n",
    "            data_dataframe = data_dataframe.drop(index[:amount_to_drop])\n",
    "        \n",
    "        params[f'raw_{split}_data_dataframe'] = data_dataframe\n",
    "            \n",
    "        relation_counter = Counter(data_dataframe['relation_type'])\n",
    "        print(split, relation_counter)\n",
    "        \n",
    "        source_ds = tf.data.Dataset.from_tensor_slices(tf.constant(data_dataframe['source_prop_text'])).map(lambda x: sequence_vectorizer(x))\n",
    "        target_ds = tf.data.Dataset.from_tensor_slices(tf.constant(data_dataframe['target_prop_text'])).map(lambda x: sequence_vectorizer(x))\n",
    "        source_type_ds = tf.data.Dataset.from_tensor_slices(tf.constant(data_dataframe['source_prop_type'])).map(lambda x: proposition_encoder(proposition_tag_vectorizer([x])))\n",
    "        target_type_ds = tf.data.Dataset.from_tensor_slices(tf.constant(data_dataframe['target_prop_type'])).map(lambda x: proposition_encoder(proposition_tag_vectorizer([x])))\n",
    "        relation_type_ds = tf.data.Dataset.from_tensor_slices(tf.constant(data_dataframe['relation_type'])).map(lambda x: relation_encoder(relation_tag_vectorizer([x])))\n",
    "        distance_ds = tf.data.Dataset.from_tensor_slices(list(data_dataframe['distance'].to_numpy(dtype=int))).map(lambda x: encode_distance(x, distance_encoding_bits))\n",
    "        \n",
    "        # Order matters\n",
    "        input_ds = tf.data.Dataset.zip((source_ds, target_ds, distance_ds))\n",
    "        output_ds = tf.data.Dataset.zip((relation_type_ds, source_type_ds, target_type_ds))\n",
    "        \n",
    "        ds = tf.data.Dataset.zip((input_ds, output_ds))\n",
    "        \n",
    "        params[f\"{split}_ds\"] = ds\n",
    "\n",
    "        \n",
    "encode_datasets(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Two versions of the model can be buit. The difference is the presence or not of an attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "def build_model(params: dict):\n",
    "    linear_embedders_dims = params['linear_embedders_dims']\n",
    "    max_sequence_size = params['max_size_prop']\n",
    "    words_amount = len(params['sequence_vectorizer'].get_vocabulary()) # Plus UNK and Pad\n",
    "    embedding_dim = params['dim']\n",
    "    embedding_matrix = params['embedding_matrix']\n",
    "    regularizer_weight = params['regularizer_weight']\n",
    "    dropout = params['dropout']\n",
    "    final_embedding_dimension = params['encoder_dense_units']\n",
    "    final_layer_size = params['final_size']\n",
    "    pool_size = params['encoder_pool_size']\n",
    "    distance_encoding_bits = params['max_distance_encoded'] * 2\n",
    "    lstm_units = params['lstm_units']\n",
    "    res_size = params['residual_size']\n",
    "    relation_amount = len(params['relation_tag_vectorizer'].get_vocabulary()) # Plus UNK and Pad\n",
    "    proposition_tag_amount = len(params['proposition_tag_vectorizer'].get_vocabulary()) # Plus UNK and Pad\n",
    "    with_attention = params['with_attention']\n",
    "    with_miltihead_attention = params['with_multi_head_attention']\n",
    "    ensemble_amount = params['ensemble_amount']\n",
    "    head_dim = params['head_dim']\n",
    "    key_dim = params['key_dim']\n",
    "    forward_dim = params['forward_dim']\n",
    "    \n",
    "    def build_embedder(max_sequence_size, words_amount, embedding_dim, embedding_matrix, linear_layers_dims, regularizer_weight, dropout):\n",
    "        \"\"\"\n",
    "        Builds a proposition embedder\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input layer\n",
    "        int_sequence_input = keras.Input(\n",
    "            shape=(max_sequence_size,), \n",
    "            dtype=\"int64\"\n",
    "        )\n",
    "\n",
    "        # Embedding layer, convert an index vector into a embedding vector, by accessing embedding_matrix\n",
    "        embedding_layer = layers.Embedding(\n",
    "            words_amount,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "            input_length=max_sequence_size,\n",
    "        )\n",
    "\n",
    "        initial_layer = model_layers = embedding_layer(int_sequence_input)\n",
    "\n",
    "        def get_linear_layer(dense_dim, linear_layer=None):\n",
    "            \"\"\"\n",
    "            Creates a single dense layer for the embedder\n",
    "            \"\"\"\n",
    "            \n",
    "            if linear_layer is None:\n",
    "                input_vec = keras.Input(shape=(embedding_dim,))\n",
    "            else:\n",
    "                input_vec = linear_layer\n",
    "            \n",
    "            linear_layer = layers.Dense(\n",
    "                units=dense_dim,\n",
    "                activation=None,\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "                bias_regularizer=keras.regularizers.l2(regularizer_weight)\n",
    "            )(input_vec)\n",
    "            \n",
    "            linear_layer = layers.BatchNormalization()(linear_layer)\n",
    "            linear_layer = layers.Dropout(dropout)(linear_layer)\n",
    "            linear_layer = layers.Activation('relu')(linear_layer)\n",
    "            return input_vec, linear_layer\n",
    "        \n",
    "        # Linear transformation\n",
    "        linear_input, linear_layer = get_linear_layer(linear_layers_dims[0])\n",
    "        for dim in linear_layers_dims[1:]:\n",
    "            _, linear_layer = get_linear_layer(dim, linear_layer)\n",
    "        linear_layer = keras.Model(inputs=linear_input, outputs=linear_layer)\n",
    "        \n",
    "        # Apply linear_layer to each word embedding\n",
    "        model_layers = layers.TimeDistributed(linear_layer)(model_layers)\n",
    "        \n",
    "        # Residual connection\n",
    "        model_layers = layers.Add()([initial_layer, model_layers])\n",
    "        \n",
    "        return int_sequence_input, model_layers\n",
    "    \n",
    "    def build_dense_encoder(max_sequence_size, embedding_dim, final_dimension, pool_size, regularizer_weight):\n",
    "        \n",
    "        # Input layer\n",
    "        embedding_inputs = keras.Input(\n",
    "            shape=(max_sequence_size, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        encoder_layer = embedding_inputs\n",
    "        \n",
    "        linear_layer = layers.Dense(\n",
    "            units=final_dimension,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "            bias_regularizer=keras.regularizers.l2(regularizer_weight)\n",
    "        )\n",
    "        \n",
    "        # Apply linear_layer to each word embedding\n",
    "        encoder_layer = layers.TimeDistributed(linear_layer)(encoder_layer)\n",
    "        \n",
    "        # Average the words embeddings\n",
    "        encoder_layer = layers.AveragePooling1D(\n",
    "            pool_size=pool_size,\n",
    "        )(encoder_layer)\n",
    "    \n",
    "        encoder_layer = layers.BatchNormalization()(encoder_layer)\n",
    "    \n",
    "        return keras.Model(inputs=embedding_inputs, outputs=encoder_layer)\n",
    "    \n",
    "    def build_bilstm_encoder(sequence_size, encoded_dim, lstm_units, dropout, regularizer_weight, return_sequences):\n",
    "\n",
    "        # Input layer\n",
    "        embedding_inputs = keras.Input(\n",
    "            shape=(sequence_size, encoded_dim)\n",
    "        )\n",
    "        \n",
    "        bilstm_layer = layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                units=lstm_units,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=dropout,\n",
    "                kernel_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "                recurrent_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "                bias_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "                return_sequences=return_sequences,\n",
    "            ),\n",
    "            merge_mode='mul'\n",
    "        )(embedding_inputs)\n",
    "        \n",
    "        return keras.Model(inputs=embedding_inputs, outputs=bilstm_layer)\n",
    "    \n",
    "    def apply_resnet(input_layer, regularizer_weight, res_size, dropout):\n",
    "        prev_layer = input_layer\n",
    "        prev_block = prev_layer\n",
    "        \n",
    "        layers_dims = (2, 2)\n",
    "        blocks = layers_dims[0]\n",
    "        res_layers = layers_dims[1]\n",
    "\n",
    "        shape = int(np.shape(input_layer)[1])\n",
    "\n",
    "        for i in range(1, blocks + 1):\n",
    "            for j in range(1, res_layers):\n",
    "                prev_layer = layers.BatchNormalization()(prev_layer)\n",
    "\n",
    "                prev_layer = layers.Dropout(dropout)(prev_layer)\n",
    "\n",
    "                prev_layer = layers.Activation('relu')(prev_layer)\n",
    "\n",
    "                prev_layer = layers.Dense(\n",
    "                    units=res_size,\n",
    "                    activation=None,\n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "                    bias_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "                )(prev_layer)\n",
    "            \n",
    "            prev_layer = layers.BatchNormalization()(prev_layer)\n",
    "\n",
    "            prev_layer = layers.Dropout(dropout)(prev_layer)\n",
    "\n",
    "            prev_layer = layers.Activation('relu')(prev_layer)\n",
    "\n",
    "            prev_layer = layers.Dense(units=shape,\n",
    "                               activation=None,\n",
    "                               kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "                               bias_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "                               )(prev_layer)\n",
    "\n",
    "            prev_layer = layers.Add()([prev_block, prev_layer])\n",
    "            prev_block = prev_layer\n",
    "\n",
    "        return prev_block\n",
    "    \n",
    "    def create_single_model(index):\n",
    "        \"\"\"\n",
    "        Create a single model for the ensemble learning\n",
    "        \"\"\"\n",
    "        \n",
    "        input_distance = keras.Input(\n",
    "            shape=(distance_encoding_bits, )\n",
    "        )\n",
    "\n",
    "        input_source_embedder, source_embedder = build_embedder(\n",
    "            max_sequence_size, \n",
    "            words_amount, \n",
    "            embedding_dim, \n",
    "            embedding_matrix, \n",
    "            linear_embedders_dims, \n",
    "            regularizer_weight, \n",
    "            dropout\n",
    "        )\n",
    "\n",
    "        input_target_embedder, target_embedder = build_embedder(\n",
    "            max_sequence_size, \n",
    "            words_amount, \n",
    "            embedding_dim, \n",
    "            embedding_matrix, \n",
    "            linear_embedders_dims, \n",
    "            regularizer_weight, \n",
    "            dropout\n",
    "        )\n",
    "\n",
    "        dense_encoder = build_dense_encoder(\n",
    "            max_sequence_size, \n",
    "            embedding_dim, \n",
    "            final_embedding_dimension, \n",
    "            pool_size, \n",
    "            regularizer_weight\n",
    "        )\n",
    "\n",
    "        bilstm_encoder = build_bilstm_encoder(\n",
    "            int(np.floor(max_sequence_size / pool_size)), \n",
    "            final_embedding_dimension, \n",
    "            lstm_units, \n",
    "            dropout, \n",
    "            regularizer_weight,\n",
    "            with_attention or with_miltihead_attention\n",
    "        )\n",
    "        \n",
    "        # Apply dense encoder to source and target sequence features\n",
    "        prev_source_layers = source_layers = dense_encoder(source_embedder)\n",
    "        prev_target_layers = target_layers = dense_encoder(target_embedder)\n",
    "\n",
    "        # Apply bilstm encoder to source and target sequence features\n",
    "        source_layers = bilstm_encoder(source_layers)\n",
    "        target_layers = bilstm_encoder(target_layers)\n",
    "\n",
    "        if with_attention:\n",
    "            source_layers, target_layers = apply_attention(\n",
    "                input_source_embedder, \n",
    "                input_target_embedder,\n",
    "                prev_source_layers,\n",
    "                prev_target_layers,\n",
    "                source_layers,\n",
    "                target_layers,\n",
    "                final_layer_size,\n",
    "                index,\n",
    "            )\n",
    "        elif with_miltihead_attention:\n",
    "            source_layers, target_layers = apply_multihead_attention(\n",
    "                source_layers=source_layers, \n",
    "                target_layers=target_layers, \n",
    "                num_head=head_dim,\n",
    "                key_dim=key_dim,\n",
    "                regularizer=keras.regularizers.l2(regularizer_weight), \n",
    "                dropout=dropout,\n",
    "                feedforward_dim=forward_dim,\n",
    "            )\n",
    "\n",
    "        # Concatenate source and target sequence features with other features \n",
    "        model_layers = layers.Concatenate()([source_layers, target_layers, input_distance])\n",
    "        model_layers = layers.BatchNormalization()(model_layers)\n",
    "        model_layers = layers.Dropout(dropout)(model_layers)\n",
    "\n",
    "        # Middle dense layer\n",
    "        model_layers = layers.Dense(\n",
    "            units=final_layer_size,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "            bias_regularizer=keras.regularizers.l2(regularizer_weight),\n",
    "        )(model_layers)\n",
    "\n",
    "        # Apply a residual network\n",
    "        model_layers = apply_resnet(\n",
    "            model_layers,\n",
    "            regularizer_weight,\n",
    "            res_size,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "\n",
    "        model_layers = layers.BatchNormalization()(model_layers)\n",
    "        model_layers = layers.Dropout(dropout)(model_layers)\n",
    "\n",
    "        # Classifiers\n",
    "        relation_classifier = layers.Dense(\n",
    "            units=relation_amount,\n",
    "            activation='softmax',\n",
    "            name=f\"relation_{index}\",\n",
    "        )(model_layers)\n",
    "\n",
    "        source_classifier = layers.Dense(\n",
    "            units=proposition_tag_amount,\n",
    "            activation='softmax',\n",
    "            name=f\"source_{index}\",\n",
    "        )(model_layers)\n",
    "\n",
    "        target_classifier = layers.Dense(\n",
    "            units=proposition_tag_amount,\n",
    "            activation='softmax',\n",
    "            name=f\"target_{index}\",\n",
    "        )(model_layers)\n",
    "\n",
    "        # Creating final model\n",
    "        model = keras.Model(\n",
    "            inputs=(input_source_embedder, input_target_embedder, input_distance),\n",
    "            outputs=(relation_classifier, source_classifier, target_classifier),\n",
    "            name=f\"{params['model_name']}_{index}\"\n",
    "        )\n",
    "    \n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for i in range(ensemble_amount):\n",
    "        model = create_single_model(i)\n",
    "        models.append(model)\n",
    "    \n",
    "    params[params['model_name']] = models\n",
    "\n",
    "build_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_and_save_model(params: dict):\n",
    "    model_name = params['model_name']\n",
    "    batch_size = params['batch_size']\n",
    "    if params['in_production']:\n",
    "        epochs = params['epochs']\n",
    "        train_ds = params['train_ds'].batch(batch_size)\n",
    "        val_ds = params['dev_ds'].batch(batch_size)\n",
    "        models = params[model_name]\n",
    "    else:\n",
    "        epochs = 2 \n",
    "        train_ds = params['train_ds'].batch(batch_size).take(30)\n",
    "        val_ds = params['dev_ds'].batch(batch_size).take(10)\n",
    "        models = params[model_name][:2]\n",
    "    loss_weights = params['loss_weights']\n",
    "    lr_alpha = params['lr_alpha']\n",
    "    lr_kappa = params['lr_kappa']\n",
    "    relation_amount = len(params['relation_tag_vectorizer'].get_vocabulary())\n",
    "    proposition_amount = len(params['proposition_tag_vectorizer'].get_vocabulary())\n",
    "    global_metrics = params['metrics']\n",
    "    beta_1 = params['beta_1']\n",
    "    beta_2 = params['beta_2']\n",
    "    min_delta = params['min_delta']\n",
    "    patience = params['patience']\n",
    "  \n",
    "    def single_train(index, model):\n",
    "        # Optimizer\n",
    "        lr_function = create_lr_annealing_function(initial_lr=lr_alpha, k=lr_kappa)\n",
    "        lr_scheduler = keras.callbacks.LearningRateScheduler(lr_function)\n",
    "        optimizer = tf.optimizers.Adam(\n",
    "            learning_rate=lr_function(0),\n",
    "            beta_1=beta_1,\n",
    "            beta_2=beta_2,\n",
    "        )\n",
    "\n",
    "        # EarlyStopping\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            min_delta=min_delta,\n",
    "            patience=patience,\n",
    "            restore_best_weights=params['return_best'],\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            f'relation_{index}': global_metrics.copy(),\n",
    "            f'source_{index}': global_metrics.copy(),\n",
    "            f'target_{index}': global_metrics.copy(),\n",
    "        }\n",
    "        for name, num_classes in [\n",
    "                (f'relation_{index}', relation_amount), \n",
    "                (f'source_{index}', proposition_amount), \n",
    "                (f'target_{index}', proposition_amount)\n",
    "            ]:\n",
    "\n",
    "            f1_macro = tfa.metrics.F1Score(\n",
    "                num_classes=num_classes,\n",
    "                average='macro',\n",
    "                name=f'{name}F1Macro',\n",
    "            )\n",
    "            metrics[name].extend([\n",
    "                f1_macro,\n",
    "            ])\n",
    "        \n",
    "        current_loss_weights = {f'{name}_{index}': value for name, value in loss_weights.items()}\n",
    "        \n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy', # Apply this loss function to all outputs\n",
    "            loss_weights=current_loss_weights, # Weights for the sum of the loss functions\n",
    "            optimizer=optimizer,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(train_ds,\n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs, \n",
    "                      validation_data=val_ds,\n",
    "                      callbacks=[\n",
    "                          lr_scheduler,\n",
    "                          early_stopping,\n",
    "                      ])\n",
    "\n",
    "        history = history.history\n",
    "        for key in history:\n",
    "            values = np.array(history[key]).tolist()\n",
    "            history[key] = values\n",
    "        params[f'history_{index}'] = history\n",
    "        with Path(params['model_path'], f\"{model_name}_{index}_history.json\").open('w') as f:\n",
    "            json.dump(history, f)\n",
    "\n",
    "        model.save(str(Path(params[\"model_path\"], f\"{model_name}_{index}\")), save_format='tf')\n",
    "\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        single_train(i, model)\n",
    "\n",
    "train_and_save_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "def load_saved_model(params: dict):\n",
    "    ensemble_amount = params['ensemble_amount']\n",
    "    \n",
    "    model_name = params[\"model_name\"]\n",
    "    models = []\n",
    "    for i in range(ensemble_amount):        \n",
    "        model_path = Path(params[\"model_path\"], f\"{model_name}_{i}\")\n",
    "        if model_path.exists():\n",
    "            model = keras.models.load_model(str(model_path))\n",
    "            models.append(model)\n",
    "        else:\n",
    "            print(f\"Model in {model_path} doesn't exist\")\n",
    "    params[model_name] = models\n",
    "\n",
    "    \n",
    "load_saved_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate_model(params: dict):\n",
    "    models = params[params['model_name']]\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    test_ds = params['test_ds'].batch(batch_size)\n",
    "    for i, model in enumerate(models):\n",
    "        print(\"Model\", i)\n",
    "        results = model.evaluate(test_ds, batch_size=batch_size, return_dict=True)\n",
    "        for key in results:\n",
    "            values = np.array(results[key]).tolist()\n",
    "            results[key] = values\n",
    "        json.dump(results, Path(params['model_path'], f\"test_result_{i}.json\").open(\"w\"))\n",
    "\n",
    "\n",
    "evaluate_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(params: dict):\n",
    "    model_name = params['model_name']\n",
    "    ensemble_amount = params['ensemble_amount']\n",
    "    images_path = Path(params['model_path'])\n",
    "\n",
    "    def plot_val_train_compare(history, val_key, train_key):\n",
    "        val = history[val_key]\n",
    "        train = history[train_key]\n",
    "        \n",
    "        X = [i for i in range(len(val))]\n",
    "        \n",
    "        plt.plot(X, val, label=val_key)\n",
    "        plt.plot(X, train, label=train_key)\n",
    "    \n",
    "    def show_plot(title, x_label=\"Epoch\", y_label=\"Value\"):\n",
    "        plt.title(title)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.legend()\n",
    "        plt.savefig(images_path / f\"{title}.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    def load_history(i):\n",
    "        history_path = Path(params['model_path'], f\"{model_name}_{i}_history.json\")\n",
    "        history = json.load(history_path.open())\n",
    "        params[f'history_{i}'] = history\n",
    "        return history\n",
    "\n",
    "    val_train = [\n",
    "        \"loss\",\n",
    "        \"relation_{i}_loss\",\n",
    "        \"source_{i}_loss\",\n",
    "        \"target_{i}_loss\",\n",
    "        \"relation_{i}_acc\",\n",
    "        \"source_{i}_acc\",\n",
    "        \"target_{i}_acc\",\n",
    "    ]\n",
    "    \n",
    "    for i in range(ensemble_amount):\n",
    "        history = load_history(i)\n",
    "        for key in val_train:\n",
    "            key = key.format_map({\"i\":i})\n",
    "            val_key = f\"val_{key}\"\n",
    "            plot_val_train_compare(history, val_key, key)\n",
    "            show_plot(key)\n",
    "        \n",
    "plot_history(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionModel(keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 models,\n",
    "                 sequence_vectorizer, \n",
    "                 proposition_tag_vectorizer, \n",
    "                 relation_tag_vectorizer, \n",
    "                 distance_encoding_bits,\n",
    "                 batch_size=32\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.models = models\n",
    "        self.sequence_vectorizer = sequence_vectorizer\n",
    "        self.proposition_tag_vectorizer = proposition_tag_vectorizer\n",
    "        self.relation_tag_vectorizer = relation_tag_vectorizer\n",
    "        self.relations = self.relation_tag_vectorizer.get_vocabulary()\n",
    "        self.propositions = self.proposition_tag_vectorizer.get_vocabulary()\n",
    "        self.distance_encoding_bits = distance_encoding_bits\n",
    "        self.linked = [i for i,r in enumerate(self.relations[2:], 2) if not r.endswith(\"_Inverse\")]\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __decode_outputs(self, outputs):\n",
    "        propositions = self.propositions\n",
    "        relations = self.relations\n",
    "        linked = self.linked\n",
    "        \n",
    "        final_result = []\n",
    "        relation_eye = tf.eye(len(relations))\n",
    "        proposition_eye = tf.eye(len(propositions) - 2) # Remove the unknown and the padding tokken\n",
    "        actual_relation = tf.gather(relation_eye, linked)\n",
    "        \n",
    "        for output in zip(*outputs):\n",
    "\n",
    "            # Reset voting vectors\n",
    "            relation_tag_tensor = tf.zeros(shape=(len(relations)))\n",
    "            linked_tag_tensor = tf.constant([0,0])\n",
    "            target_tag_tensor = tf.zeros(shape=(len(propositions) - 2)) # Remove the unknown and the padding tokken\n",
    "            source_tag_tensor = tf.zeros(shape=(len(propositions) - 2)) # Remove the unknown and the padding tokken\n",
    "            \n",
    "            for relation_output, source_output, target_output in zip(*output):\n",
    "                # Add the vote for each class\n",
    "                \n",
    "                source_output = source_output[2:] # Remove the unknown and the padding tokken\n",
    "                target_output = target_output[2:] # Remove the unknown and the padding tokken\n",
    "                \n",
    "                relation_tag_tensor = tf.add(relation_tag_tensor, relation_eye[tf.argmax(relation_output)])\n",
    "                target_tag_tensor = tf.add(target_tag_tensor, proposition_eye[tf.argmax(target_output)])\n",
    "                source_tag_tensor = tf.add(source_tag_tensor, proposition_eye[tf.argmax(source_output)])\n",
    "                \n",
    "                linked_prob = tf.reduce_sum(tf.foldl(lambda x, y: x + tf.math.multiply(y, relation_output), actual_relation, initializer = tf.zeros(shape=(len(relations)))))\n",
    "                \n",
    "                if linked_prob >  0.5:\n",
    "                    linked_add = tf.constant([0,1])\n",
    "                else:\n",
    "                    linked_add = tf.constant([1,0])\n",
    "                linked_tag_tensor = tf.add(linked_tag_tensor, linked_add)\n",
    "                \n",
    "            # Get the most voted class\n",
    "            relation_tag = relations[tf.argmax(relation_tag_tensor)]\n",
    "            target_tag_tensor = propositions[tf.argmax(target_tag_tensor) + 2] # Remove the unknown and the padding tokken\n",
    "            source_tag_tensor = propositions[tf.argmax(source_tag_tensor) + 2] # Remove the unknown and the padding tokken\n",
    "            linked = tf.argmax(linked_tag_tensor) == 1\n",
    "            \n",
    "            final_result.append((relation_tag, source_tag_tensor, target_tag_tensor, linked.numpy()))\n",
    "            \n",
    "        return final_result\n",
    "    \n",
    "    def call(self, source_inputs, target_inputs, distance_inputs):\n",
    "        \n",
    "        outputs = [[], [], []]\n",
    "        if source_inputs == [] or target_inputs == [] or distance_inputs == []:\n",
    "            return outputs\n",
    "\n",
    "        if len(set([len(source_inputs), len(target_inputs), len(distance_inputs)])) > 1:\n",
    "            print(\"WARNING: source_inputs, target_inputs, distance_inputs with different lengths\")\n",
    "\n",
    "        source_ds = tf.data.Dataset.from_tensor_slices(tf.constant(source_inputs)).map(lambda x: self.sequence_vectorizer(x))\n",
    "        target_ds = tf.data.Dataset.from_tensor_slices(tf.constant(target_inputs)).map(lambda x: self.sequence_vectorizer(x))\n",
    "        distance_ds = tf.data.Dataset.from_tensor_slices(tf.constant(distance_inputs)).map(lambda x: encode_distance(x, self.distance_encoding_bits))\n",
    "    \n",
    "        inputs_ds = tf.data.Dataset.zip((source_ds, target_ds, distance_ds)).batch(self.batch_size)\n",
    "        \n",
    "        for inputs in inputs_ds:\n",
    "            relations = []\n",
    "            sources = []\n",
    "            targets = []\n",
    "            for model in self.models:\n",
    "                output = model(list(inputs))\n",
    "                for i, (relation, source, target) in enumerate(zip(*output)):\n",
    "                    if i == len(relations):\n",
    "                        relations.append([])\n",
    "                    relations[i].append(relation)\n",
    "                    if i == len(sources):\n",
    "                        sources.append([])\n",
    "                    sources[i].append(source)\n",
    "                    if i == len(targets):\n",
    "                        targets.append([])\n",
    "                    targets[i].append(target)\n",
    "            outputs[0].extend(relations)\n",
    "            outputs[1].extend(sources)\n",
    "            outputs[2].extend(targets)\n",
    "        return self.__decode_outputs(outputs)\n",
    "\n",
    "def build_link_prediction_model(params: dict):\n",
    "    models = params[params['model_name']]\n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    proposition_tag_vectorizer = params['proposition_tag_vectorizer']\n",
    "    relation_tag_vectorizer = params['relation_tag_vectorizer']\n",
    "    distance_encoding_bits = params['max_distance_encoded'] * 2\n",
    "    \n",
    "    \n",
    "    model = LinkPredictionModel(\n",
    "        models=models,\n",
    "        sequence_vectorizer=sequence_vectorizer,\n",
    "        proposition_tag_vectorizer=proposition_tag_vectorizer,\n",
    "        relation_tag_vectorizer=relation_tag_vectorizer,\n",
    "        distance_encoding_bits=distance_encoding_bits\n",
    "    )\n",
    "    \n",
    "    source = \"muchos años , la gente tenía que pagar una gran cantidad de dinero prar enviar sus cartas , y sus pagos estaban relacionados con el peso de sus cartas o cajas , y muchos accidentes pueden causar el problema de que el correo no se pueda entregar\"\n",
    "    target = \"electrónico puede contarse como uno de los resultados más beneficiosos de la tecnología moderna\"\n",
    "    \n",
    "    result = model([source, source, source], [target, target, target], [-1, 0 , 1])\n",
    "    \n",
    "    print(result)\n",
    "    print(len(result))\n",
    "\n",
    "    params[params['model_name'] + \"_final\"] = model\n",
    "    \n",
    "build_link_prediction_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_statistic(params: dict, model=None):\n",
    "    model = model if model else params[params['model_name'] + \"_final\"]\n",
    "    data_dataframe = params[f'raw_test_data_dataframe']\n",
    "\n",
    "    statistic = {\n",
    "        'source_prop_text': [],\n",
    "        'target_prop_text': [],\n",
    "        'source_prop_type': [],\n",
    "        'target_prop_type': [],\n",
    "        'linked': [],\n",
    "        'relation_type': [],\n",
    "        'infered_source_prop_type': [],\n",
    "        'infered_target_prop_type': [],\n",
    "        'infered_relation_type': [], \n",
    "        'infered_linked': [],\n",
    "        'distance': [],\n",
    "    }\n",
    "    \n",
    "    source_ds = tf.data.Dataset.from_tensor_slices(data_dataframe['source_prop_text'])\n",
    "    target_ds = tf.data.Dataset.from_tensor_slices(data_dataframe['target_prop_text'])\n",
    "    distance_ds = tf.data.Dataset.from_tensor_slices(list(data_dataframe['distance'].to_numpy(dtype=int)))\n",
    "    source_tag_ds = tf.data.Dataset.from_tensor_slices(data_dataframe['source_prop_type'])\n",
    "    target_tag_ds = tf.data.Dataset.from_tensor_slices(data_dataframe['target_prop_type'])\n",
    "    relation_tag_ds = tf.data.Dataset.from_tensor_slices(data_dataframe['relation_type'])\n",
    "\n",
    "    batch_num = 0\n",
    "    log_period = 10\n",
    "    for sources, targets, distances, source_tags, target_tags, relation_tags in tf.data.Dataset.zip((source_ds, target_ds, distance_ds, source_tag_ds, target_tag_ds, relation_tag_ds)).batch(32):\n",
    "        if batch_num % log_period == 0:\n",
    "            print(\"batch:\", batch_num)\n",
    "        batch_num += 1\n",
    "        \n",
    "        inference = model(sources, targets, distances)\n",
    "        \n",
    "        statistic['source_prop_text'].extend([x.numpy().decode() for x in sources])\n",
    "        statistic['target_prop_text'].extend([x.numpy().decode() for x in targets])\n",
    "        statistic['source_prop_type'].extend([x.numpy().decode() for x in source_tags])\n",
    "        statistic['target_prop_type'].extend([x.numpy().decode() for x in target_tags])\n",
    "        statistic['relation_type'].extend([x.numpy().decode() for x in relation_tags])\n",
    "        statistic['distance'].extend(distances)\n",
    "        statistic['linked'].extend([\"_Inverse\" not in x.numpy().decode() and x.numpy().decode() != \"\" for x in relation_tags])\n",
    "        \n",
    "        for relation_tag, source_tag, target_tag, linked in inference:\n",
    "            statistic['infered_source_prop_type'].append(source_tag)\n",
    "            statistic['infered_target_prop_type'].append(target_tag)\n",
    "            statistic['infered_relation_type'].append(relation_tag)\n",
    "            statistic['infered_linked'].append(linked)\n",
    "        \n",
    "        if not params['in_production']:\n",
    "            if batch_num > 10:\n",
    "                break\n",
    "    \n",
    "    statistic = pandas.DataFrame(statistic)\n",
    "    print(statistic.describe())\n",
    "    statistic_df_path = Path(params['model_path'], \"statistic.pkl\")\n",
    "    statistic.to_pickle(statistic_df_path)\n",
    "    params['statistic'] = statistic\n",
    "    \n",
    "compute_statistic(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show statistic\n",
    "\n",
    "- [ ] Calculate consistency (If support or Inverse_support are present its inverse should be present as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_statistic(params: dict, model_path=None, model_name=None):\n",
    "    base_path = Path(model_path if model_path else params['model_path'])\n",
    "    model_name = model_name if model_name else params['model_name']\n",
    "    statistic_path = base_path / \"statistic.json\"\n",
    "    statistic_df_path = base_path / \"statistic.pkl\"\n",
    "    if statistic_df_path.exists():\n",
    "        statistic = pandas.read_pickle(statistic_df_path)\n",
    "        print(\"Statistic loaded.\")\n",
    "    else:\n",
    "        statistic = params['statistic']\n",
    "    \n",
    "    def plot_confusion_matrix(true_y, pred_y, title, xticks_rotation=0, tag = \"\"):\n",
    "        ConfusionMatrixDisplay.from_predictions(true_y, pred_y, normalize=\"true\")\n",
    "        plt.xticks(rotation = xticks_rotation)\n",
    "        plt.title(title)\n",
    "        fig_path = base_path / (\"confusion_matrix_\" + tag + \"_\" + title + \".png\")\n",
    "        plt.savefig(fig_path)\n",
    "        plt.show()\n",
    "        \n",
    "    def sk_f1_report(title, y_true, y_infered):\n",
    "        print(title)\n",
    "        print(classification_report(y_true, y_infered, zero_division=0))\n",
    "        return classification_report(y_true, y_infered, zero_division=0, output_dict=True)\n",
    "    \n",
    "    def full_report(title, statistic):\n",
    "    \n",
    "        print(title)\n",
    "        print()\n",
    "\n",
    "        relation = sk_f1_report(\"Relation\", statistic['relation_type'], statistic['infered_relation_type'])\n",
    "        source = sk_f1_report(\"Source\", statistic['source_prop_type'], statistic['infered_source_prop_type'])\n",
    "        target = sk_f1_report(\"Target\", statistic['target_prop_type'], statistic['infered_target_prop_type'])\n",
    "        link = sk_f1_report(\"Link\", statistic['linked'], statistic['infered_linked'])\n",
    "\n",
    "        plot_confusion_matrix(statistic['relation_type'], statistic['infered_relation_type'], \"Relation\", xticks_rotation=45, tag=title)\n",
    "        plot_confusion_matrix(statistic['source_prop_type'], statistic['infered_source_prop_type'], \"Source\", tag=title)\n",
    "        plot_confusion_matrix(statistic['target_prop_type'], statistic['infered_target_prop_type'], \"Target\", tag=title)\n",
    "        plot_confusion_matrix(statistic['linked'], statistic['infered_linked'], \"Link\", tag=title)\n",
    "\n",
    "        return {\n",
    "            \"relation\": relation,\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "            \"link\": link,\n",
    "        }\n",
    "    \n",
    "    all_relations = full_report(\"all_relations\", statistic)\n",
    "\n",
    "    only_straight_relations = statistic[(~statistic['relation_type'].str.endswith(\"_Inverse\"))]\n",
    "    only_straight_relations['infered_relation_type'] = only_straight_relations['infered_relation_type'].map(lambda x: \"\" if x.endswith(\"_Inverse\") else x)\n",
    "    # If a relation r_Inverse is predicted, then it will count as an empty relation.\n",
    "    inverse_no_count_relations = full_report(\"forward_relations\", only_straight_relations)\n",
    "\n",
    "    only_straight_relations = statistic[~statistic['relation_type'].str.endswith(\"_Inverse\")]\n",
    "    only_straight_relations['infered_relation_type'] = only_straight_relations['infered_relation_type'].map(lambda x: x.replace(\"_Inverse\", \"\") if x.endswith(\"_Inverse\") else x)\n",
    "    # If a relation r_Inverse is predicted, then it will count as a r relation. \n",
    "    bidirectional_relations = full_report(\"forward_relations_with_inverse_prediction_without_Inverse\", only_straight_relations)\n",
    "\n",
    "    with statistic_path.open(\"w\") as file:\n",
    "        json.dump({\n",
    "            \"all_relations\": all_relations,\n",
    "            \"inverse_no_count_relations\": inverse_no_count_relations,\n",
    "            \"bidirectional_relations\": bidirectional_relations\n",
    "        }, file)\n",
    "        \n",
    "show_statistic(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cross_model(params: dict):\n",
    "    \n",
    "    models = []\n",
    "    ensemble = {\n",
    "        \"model\": [0],\n",
    "        \"model_attention\": [0],\n",
    "        \"model_attention_changes2\": [0,1],\n",
    "    }\n",
    "    mixed_model_name = \"_\".join(f\"{name}_{i}\" for name in ensemble for i in ensemble[name])\n",
    "    \n",
    "    # Loading Models\n",
    "    \n",
    "    for model_name in ensemble:\n",
    "        for i in ensemble[model_name]:\n",
    "            model_path = Path(params[\"export_path\"], model_name, f\"{model_name}_{i}\")\n",
    "            if model_path.exists():\n",
    "                model = keras.models.load_model(str(model_path))\n",
    "                models.append(model)\n",
    "            else:\n",
    "                print(f\"Model in {model_path} doesn't exist\")\n",
    "    \n",
    "    # Building Model\n",
    "    \n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    proposition_tag_vectorizer = params['proposition_tag_vectorizer']\n",
    "    relation_tag_vectorizer = params['relation_tag_vectorizer']\n",
    "    distance_encoding_bits = params['max_distance_encoded'] * 2\n",
    "    \n",
    "    model = LinkPredictionModel(\n",
    "        models=models,\n",
    "        sequence_vectorizer=sequence_vectorizer,\n",
    "        proposition_tag_vectorizer=proposition_tag_vectorizer,\n",
    "        relation_tag_vectorizer=relation_tag_vectorizer,\n",
    "        distance_encoding_bits=distance_encoding_bits\n",
    "    )\n",
    "    \n",
    "    source = \"muchos años , la gente tenía que pagar una gran cantidad de dinero prar enviar sus cartas , y sus pagos estaban relacionados con el peso de sus cartas o cajas , y muchos accidentes pueden causar el problema de que el correo no se pueda entregar\"\n",
    "    target = \"electrónico puede contarse como uno de los resultados más beneficiosos de la tecnología moderna\"\n",
    "    \n",
    "    result = model([source, source, source], [target, target, target], [-1, 0 , 1])\n",
    "    \n",
    "    print(result)\n",
    "    print(len(result))\n",
    "  \n",
    "    params[\"mixed_model\"] = model\n",
    "    \n",
    "    model_path = Path(params['export_path'], mixed_model_name)\n",
    "    model_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    compute_statistic(params, model=model)\n",
    "    show_statistic(params, model_path=model_path, model_name=mixed_model_name)\n",
    "\n",
    "    \n",
    "build_cross_model(params)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_multiple_bar_plot(labels: list, values: dict, width=0.35, save: Path = None, legend_map: dict=None, skip=2):\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects = []\n",
    "    \n",
    "    for i, label in enumerate(values):\n",
    "        values_values = values[label]\n",
    "        diff = width / len(values) \n",
    "        label = \"_\".join(label.split(\"_\")[skip:])\n",
    "        label = label.replace(\"mean\", \"promedio\")\n",
    "        label = label.replace(\"source\", \"origen\")\n",
    "        label = label.replace(\"target\", \"objetivo\")\n",
    "        label = label.replace(\"relations\", \"relaciones\")\n",
    "        label = label.replace(\"relation\", \"relación\")\n",
    "        label = label.replace(\"loss\", \"pérdida\")\n",
    "        label = label.replace(\"linked\", \"enlazado\")\n",
    "        label = label.replace(\"_\", \" \")\n",
    "\n",
    "        rect = ax.bar(x + diff * i, [round(x,2) for x in values_values], diff, label=label)\n",
    "        rects.append(rect)\n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Valor')\n",
    "#     ax.set_ylabel('Values')\n",
    "    ax.set_title('Métricas por modelos')\n",
    "#     ax.set_title('Metrics per Model')\n",
    "    labels = [legend_map.get(label, label) if legend_map else label for label in labels]\n",
    "    ax.set_xticks(x, labels, rotation=0)\n",
    "    ax.legend(loc=\"lower left\")\n",
    "\n",
    "    for rect in rects:\n",
    "        ax.bar_label(rect, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def joint_metrics(params: dict, use_statistic=True, use_history=True, use_evaluation=True):\n",
    "    models_path = Path(params['export_path'])\n",
    "           \n",
    "    relation_tags = params['relation_tags'] + [\"\"]\n",
    "    propositions_tags = params['proposition_tags']\n",
    "\n",
    "    name_map = {\n",
    "    }\n",
    "    \n",
    "    def plot_history(history, key, model_name, legend_key=None):\n",
    "        model_name = name_map.get(model_name, model_name)\n",
    "        legend_key = legend_key if legend_key else key\n",
    "#         plt.plot(history[key], label=f'train {legend_key} {model_name}')\n",
    "        plt.plot(history[key], label=f'{legend_key} entrenamiento {model_name}')\n",
    "#         plt.plot(history[f'val_{key}'], label=f'val {legend_key} {model_name}')\n",
    "        plt.plot(history[f'val_{key}'], label=f'{legend_key} validación {model_name}')\n",
    "\n",
    "    def show_plot(title, base_path: Path):\n",
    "        plt.title(title)\n",
    "        plt.ylabel('Entropía cruzada')\n",
    "#         plt.ylabel('Cross entropy value')\n",
    "        plt.xlabel('No. época')\n",
    "#         plt.xlabel('No. epoch')\n",
    "        plt.legend()\n",
    "        plt.savefig(base_path / f\"{title}.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    models_info = {\n",
    "    }\n",
    "    \n",
    "    for model_path in sorted(models_path.iterdir(), key=lambda x: x.name):\n",
    "        if model_path.name[0] == \"_\": continue\n",
    "        \n",
    "        \n",
    "        if model_path.is_dir():\n",
    "            model_name = model_path.name\n",
    "#             name_map[model_name] = f\"model {len(name_map) + 1}\"\n",
    "            name_map[model_name] = f\"modelo {len(name_map) + 1}\"\n",
    "            statistic = model_path / \"statistic.json\"\n",
    "            statistic = json.loads(statistic.read_text())\n",
    "            histories = []\n",
    "            evaluations = []\n",
    "            for i in range(1000):\n",
    "                history_path = model_path / f\"{model_name}_{i}_history.json\"\n",
    "                if history_path.exists():\n",
    "                    histories.append(json.loads(history_path.read_text()))\n",
    "                else:\n",
    "                    break\n",
    "                evaluation_path = model_path / f\"test_result_{i}.json\"\n",
    "                if evaluation_path.exists():\n",
    "                    evaluations.append(json.loads(evaluation_path.read_text()))\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            models_info[model_name] = {\n",
    "                \"statistic\": statistic,\n",
    "                \"histories\": histories,\n",
    "                \"evaluations\": evaluations,\n",
    "            }\n",
    "            \n",
    "    print(name_map)\n",
    "    \n",
    "    def plot_joint_history_keys(keys):\n",
    "        histories = [(model_name, models_info[model_name]['histories']) for model_name in models_info]\n",
    "        for key in keys:\n",
    "            print(key)\n",
    "            for i in range(1000):\n",
    "                some = False\n",
    "                true_key = key.format_map({'i': i})\n",
    "                for model_name, model_histories in histories:\n",
    "                    if i < len(model_histories):\n",
    "                        history = model_histories[i]\n",
    "                        legend_key = \"pérdida\"\n",
    "#                         legend_key = None\n",
    "                        plot_history(history, true_key, model_name, legend_key)\n",
    "                        some = True\n",
    "                if not some:\n",
    "                    break\n",
    "                else:\n",
    "                    title = f\"{true_key}_ensemble_{i}\"\n",
    "                    title = title.replace(\"relation\", \"relación\")\n",
    "                    title = title.replace(\"loss\", \"pérdida\")\n",
    "                    title = title.replace(\"ensemble\", \"conjunto\")\n",
    "                    title = title.replace(\"_\", \" \")\n",
    "                    show_plot(title=title, base_path=models_path)\n",
    "    \n",
    "    if use_history:\n",
    "        keys = [\n",
    "            \"loss\",\n",
    "            \"relation_{i}_loss\",\n",
    "            \"source_{i}_loss\",\n",
    "            \"target_{i}_loss\",\n",
    "        ]\n",
    "        plot_joint_history_keys(keys)\n",
    "    \n",
    "    analysis = [\n",
    "        \"all_relations\",\n",
    "        \"inverse_no_count_relations\",\n",
    "    ]\n",
    "    metrics = [\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f1-score\"\n",
    "    ]\n",
    "    macro_metrics = [\n",
    "        \"precision\",\n",
    "        \"precision\",\n",
    "        \"precision\",\n",
    "    ]\n",
    "    \n",
    "    statistic_df = {\n",
    "        \"model_name\": [],\n",
    "        **{f\"{a}_relation_{tag}_{metric}\": [] for tag in relation_tags for metric in metrics for a in analysis},\n",
    "        **{f\"{a}_source_{tag}_{metric}\": [] for tag in propositions_tags for metric in metrics for a in analysis},\n",
    "        **{f\"{a}_target_{tag}_{metric}\": [] for tag in propositions_tags for metric in metrics for a in analysis},\n",
    "        **{f\"{a}_relation_macro_{metric}\": [] for metric in metrics for a in analysis},\n",
    "        **{f\"{a}_source_macro_{metric}\": [] for metric in metrics for a in analysis},\n",
    "        **{f\"{a}_target_macro_{metric}\": [] for metric in metrics for a in analysis},\n",
    "        **{f\"{a}_linked_macro_{metric}\": [] for metric in metrics for a in analysis},\n",
    "        **{f\"{a}_relation_accuracy\": [] for a in analysis},\n",
    "        **{f\"{a}_source_accuracy\": [] for a in analysis},\n",
    "        **{f\"{a}_target_accuracy\": [] for a in analysis},\n",
    "        **{f\"{a}_linked_accuracy\": [] for a in analysis},\n",
    "        \"train_mean_loss\": [],\n",
    "        \"train_mean_relation_loss\": [],\n",
    "        \"train_mean_source_loss\": [],\n",
    "        \"train_mean_target_loss\": [],\n",
    "        \"train_mean_relation_acc\": [],\n",
    "        \"train_mean_source_acc\": [],\n",
    "        \"train_mean_target_acc\": [],\n",
    "        \"train_mean_relation_f1\": [],\n",
    "        \"train_mean_source_f1\": [],\n",
    "        \"train_mean_target_f1\": [],\n",
    "    }\n",
    "    \n",
    "    for model_name in models_info:\n",
    "        info = models_info[model_name]['statistic']\n",
    "        train_eval = models_info[model_name]['evaluations']\n",
    "        \n",
    "        statistic_df[\"model_name\"].append(model_name)\n",
    "        if use_statistic:\n",
    "            for a in analysis:\n",
    "                statistic_df[f\"{a}_relation_accuracy\"].append(info[a]['relation'][\"accuracy\"])\n",
    "                statistic_df[f\"{a}_source_accuracy\"].append(info[a]['source'][\"accuracy\"])\n",
    "                statistic_df[f\"{a}_target_accuracy\"].append(info[a]['target'][\"accuracy\"])\n",
    "                statistic_df[f\"{a}_linked_accuracy\"].append(info[a]['link'][\"accuracy\"])\n",
    "\n",
    "                for metric in metrics:\n",
    "                    statistic_df[f\"{a}_relation_macro_{metric}\"].append(info[a]['relation'][\"macro avg\"][metric])\n",
    "                    statistic_df[f\"{a}_source_macro_{metric}\"].append(info[a]['source'][\"macro avg\"][metric])\n",
    "                    statistic_df[f\"{a}_target_macro_{metric}\"].append(info[a]['target'][\"macro avg\"][metric])\n",
    "                    statistic_df[f\"{a}_linked_macro_{metric}\"].append(info[a]['link'][\"macro avg\"][metric])\n",
    "\n",
    "                    for tag in relation_tags:\n",
    "                        try:\n",
    "                            statistic_df[f\"{a}_relation_{tag}_{metric}\"].append(info[a]['relation'][tag][metric])\n",
    "                        except:\n",
    "                            statistic_df[f\"{a}_relation_{tag}_{metric}\"].append(0)\n",
    "                    for tag in propositions_tags:\n",
    "                        try:\n",
    "                            statistic_df[f\"{a}_source_{tag}_{metric}\"].append(info[a]['source'][tag][metric])\n",
    "                        except:\n",
    "                            statistic_df[f\"{a}_source_{tag}_{metric}\"].append(0)\n",
    "                        try:\n",
    "                            statistic_df[f\"{a}_target_{tag}_{metric}\"].append(info[a]['target'][tag][metric])\n",
    "                        except:\n",
    "                            statistic_df[f\"{a}_target_{tag}_{metric}\"].append(0)\n",
    "    \n",
    "        eval_keys = [\n",
    "            (\"loss\", \"train_mean_loss\"),\n",
    "            (\"relation_{i}_loss\", \"train_mean_relation_loss\"),\n",
    "            (\"source_{i}_loss\", \"train_mean_source_loss\"),\n",
    "            (\"target_{i}_loss\", \"train_mean_target_loss\"),\n",
    "            (\"relation_{i}_acc\", \"train_mean_relation_acc\"),\n",
    "            (\"source_{i}_acc\", \"train_mean_source_acc\"),\n",
    "            (\"target_{i}_acc\", \"train_mean_target_acc\"),\n",
    "            (\"relation_{i}_relation_{i}F1Macro\", \"train_mean_relation_f1\"),\n",
    "            (\"source_{i}_source_{i}F1Macro\", \"train_mean_source_f1\"),\n",
    "            (\"target_{i}_target_{i}F1Macro\", \"train_mean_target_f1\"),\n",
    "        ]\n",
    "        if use_evaluation:\n",
    "            for key, save_key in eval_keys:\n",
    "                value = 0\n",
    "                for i in range(len(train_eval)):\n",
    "                    true_key = key.format_map({'i': i})\n",
    "                    value += train_eval[i][true_key]\n",
    "                if train_eval:\n",
    "                    value /= len(train_eval)\n",
    "                statistic_df[save_key].append(value)\n",
    "    \n",
    "    model_names = [x for x in models_info]\n",
    "    \n",
    "    if use_statistic:\n",
    "    \n",
    "        keys = [\n",
    "            *[f\"{a}_relation_macro_{metric}\" for metric in [\"f1-score\"] for a in [\"all_relations\"]],\n",
    "            *[f\"{a}_source_macro_{metric}\" for metric in [\"f1-score\"] for a in [\"all_relations\"]],\n",
    "            *[f\"{a}_target_macro_{metric}\" for metric in [\"f1-score\"] for a in [\"all_relations\"]],\n",
    "        ]\n",
    "        save = models_path / \"all_relation_f1_scores.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map)\n",
    "\n",
    "        keys = [\n",
    "            *[f\"{a}_linked_{metric}\" for metric in [\"accuracy\"] for a in [\"all_relations\"]],\n",
    "            *[f\"{a}_linked_macro_{metric}\" for metric in [\"f1-score\"] for a in [\"all_relations\"]],\n",
    "        ]\n",
    "        save = models_path / \"all_relation_linked.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map)\n",
    "\n",
    "        keys = [\n",
    "            *[f\"{a}_relation_{metric}\" for metric in [\"accuracy\"] for a in [\"all_relations\"]],\n",
    "            *[f\"{a}_source_{metric}\" for metric in [\"accuracy\"] for a in [\"all_relations\"]],\n",
    "            *[f\"{a}_target_{metric}\" for metric in [\"accuracy\"] for a in [\"all_relations\"]],\n",
    "        ]\n",
    "        save = models_path / \"all_relation_accuracy.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map)\n",
    "\n",
    "        keys = [\n",
    "            *[f\"{a}_relation_{tag}_{metric}\" for tag in relation_tags for metric in [\"f1-score\"] for a in [\"all_relations\"]]\n",
    "        ]\n",
    "        save = models_path / \"all_relation_relations_f1_scores.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map)\n",
    "\n",
    "        keys = [\n",
    "            *[f\"{a}_source_{tag}_{metric}\" for tag in propositions_tags for metric in [\"f1-score\"] for a in [\"all_relations\"]]\n",
    "        ]\n",
    "        save = models_path / \"all_relation_source_f1_scores.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map)\n",
    "\n",
    "        keys = [\n",
    "            *[f\"{a}_target_{tag}_{metric}\" for tag in propositions_tags for metric in [\"f1-score\"] for a in [\"all_relations\"]]\n",
    "        ]\n",
    "        save = models_path / \"all_relation_target_f1_scores.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map)\n",
    "    \n",
    "    if use_evaluation:\n",
    "        keys = [\n",
    "            \"train_mean_loss\",\n",
    "            \"train_mean_relation_loss\",\n",
    "            \"train_mean_source_loss\",\n",
    "            \"train_mean_target_loss\",\n",
    "        ]\n",
    "        save = models_path / \"evaluation_losses.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map, skip=0)\n",
    "\n",
    "        keys = [\n",
    "            \"train_mean_relation_acc\",\n",
    "            \"train_mean_source_acc\",\n",
    "            \"train_mean_target_acc\",\n",
    "        ]\n",
    "        save = models_path / \"evaluation_acc.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map, skip=0)\n",
    "\n",
    "        keys = [\n",
    "            \"train_mean_relation_f1\",\n",
    "            \"train_mean_source_f1\",\n",
    "            \"train_mean_target_f1\",\n",
    "        ]\n",
    "        save = models_path / \"evaluation_f1.png\"\n",
    "        build_multiple_bar_plot(model_names, {key: statistic_df[key] for key in keys}, width=0.8, save=save, legend_map=name_map, skip=0)\n",
    "    \n",
    "    \n",
    "joint_metrics(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_build_model_from_params(params: dict):\n",
    "    extract_propositions(params)\n",
    "    load_saved_model(params)\n",
    "    build_link_prediction_model(params)\n",
    "    return params[params['model_name'] + \"_final\"]\n",
    "\n",
    "load_and_build_model_from_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(params: dict):\n",
    "    extract_propositions(params)\n",
    "    creating_glove_embeddings(params)\n",
    "    encode_datasets(params)\n",
    "    build_model(params)\n",
    "    train_and_save_model(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_file(params: dict, content: str, target_file: Path = None, source_language: str=\"english\", source_file: str = None, **kwargs):\n",
    "    model = params[params['model_name'] + \"_final\"]\n",
    "    max_argumentative_distance = params['max_proposition_distance']\n",
    "    \n",
    "#     print(\"Processing\", file)\n",
    "    \n",
    "    parser = ConllParser(bioes=True)\n",
    "    argumentative, _, non_argumentative = parser.parse(content)\n",
    "\n",
    "    source_ids = []\n",
    "    target_ids = []\n",
    "    sources = []\n",
    "    targets = []\n",
    "    distances = []\n",
    "    \n",
    "    predictions: Dict[Tuple[int,int], Tuple[str,str,str]] = {\n",
    "            \n",
    "    }\n",
    "\n",
    "    for _, source in argumentative.iterrows():\n",
    "        for _, target in argumentative.iterrows():\n",
    "            distance = target['prop_id'] - source['prop_id']\n",
    "            if source['prop_id'] == target['prop_id'] or abs(distance) > max_argumentative_distance:\n",
    "                continue\n",
    "            source_ids.append(source['prop_id'])\n",
    "            target_ids.append(target['prop_id'])\n",
    "            sources.append(source['prop_text'])\n",
    "            targets.append(target['prop_text'])\n",
    "            distances.append(distance)\n",
    "\n",
    "    inference = model(sources, targets, distances)\n",
    "\n",
    "    for source_id, target_id, (predicted_relation_tag, predicted_source_tag, predicted_target_tag, linked) in zip(source_ids, target_ids, inference):\n",
    "        if linked:\n",
    "            if predicted_relation_tag == \"\" or predicted_relation_tag.endswith(\"_Inverse\"):\n",
    "                print(\"WARNING: Linked prediction with invalid relation tag. Ignoring relation.\")\n",
    "            elif predicted_relation_tag:\n",
    "                predictions[source_id, target_id] = predicted_relation_tag, predicted_source_tag, predicted_target_tag\n",
    "#     # Remove inverse relations. If the forward relation doesn't exist then\n",
    "#     # it will be added in either case the inverse relation will be removed.\n",
    "#     to_add = {}\n",
    "#     to_remove = set()\n",
    "#     for (source_id, target_id), (predicted_relation_tag, predicted_source_tag, predicted_target_tag) in predictions.items():\n",
    "#         inverse_relation = predicted_relation_tag.endswith(\"_Inverse\")\n",
    "#         if inverse_relation:\n",
    "#             try:\n",
    "#                 _ = predictions[target_id, source_id]\n",
    "#             except KeyError:\n",
    "#                 # Add the forward relation\n",
    "#                 no_inverse_tag = predicted_relation_tag[:-len(\"_Inverse\")]\n",
    "#                 to_add[target_id, source_id] = no_inverse_tag, predicted_target_tag, predicted_source_tag\n",
    "#             # Remove inverse relation\n",
    "#             to_remove.add((source_id, target_id))\n",
    "\n",
    "#     # Commit actions to predictions\n",
    "#     for key in to_remove:\n",
    "#         predictions.pop(key)\n",
    "#     predictions.update(to_add)\n",
    "\n",
    "    # Empty relations table and fill with calculated values\n",
    "    relation_dict = {\n",
    "        'relation_id': [],\n",
    "        'relation_type': [],\n",
    "        'prop_id_source': [],\n",
    "        'prop_id_target': [],\n",
    "    }\n",
    "    relation_id = 1\n",
    "    for (source_id, target_id), (predicted_relation_tag, predicted_source_tag, predicted_target_tag) in predictions.items():\n",
    "        relation_dict['relation_id'].append(relation_id)\n",
    "        relation_dict['relation_type'].append(predicted_relation_tag)\n",
    "        relation_dict['prop_id_source'].append(source_id)\n",
    "        relation_dict['prop_id_target'].append(target_id)\n",
    "        relation_id += 1\n",
    "    relations = pandas.DataFrame(relation_dict)\n",
    "\n",
    "    file_key = \"str(file)\" if not source_file else source_file\n",
    "#     print(\"Parsing from dataframe\")\n",
    "#     print(\"Relations:\", len(relations))\n",
    "    result = parser.from_dataframes({file_key: (argumentative, relations, non_argumentative)}, source_language=source_language, **kwargs)\n",
    "    \n",
    "    if target_file:\n",
    "        target_file.write_text(result[file_key][0])\n",
    "\n",
    "    return result[file_key][0]\n",
    "\n",
    "def use_model(params: dict):\n",
    "    to_process_dir = params['to_process_data_path']\n",
    "    processed_data_dir = params['processed_data_path']\n",
    "    \n",
    "    segmentation_models = sorted(list(os.walk(to_process_dir))[0][1])\n",
    "    print(segmentation_models)\n",
    "    \n",
    "    for segmentation_model in segmentation_models:\n",
    "        \n",
    "        base_path = Path(processed_data_dir) / params['model_name']\n",
    "        base_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        current_to_process_dir = Path(to_process_dir, segmentation_model)\n",
    "        corpus_labels = sorted(list(os.walk(current_to_process_dir))[0][1])\n",
    "        print(corpus_labels)\n",
    "        \n",
    "        for corpus_label in corpus_labels:\n",
    "            final_current_to_process_dir = current_to_process_dir / corpus_label\n",
    "            dest_folder = base_path / segmentation_model / corpus_label\n",
    "            dest_folder.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "            for file in final_current_to_process_dir.iterdir():\n",
    "                if file.exists() and file.is_file():\n",
    "                    dest_file = dest_folder / file.name\n",
    "                    dest_file.touch(exist_ok=True)\n",
    "                    process_file(params, file.read_text(), dest_file)\n",
    "            \n",
    "use_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export jupyter as module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    try:\n",
    "        if Path(__file__).suffix == \".ipynb\":\n",
    "            raise NameError()\n",
    "    except NameError:\n",
    "        # In Jupyer Notebook\n",
    "        from utils.notebook_utils import export_notebook_as_module\n",
    "        export_notebook_as_module(Path(\"link_prediction.ipynb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45db824c686f92ff5a2120110a200c79437caa527895339497af8122a932288b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}