{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Segmentation Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    BASE_PATH = str(Path(__file__, \"..\", \"..\", \"..\").resolve())\n",
    "    import sys\n",
    "except NameError:\n",
    "    import sys\n",
    "    BASE_PATH = str(Path(\"..\", \"..\").resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if BASE_PATH not in sys.path:\n",
    "        sys.path.insert(0, BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pos_tagger.pos_tagger import NLTKPOSTagger, SpacyPOSTagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# INFO_TAG = \"persuasive_essays_paragraph\"\n",
    "INFO_TAG = \"persuasive_essays_paragraph_all_linked\"\n",
    "# INFO_TAG = \"cdcp\"\n",
    "# INFO_TAG = \"drinventor\"\n",
    "# INFO_TAG = \"abstrct\"\n",
    "\n",
    "\n",
    "# LANGUAGE = \"english\"\n",
    "LANGUAGE = \"spanish\"\n",
    "if LANGUAGE == \"english\":\n",
    "    GLOVE_DIR = Path(BASE_PATH, \"data/\", 'glove.840B.300d.txt')\n",
    "elif LANGUAGE == \"spanish\":\n",
    "    GLOVE_DIR = Path(BASE_PATH, \"data/\", 'glove-sbwc.i25.vec')\n",
    "else:\n",
    "    raise Exception(f\"Language {LANGUAGE} not supported\")\n",
    "\n",
    "SOURCE_DATADIR = Path(BASE_PATH, f\"data/projection/{INFO_TAG}\").resolve()\n",
    "DATA_DIR = Path(BASE_PATH, f\"data/segmenter_corpus/{INFO_TAG}\").resolve()\n",
    "TO_PROCESS_DATADIR = Path(BASE_PATH, f\"data/to_process\").resolve()\n",
    "PROCESSED_DATADIR = Path(BASE_PATH, f\"data/segmenter_processed/{INFO_TAG}\").resolve()\n",
    "\n",
    "params = {\n",
    "    # POS Tagger\n",
    "    \"pos_tagger\": \"nltk\", # Can be nltk or spacy\n",
    "    \n",
    "    \n",
    "    # POS Hyperparameters\n",
    "    'with_pos': True,\n",
    "    'pos_units': 5,\n",
    "    'pos_activation': 'relu',\n",
    " \n",
    "    # Model Hyperparameters\n",
    "    'dim': 300,\n",
    "    'dropout': 0.5,\n",
    "    'epochs': 120,\n",
    "    'batch_size': 20,\n",
    "    'lstm_size': 200,\n",
    "    'optimizer': 'adam',\n",
    "    'trainable_word_emeddings': False,\n",
    "    'regularization': \"l2\",\n",
    "    \n",
    "    # ResNet\n",
    "    'with_resnet': True,\n",
    "    \n",
    "    # Layer Normalization\n",
    "    'with_layer_normalization': True,\n",
    "    \n",
    "    # Early Stopping\n",
    "    'with_early_stopping': True,\n",
    "    'early_stopping_monitor': 'val_crf_loss',\n",
    "    'early_stopping_patience': 10,\n",
    "    'restore_best_weights': True,\n",
    "    \n",
    "    # CNN Char Hyperparameters\n",
    "    'with_cnn': True,\n",
    "    'dim_chars': 50,\n",
    "    'filters': 30,\n",
    "    'kernel_size': 3,\n",
    "    \n",
    "    # LSTM Char Hyperparameters\n",
    "    'with_lstm': True,\n",
    "    'dim_chars_lstm': 25,\n",
    "    \n",
    "    # Dense Layer Hyperparameters\n",
    "    'with_dn': True,\n",
    "    'dense_units': 100,\n",
    "    'dense_activation': \"relu\",\n",
    "    \n",
    "    # Vectorizer Hyperparameters\n",
    "    'standardize': \"lower\", \n",
    "    'split': \"whitespace\",\n",
    "    'max_seq_size': 800,\n",
    "    \n",
    "    # Corpus info\n",
    "#     'corpus_type': \"sentence\",\n",
    "    'corpus_type': \"paragraph\", \n",
    "    'language': LANGUAGE,\n",
    "#     'meta_tags_level': 0, # BIOES \n",
    "    'meta_tags_level': 1, # BIOES-MetaTag\n",
    "    'words_path': str(Path(DATA_DIR, f'{LANGUAGE}_vocab.words.txt')),\n",
    "    'chars_path': str(Path(DATA_DIR, f'{LANGUAGE}_vocab.chars.txt')),\n",
    "    'tags_path': str(Path(DATA_DIR, f'{LANGUAGE}_vocab.tags.txt')),\n",
    "    'pos_path': str(Path(DATA_DIR, f'{LANGUAGE}_vocab.pos.txt')),\n",
    "    'sequences_path': (str(Path(DATA_DIR, f'{LANGUAGE}_train.words.txt')), str(Path(DATA_DIR, f'{LANGUAGE}_testa.words.txt')), str(Path(DATA_DIR, f'{LANGUAGE}_testb.words.txt'))),\n",
    "    'labels_path': (str(Path(DATA_DIR, f'{LANGUAGE}_train.tags.txt')), str(Path(DATA_DIR, f'{LANGUAGE}_testa.tags.txt')), str(Path(DATA_DIR, f'{LANGUAGE}_testb.tags.txt'))),\n",
    "    'pos_labels_path': (str(Path(DATA_DIR, f'{LANGUAGE}_train.pos.txt')), str(Path(DATA_DIR, f'{LANGUAGE}_testa.pos.txt')), str(Path(DATA_DIR, f'{LANGUAGE}_testb.pos.txt'))),\n",
    "    'char_vectorization_path': str(Path(DATA_DIR, f'{LANGUAGE}_char_vectorization.npz')),\n",
    "    'glove_raw_path': str(Path(GLOVE_DIR)),\n",
    "\n",
    "    # Data info\n",
    "    'source_data_path': str(Path(SOURCE_DATADIR)), # Directory with conll annotated texts\n",
    "    'data_path': str(Path(DATA_DIR)), # Directory with the segmenter corpus\n",
    "    'to_process_data_path': str(Path(TO_PROCESS_DATADIR)), # Directory with text to be processed\n",
    "    'processed_data_path': str(Path(PROCESSED_DATADIR)), # Directory to save the processed data\n",
    "}\n",
    "\n",
    "MODEL_NAME = params['language'] + \\\n",
    "    (\"_pos\" if params['with_pos'] else \"\") + \\\n",
    "    \"_model\" + \\\n",
    "    (\"_cnn\" if params['with_cnn'] else \"\") + \\\n",
    "    (\"_lstm\" if params['with_lstm'] else \"\") + \\\n",
    "    \"_blstm\" + \\\n",
    "    (\"_resnet\" if params['with_resnet'] else \"\") + \\\n",
    "    (\"_norm\" if params['with_layer_normalization'] else \"\") + \\\n",
    "    (\"_dn\" if params['with_dn'] else \"\") + \\\n",
    "    (f\"_{params['standardize']}\" if params['standardize'] else \"\") + \\\n",
    "    \"_crf\"\n",
    "\n",
    "params.update({\n",
    "    'model_name': MODEL_NAME,\n",
    "    \n",
    "    # Model serialization info\n",
    "    'model_path': str(Path(DATA_DIR, MODEL_NAME)),\n",
    "    'full_model_path': str(Path(DATA_DIR, MODEL_NAME)),\n",
    "    'model_histories_path': str(Path(DATA_DIR, MODEL_NAME)),\n",
    "    'checkpoint_path': str(Path(Path(DATA_DIR, MODEL_NAME, \"checkpoints\"))),\n",
    "    'glove_path': str(Path(DATA_DIR, f'glove_{LANGUAGE}.npz' if not params['standardize'] else f'glove_{params[\"standardize\"]}_{LANGUAGE}.npz')),\n",
    "})\n",
    "\n",
    "\n",
    "# Non serializable check\n",
    "if params['pos_tagger'] not in ['nltk', 'spacy']:\n",
    "    raise Exception(\"Invalid POS Tagguer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_save_params(params: dict, save_params=True, load_params=False):\n",
    "    if load_params:\n",
    "        params = json.load(Path(params['model_path'], \"params.json\").open(\"r\"))\n",
    "    elif save_params:\n",
    "        Path(params['model_path']).mkdir(exist_ok=True)\n",
    "        json.dump(params, Path(params['model_path'], \"params.json\").open(\"w\"))\n",
    "\n",
    "load_save_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Corpus\n",
    "\n",
    "Creates the corpus for the model based on the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare model corpus\n",
    "from segmenter.models.segmenter_exporter import export_directory, export_files\n",
    "\n",
    "def create_segmenter_corpus(params: dict, force=False):\n",
    "    corpus_type = params['corpus_type']\n",
    "    if not force and Path(params['sequences_path'][0]).exists(): return\n",
    "    if corpus_type == \"paragraph\":\n",
    "        export_directory(Path(params['source_data_path']), \n",
    "                         Path(params['data_path']), \n",
    "                         language=params['language'],\n",
    "                         meta_tags_level=params['meta_tags_level'])\n",
    "    elif corpus_type == \"sentence\":\n",
    "        # For sentences\n",
    "        export_files(Path(params['source_data_path']), \n",
    "                     Path(params['data_path']), \n",
    "                     language=params['language'],\n",
    "                     meta_tags_level=params['meta_tags_level'])\n",
    "    else:\n",
    "        print(f\"WARNING: {corpus_type} is an invalid corpus_type\")\n",
    "\n",
    "# create_segmenter_corpus(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Vectorizers\n",
    "\n",
    "Add the datasets and vectorizers. Also other values are computed such as char amount, word amount, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_datasets(params: dict):\n",
    "    \n",
    "    def configure_dataset(dataset):\n",
    "        return dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    def get_length_dataset(dataset):\n",
    "        return int(dataset.reduce(initial_state=0, reduce_func=lambda x,y: x + 1))\n",
    "    \n",
    "    # Creating datasets\n",
    "    \n",
    "    train_sequences_dataset = configure_dataset(tf.data.TextLineDataset([params['sequences_path'][0]]))\n",
    "    testa_sequences_dataset = configure_dataset(tf.data.TextLineDataset([params['sequences_path'][1]]))\n",
    "    testb_sequences_dataset = configure_dataset(tf.data.TextLineDataset([params['sequences_path'][2]]))\n",
    "    \n",
    "    train_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['labels_path'][0]]))\n",
    "    testa_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['labels_path'][1]]))\n",
    "    testb_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['labels_path'][2]]))\n",
    "    \n",
    "    train_pos_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['pos_labels_path'][0]]))\n",
    "    testa_pos_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['pos_labels_path'][1]]))\n",
    "    testb_pos_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['pos_labels_path'][2]]))\n",
    "    \n",
    "    if params['with_pos']:\n",
    "        train_sequences_dataset = tf.data.Dataset.zip((train_sequences_dataset, train_pos_labels_dataset))\n",
    "        testa_sequences_dataset = tf.data.Dataset.zip((testa_sequences_dataset, testa_pos_labels_dataset))\n",
    "        testb_sequences_dataset = tf.data.Dataset.zip((testb_sequences_dataset, testb_pos_labels_dataset))\n",
    "    \n",
    "    chars_dataset = configure_dataset(tf.data.TextLineDataset([params['chars_path']]))\n",
    "    words_dataset = configure_dataset(tf.data.TextLineDataset([params['words_path']]))\n",
    "    tags_dataset = configure_dataset(tf.data.TextLineDataset([params['tags_path']]))\n",
    "    pos_dataset = configure_dataset(tf.data.TextLineDataset([params['pos_path']]))\n",
    "    \n",
    "    # Saving datasets\n",
    "    \n",
    "    params[\"train_sequences\"] = train_sequences_dataset\n",
    "    params[\"testa_sequences\"] = testa_sequences_dataset\n",
    "    params[\"testb_sequences\"] = testb_sequences_dataset\n",
    "    \n",
    "    params[\"train_labels\"] = train_labels_dataset\n",
    "    params[\"testa_labels\"] = testa_labels_dataset\n",
    "    params[\"testb_labels\"] = testb_labels_dataset\n",
    "    \n",
    "    params[\"train_pos_labels\"] = train_pos_labels_dataset\n",
    "    params[\"testa_pos_labels\"] = testa_pos_labels_dataset\n",
    "    params[\"testb_pos_labels\"] = testb_pos_labels_dataset\n",
    "    \n",
    "    params[\"chars\"] = chars_dataset\n",
    "    params[\"words\"] = words_dataset\n",
    "    params[\"tags\"] = tags_dataset\n",
    "    params[\"pos\"] = pos_dataset\n",
    "    \n",
    "    # Computing dataset information\n",
    "    \n",
    "    params['tag_amount'] = len([x for x in Path(params['tags_path']).read_text().split(\"\\n\") if x])#get_length_dataset(tags_dataset)\n",
    "    params['char_amount'] = len([x for x in Path(params['chars_path']).read_text().split(\"\\n\") if x])#get_length_dataset(chars_dataset)\n",
    "    params['pos_amount'] = len([x for x in Path(params['pos_path']).read_text().split(\"\\n\") if x])#get_length_dataset(pos_dataset)\n",
    "    params['max_word_size'] = int(words_dataset.reduce(initial_state=0, reduce_func=lambda x,y: tf.maximum(x, tf.strings.length(y))))\n",
    "    max_seq_size = int(train_labels_dataset.reduce(initial_state=tf.constant([0]), reduce_func=lambda x,y: tf.maximum(x, tf.shape(tf.strings.split(y, sep=\" \"))))[0])\n",
    "    params['max_seq_size'] = min(max_seq_size, params['max_seq_size'])\n",
    "    # Creating sequence vectorizer\n",
    "    \n",
    "    sequence_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "#         max_tokens = params['word_amount'] + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = params['max_seq_size'],\n",
    "        standardize = params['standardize'],\n",
    "        split = params['split']\n",
    "    )\n",
    "    \n",
    "    sequence_vectorizer.adapt(train_sequences_dataset)\n",
    "    params['sequence_vectorizer'] = sequence_vectorizer\n",
    "    \n",
    "    # Creating tag vectorizer\n",
    "    \n",
    "    tag_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = params['tag_amount'] + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = params['max_seq_size'],\n",
    "        standardize = None,\n",
    "        split = \"whitespace\"\n",
    "    )\n",
    "    \n",
    "    tag_vectorizer.adapt(train_labels_dataset)\n",
    "    params['tag_vectorizer'] = tag_vectorizer\n",
    "    \n",
    "    # Creating pos vectorizer\n",
    "    pos_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = params['pos_amount'] + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = params['max_seq_size'],\n",
    "        standardize = None,\n",
    "        split = \"whitespace\"\n",
    "    )\n",
    "    pos_encoder = layers.CategoryEncoding(\n",
    "        num_tokens=params['pos_amount'] + 2,# Plus PAD and UNK,\n",
    "        output_mode=\"one_hot\"\n",
    "    )\n",
    "    \n",
    "    pos_vectorizer.adapt(train_pos_labels_dataset)\n",
    "    params['pos_vectorizer'] = pos_vectorizer\n",
    "    params['pos_encoder'] = pos_encoder\n",
    "    \n",
    "    # Creating char vectorizer\n",
    "    \n",
    "    char_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = params['char_amount'] + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = params['max_word_size'],\n",
    "        standardize = None,\n",
    "        split = \"character\"\n",
    "    )\n",
    "\n",
    "    char_vectorizer.adapt(train_sequences_dataset)\n",
    "    params['char_vectorizer'] = char_vectorizer\n",
    "    \n",
    "    # Adding lookups\n",
    "    word_to_index = dict(map(lambda x: (x[1],x[0]), enumerate(sequence_vectorizer.get_vocabulary())))\n",
    "    params['word_to_index'] = word_to_index\n",
    "    params['word_amount'] = len(word_to_index) - 2\n",
    "    \n",
    "    tag_to_index = dict(map(lambda x: (x[1],x[0]), enumerate(tag_vectorizer.get_vocabulary())))\n",
    "    params['tag_to_index'] = tag_to_index\n",
    "    \n",
    "    char_to_index = dict(map(lambda x: (x[1],x[0]), enumerate(char_vectorizer.get_vocabulary())))\n",
    "    params['char_to_index'] = char_to_index\n",
    "    \n",
    "    pos_to_index = dict(map(lambda x: (x[1],x[0]), enumerate(pos_vectorizer.get_vocabulary())))\n",
    "    params['pos_to_index'] = pos_to_index\n",
    "        \n",
    "    for key in ['pos_amount', 'tag_amount', 'char_amount', 'word_amount', 'max_word_size', 'max_seq_size']:\n",
    "        print(key, params[key])\n",
    "    \n",
    "    for key in ['word_to_index', 'tag_to_index', 'char_to_index', 'pos_to_index']:\n",
    "        print()\n",
    "        print(key)\n",
    "        print('Length:', len(params[key]))\n",
    "        print('First 20:', list(params[key].items())[:20])\n",
    "    \n",
    "add_datasets(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Compute the embedding matrix for the words present in the corpus. The matrix is serialized and saved for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(params: dict):\n",
    "    if Path(params[\"glove_path\"]).exists():\n",
    "        print(\"Glove Embedding Matrix Found\")\n",
    "        embedding_matrix = np.load(params[\"glove_path\"])[\"embeddings\"]\n",
    "        params['embedding_matrix'] = embedding_matrix\n",
    "        return\n",
    "    \n",
    "    # Loading Glove\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    embedding_dim = params['dim']\n",
    "    word_to_index = params['word_to_index']\n",
    "    num_tokens = len(word_to_index) # Plus padding and unknown \n",
    "\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    with Path(params[\"glove_raw_path\"]).open() as f:\n",
    "        for line_idx, line in enumerate(f):\n",
    "            if line_idx % 100000 == 0:\n",
    "                print('- At line {}'.format(line_idx))\n",
    "            line = line.strip().split()\n",
    "            if len(line) != 300 + 1:\n",
    "                continue\n",
    "            word = line[0]\n",
    "            embedding = line[1:]\n",
    "            if word in word_to_index:\n",
    "                hits += 1\n",
    "                word_idx = word_to_index[word]\n",
    "                embedding_matrix[word_idx] = embedding\n",
    "    print('- Done. Found {} vectors for {} words'.format(hits, num_tokens - 2))\n",
    "    \n",
    "    params['embedding_matrix'] = embedding_matrix\n",
    "    np.savez_compressed(params[\"glove_path\"], embeddings=embedding_matrix)\n",
    "    \n",
    "add_embeddings(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Creates the model based on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(params: dict):\n",
    "    # Common\n",
    "    model_name = params['model_name']\n",
    "    words_amount = params['word_amount'] + 2 # Plus padding and unknown \n",
    "    embedding_dim = params['dim']\n",
    "    embedding_matrix = params['embedding_matrix']\n",
    "    max_sequence_size = params['max_seq_size']\n",
    "    lstm_size = params[\"lstm_size\"]\n",
    "    tag_amount = params['tag_amount'] + 2 # Plus padding and unknown \n",
    "    dropout = params['dropout']\n",
    "    batch_size = params['batch_size']\n",
    "    trainable_word_emeddings = params['trainable_word_emeddings']\n",
    "    regularization = params['regularization']\n",
    "    with_resnet = params['with_resnet']\n",
    "    with_layer_normalization = params['with_layer_normalization']\n",
    "    \n",
    "    # POS\n",
    "    with_pos = params['with_pos']\n",
    "    pos_amount = params['pos_amount'] + 2 # Plus padding and unknown\n",
    "    pos_units = params['pos_units']\n",
    "    pos_activation = params['pos_activation']\n",
    "    \n",
    "    # CNN\n",
    "    with_cnn = params['with_cnn']\n",
    "    dim_chars = params[\"dim_chars\"]\n",
    "    filters = params['filters']\n",
    "    kernel_size = params[\"kernel_size\"]\n",
    "    max_word_size = params['max_word_size']\n",
    "    char_amount = params['char_amount'] + 2 # Plus padding and unknown\n",
    "    \n",
    "    # DN\n",
    "    with_dn = params['with_dn']\n",
    "    dense_units = params['dense_units']\n",
    "    dense_activation = params['dense_activation']\n",
    "    \n",
    "    # Char-LSTM\n",
    "    with_lstm = params['with_lstm']\n",
    "    dim_chars_lstm = params['dim_chars_lstm']\n",
    "    \n",
    "    \n",
    "    # POS input\n",
    "    if with_pos:\n",
    "        inputs_pos = model_pos_layers = keras.Input(\n",
    "            shape=(max_sequence_size, pos_amount)\n",
    "        )\n",
    "        model_pos_layers = layers.TimeDistributed(layers.Dense(units=pos_units, activation=pos_activation))(model_pos_layers)\n",
    "    \n",
    "    # Char input\n",
    "    if with_cnn or with_lstm:\n",
    "        inputs_word_chars = keras.Input(\n",
    "            shape=(max_sequence_size, max_word_size), \n",
    "            dtype=\"int64\",\n",
    "        )\n",
    "    \n",
    "    if with_cnn:\n",
    "        # Input layer char\n",
    "        int_char_input = keras.Input(shape=(max_word_size,), dtype=\"int64\")\n",
    "\n",
    "        # Embedding layer char\n",
    "        embedding_char_layer = layers.Embedding(\n",
    "            char_amount,\n",
    "            dim_chars,\n",
    "            trainable=True,\n",
    "            input_length=max_word_size,\n",
    "        )\n",
    "\n",
    "        embedded_chars = embedding_char_layer(int_char_input)\n",
    "\n",
    "        # Dropout layer char\n",
    "        model_char_layers = layers.Dropout(dropout)(embedded_chars)\n",
    "\n",
    "        # Convolution Layer char\n",
    "\n",
    "        model_char_layers = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            input_shape=(max_word_size, dim_chars),\n",
    "        )(model_char_layers)\n",
    "\n",
    "        # Max Polling layer char\n",
    "\n",
    "        model_char_layers = layers.MaxPooling1D(\n",
    "            max_word_size,\n",
    "            input_shape=(max_word_size, filters),\n",
    "        )(model_char_layers)\n",
    "\n",
    "        # Reshape layer char\n",
    "\n",
    "        model_char_layers = layers.Reshape((filters,), input_shape=(1, filters))(model_char_layers)\n",
    "\n",
    "        char_model = keras.Model(int_char_input, model_char_layers)\n",
    "\n",
    "        char_model.summary()\n",
    "\n",
    "        # Time Distributed with words\n",
    "        model_char_layers = layers.TimeDistributed(\n",
    "            char_model)(inputs_word_chars)\n",
    "\n",
    "        # Dropout layer\n",
    "        model_char_layers = layers.Dropout(dropout)(model_char_layers)\n",
    "    \n",
    "    if with_lstm:\n",
    "        # Input layer char\n",
    "        int_char_input = keras.Input(shape=(max_word_size,), dtype=\"int64\")\n",
    "\n",
    "        # Embedding layer char\n",
    "        embedding_char_lstm_layer = layers.Embedding(\n",
    "            char_amount,\n",
    "            dim_chars_lstm,\n",
    "            trainable=True,\n",
    "            input_length=max_word_size,\n",
    "        )\n",
    "\n",
    "        embedded_chars_lstm = embedding_char_lstm_layer(int_char_input)\n",
    "\n",
    "        # Dropout layer char\n",
    "        model_char_lstm_layers = layers.Dropout(dropout)(embedded_chars_lstm)\n",
    "        \n",
    "        model_char_lstm_layers = layers.Bidirectional(layers.LSTM(\n",
    "            dim_chars_lstm,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=dropout,\n",
    "        ))(model_char_lstm_layers)\n",
    "        \n",
    "        lstm_model = keras.Model(int_char_input, model_char_lstm_layers)\n",
    "        \n",
    "        lstm_model.summary()\n",
    "        \n",
    "        # Time Distributed with words\n",
    "        model_char_lstm_layers = layers.TimeDistributed(\n",
    "            lstm_model)(inputs_word_chars)\n",
    "        \n",
    "        # Dropout layer\n",
    "        model_char_lstm_layers = layers.Dropout(dropout)(model_char_lstm_layers)\n",
    "\n",
    "    # Input layer\n",
    "    int_sequences_input = keras.Input(\n",
    "        shape=(max_sequence_size,), \n",
    "        dtype=\"int64\"\n",
    "    )\n",
    "    \n",
    "    # Embedding layer, convert an index vector into a embedding vector, by accessing embedding_matrix\n",
    "    embedding_layer = layers.Embedding(\n",
    "        words_amount,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        mask_zero=True,\n",
    "        trainable=trainable_word_emeddings,\n",
    "        input_length=max_sequence_size,\n",
    "    )\n",
    "\n",
    "    model_layers = embedding_layer(int_sequences_input)\n",
    "    \n",
    "    if with_cnn:\n",
    "        # Concatenate char embedding with sentence embedding\n",
    "        model_layers = layers.Concatenate()([model_layers, model_char_layers])\n",
    "    \n",
    "    if with_lstm:\n",
    "        # Concatenate char lstm embedding with sentence embedding\n",
    "        model_layers = layers.Concatenate()([model_layers, model_char_lstm_layers])\n",
    "    \n",
    "    if with_pos:\n",
    "        # Concatenate POS one hot with sentence embedding\n",
    "        model_layers = layers.Concatenate()([model_layers, model_pos_layers])\n",
    "    \n",
    "    # Feature Normalization\n",
    "    if with_layer_normalization:\n",
    "        model_layers = layers.LayerNormalization()(model_layers)\n",
    "\n",
    "    prev_model_layers = model_layers\n",
    "        \n",
    "    # BiLSTM Layer\n",
    "    model_layers = layers.Bidirectional(layers.LSTM(\n",
    "        lstm_size, \n",
    "        return_sequences=True,\n",
    "        dropout=dropout,\n",
    "        recurrent_dropout=dropout,\n",
    "        # Regularizer\n",
    "        kernel_regularizer=regularization,\n",
    "        recurrent_regularizer=regularization,\n",
    "        bias_regularizer=regularization,\n",
    "    ))(model_layers)\n",
    "\n",
    "    # Residual Network Connection\n",
    "    if with_resnet:\n",
    "        prev_model_layers = layers.TimeDistributed(layers.Dense(lstm_size))(prev_model_layers)\n",
    "        prev_model_layers = layers.Concatenate()([prev_model_layers, prev_model_layers])\n",
    "        model_layers = layers.Add()([model_layers, prev_model_layers])\n",
    "    \n",
    "    # Feature Normalization\n",
    "    if with_layer_normalization:\n",
    "        model_layers = layers.LayerNormalization()(model_layers)\n",
    "  \n",
    "    # Dropout layer\n",
    "    model_layers = layers.Dropout(dropout)(model_layers)\n",
    "       \n",
    "    if with_dn:\n",
    "        # Dense layer\n",
    "        model_layers = layers.TimeDistributed(\n",
    "            layers.Dense(\n",
    "                dense_units, \n",
    "                activation=dense_activation,\n",
    "                kernel_regularizer=regularization,\n",
    "                bias_regularizer=regularization,\n",
    "                activity_regularizer=regularization,\n",
    "            ))(model_layers)\n",
    "\n",
    "        # Dropout layer\n",
    "        model_layers = layers.Dropout(dropout)(model_layers)\n",
    "    \n",
    "    # Model\n",
    "    if with_cnn or with_lstm:\n",
    "        if with_pos:\n",
    "            model = keras.Model(\n",
    "                [int_sequences_input, inputs_word_chars, inputs_pos], \n",
    "                model_layers\n",
    "            )\n",
    "        else:\n",
    "            model = keras.Model(\n",
    "                [int_sequences_input, inputs_word_chars], \n",
    "                model_layers\n",
    "            )\n",
    "    elif with_pos:\n",
    "        model = keras.Model(\n",
    "            [int_sequences_input, inputs_pos], \n",
    "            model_layers\n",
    "        )\n",
    "    else:\n",
    "        model = keras.Model(\n",
    "            int_sequences_input, \n",
    "            model_layers\n",
    "        )\n",
    "        \n",
    "    model.summary()\n",
    "    \n",
    "    # CRF layer\n",
    "    model = tfa.text.crf_wrapper.CRFModelWrapper(\n",
    "        model, \n",
    "        tag_amount)\n",
    "\n",
    "    optimizer = params['optimizer']\n",
    "#     metrics = params['metrics']\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "#         metrics=[SequenceAccuracy()]# + metrics,\n",
    "    )\n",
    "    \n",
    "    params[model_name] = model\n",
    "    \n",
    "    \n",
    "create_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Creates the functions to encode the datasets to perform the training and evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encode_dataset(sequence_dataset, label_dataset, batch_size, with_char_input, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table, with_pos, pos_vectorizer, pos_encoder):\n",
    "    if with_pos:\n",
    "        dataset = sequence_dataset.map(lambda sequence, pos: (sequence_vectorizer(sequence), pos_encoder(pos_vectorizer(pos))))\n",
    "    else:\n",
    "        dataset = sequence_dataset.map(lambda sequence: sequence_vectorizer(sequence))\n",
    "    if with_char_input:\n",
    "        if with_pos:\n",
    "            dataset = dataset.map(lambda sequence_vec, pos_vec: (sequence_vec, char_vectorizer(index_to_word_table.lookup(sequence_vec)), pos_vec))\n",
    "        else:\n",
    "            dataset = dataset.map(lambda sequence_vec: (sequence_vec, char_vectorizer(index_to_word_table.lookup(sequence_vec))))\n",
    "    if label_dataset is not None:\n",
    "        labels = label_dataset.map(lambda labels: tag_vectorizer(labels))\n",
    "        dataset = tf.data.Dataset.zip((dataset, labels))\n",
    "    if batch_size:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def encode_datasets(params: dict):\n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    char_vectorizer = params['char_vectorizer']\n",
    "    tag_vectorizer = params['tag_vectorizer']\n",
    "    with_char_input = params['with_cnn'] or params['with_lstm']\n",
    "    with_pos = params['with_pos']\n",
    "    pos_vectorizer = params['pos_vectorizer']\n",
    "    pos_encoder = params['pos_encoder']\n",
    "    word_and_index = [x for x in params['word_to_index'].items()]\n",
    "    tag_and_index = [x for x in params['tag_to_index'].items()]\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # Creating lookups\n",
    "    keys_tensor = tf.constant([x[1] for x in word_and_index], dtype=\"int64\")\n",
    "    vals_tensor = tf.constant([x[0] for x in word_and_index])\n",
    "    index_to_word_table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
    "        default_value=\"\")\n",
    "    params['index_to_word_table'] = index_to_word_table\n",
    "\n",
    "    keys_tensor = tf.constant([x[1] for x in tag_and_index], dtype=\"int32\")\n",
    "    vals_tensor = tf.constant([x[0] for x in tag_and_index])\n",
    "    index_to_tag_table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
    "        default_value=\"\")\n",
    "    params['index_to_tag_table'] = index_to_tag_table\n",
    "\n",
    "    # Creating datasets\n",
    "    train_sequences = params['train_sequences']\n",
    "    train_labels = params['train_labels']\n",
    "    params['train_ds'] = encode_dataset(train_sequences, train_labels, batch_size, with_char_input, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table, with_pos, pos_vectorizer, pos_encoder)\n",
    "    testa_sequences = params['testa_sequences']\n",
    "    testa_labels = params['testa_labels']\n",
    "    params['testa_ds'] = encode_dataset(testa_sequences, testa_labels, batch_size, with_char_input, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table, with_pos, pos_vectorizer, pos_encoder)\n",
    "    testb_sequences = params['testb_sequences']\n",
    "    testb_labels = params['testb_labels']\n",
    "    params['testb_ds'] = encode_dataset(testb_sequences, testb_labels, batch_size, with_char_input, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table, with_pos, pos_vectorizer, pos_encoder)\n",
    "    \n",
    "encode_datasets(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the model\n",
    "\n",
    "Perform the model training and serialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_and_save_model(params: dict, load_weight_epoch=None):\n",
    "\n",
    "    model_name = params['model_name']\n",
    "    model = params[model_name]\n",
    "\n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    train_ds = params['train_ds']\n",
    "    val_ds = params['testb_ds']\n",
    "    \n",
    "    \n",
    "    callbacks = []\n",
    "    # Model Checkpoints\n",
    "    checkpoint_path = params['checkpoint_path']\n",
    "    checkpoint_path = str(Path(checkpoint_path, \"cp-{epoch:04d}.ckpt\"))\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                             save_weights_only=True,\n",
    "                                                             save_freq=150,\n",
    "                                                             verbose=1)\n",
    "    callbacks.append(checkpoint_callback)\n",
    "    \n",
    "    if params['with_early_stopping']:\n",
    "        # Model Early Stopping\n",
    "        early_stopping_monitor = params['early_stopping_monitor']\n",
    "        early_stopping_patience = params['early_stopping_patience']\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=early_stopping_monitor,\n",
    "            patience=early_stopping_patience,\n",
    "            restore_best_weights=params['restore_best_weights'],\n",
    "        )\n",
    "        callbacks.append(early_stopping)\n",
    "    \n",
    "    if load_weight_epoch is not None:\n",
    "        model.load_weights(checkpoint_path.format_map({\"epoch\":load_weight_epoch}))\n",
    "        epochs = max(1, epochs - load_weight_epoch) # Remaining epochs to train. At least train one more\n",
    "    \n",
    "    history = model.fit(train_ds,\n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  callbacks=callbacks,\n",
    "                  validation_data=val_ds)\n",
    "\n",
    "    Path(params['model_histories_path']).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with Path(params['model_histories_path'], f\"history.json\").open('w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    model.save(params[\"model_path\"], save_format='tf')\n",
    "\n",
    "train_and_save_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_save_model(params: dict):\n",
    "    model_name = params['model_name']\n",
    "    model = params[model_name]\n",
    "    model.save(params[\"model_path\"], save_format='tf')\n",
    "    \n",
    "only_save_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_history(params: dict):\n",
    "    \n",
    "    title = model_name = params[\"model_name\"]\n",
    "    \n",
    "    base_path = Path(params['model_histories_path'])\n",
    "    path = base_path / f\"history.json\"\n",
    "    \n",
    "    history = json.load(path.open())\n",
    "    print(history.keys())\n",
    "    \n",
    "    plt.plot(history['crf_loss'], label='Cross entropy loss train')\n",
    "    plt.plot(history['val_crf_loss'], label='Cross entropy loss validation')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Cross entropy value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(base_path / \"crf_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_history(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(params: dict, path_key: str, param_save_key: str):\n",
    "    model = keras.models.load_model(params[path_key])\n",
    "    params[param_save_key] = model\n",
    "    \n",
    "load_saved_model(params, 'model_path', params['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(params: dict):\n",
    "    model_name = params['model_name']\n",
    "    batch_size = params['batch_size']\n",
    "    model = params[model_name]\n",
    "    test_ds = params['testa_ds']\n",
    "\n",
    "    results = model.evaluate(test_ds, batch_size=batch_size, return_dict = True)\n",
    "    print(results)\n",
    "    json.dump(results, Path(params[\"model_path\"], \"evaluation.json\").open(\"w\"))\n",
    "\n",
    "evaluate_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models to Export\n",
    "\n",
    "Creates models that are easier to use by performing encoding and decoding to the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def decode_output(output, index_to_tag_table):\n",
    "    return tf.map_fn(lambda x: index_to_tag_table.lookup(x), output, fn_output_signature=tf.string)\n",
    "\n",
    "class ExportModel(keras.Model):\n",
    "    \n",
    "    def __init__(self, model, with_char_input, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table, index_to_tag_table, with_pos, pos_vectorizer, pos_encoder, language, pos_tagger):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.with_char_input = with_char_input\n",
    "        self.sequence_vectorizer = sequence_vectorizer\n",
    "        self.tag_vectorizer = tag_vectorizer\n",
    "        self.char_vectorizer = char_vectorizer\n",
    "        self.index_to_word_table = index_to_word_table\n",
    "        self.index_to_tag_table = index_to_tag_table\n",
    "        self.with_pos = with_pos\n",
    "        self.pos_vectorizer = pos_vectorizer\n",
    "        self.pos_encoder = pos_encoder\n",
    "        self.pos_tagger = pos_tagger\n",
    "        self.language = language\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_tensors = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "        \n",
    "        if self.with_pos:\n",
    "            # Input must be separated by withespaces\n",
    "            input_pos_tensors = [tf.strings.join(self.pos_tagger.pos_tags(current.numpy().decode().split(), self.language), \" \") for current in inputs]\n",
    "            input_pos_tensors = tf.data.Dataset.from_tensor_slices(input_pos_tensors)\n",
    "            input_tensors = tf.data.Dataset.zip((input_tensors, input_pos_tensors))\n",
    "            \n",
    "        encoded_inputs = encode_dataset(input_tensors, None, 0, self.with_char_input, self.sequence_vectorizer, self.tag_vectorizer, self.char_vectorizer, self.index_to_word_table, self.with_pos, self.pos_vectorizer, self.pos_encoder)\n",
    "        \n",
    "        if self.with_char_input:\n",
    "            word_encoded_inputs = encoded_inputs.map(lambda *x: x[0])\n",
    "            word_encoded_inputs = tf.convert_to_tensor(list(word_encoded_inputs))\n",
    "            char_encoded_inputs = encoded_inputs.map(lambda *x: x[1])\n",
    "            char_encoded_inputs = tf.convert_to_tensor(list(char_encoded_inputs))\n",
    "            if self.with_pos:\n",
    "                pos_encoded_inputs = encoded_inputs.map(lambda *x: x[2])\n",
    "                pos_encoded_inputs = tf.convert_to_tensor(list(pos_encoded_inputs))\n",
    "                results = self.model([word_encoded_inputs, char_encoded_inputs, pos_encoded_inputs])\n",
    "            else:\n",
    "                results = self.model([word_encoded_inputs, char_encoded_inputs])\n",
    "        elif self.with_pos:\n",
    "            word_encoded_inputs = encoded_inputs.map(lambda *x: x[0])\n",
    "            word_encoded_inputs = tf.convert_to_tensor(list(word_encoded_inputs))\n",
    "            pos_encoded_inputs = encoded_inputs.map(lambda *x: x[2])\n",
    "            pos_encoded_inputs = tf.convert_to_tensor(list(pos_encoded_inputs))\n",
    "            results = self.model([word_encoded_inputs, pos_encoded_inputs])\n",
    "        else:\n",
    "            word_encoded_inputs = tf.convert_to_tensor(list(encoded_inputs))\n",
    "            results = self.model(word_encoded_inputs)\n",
    "\n",
    "        decoded_output = decode_output(results, self.index_to_tag_table)\n",
    "        return decoded_output\n",
    "    \n",
    "def save_final_model(params: dict):\n",
    "    model_name = params['model_name']\n",
    "    model = params[model_name]\n",
    "    with_char_input = params['with_cnn'] or params['with_lstm']\n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    tag_vectorizer = params['tag_vectorizer']\n",
    "    char_vectorizer = params['char_vectorizer']\n",
    "    index_to_word_table = params['index_to_word_table'] \n",
    "    index_to_tag_table = params['index_to_tag_table']\n",
    "    with_pos = params['with_pos']\n",
    "    pos_vectorizer = params['pos_vectorizer']\n",
    "    pos_encoder = params['pos_encoder']\n",
    "    language = params['language']\n",
    "    pos_tagger = NLTKPOSTagger() if params['pos_tagger'] == \"nltk\" else (SpacyPOSTagger() if params['pos_tagger'] == \"spacy\" else None)\n",
    "    \n",
    "    full_model = ExportModel(\n",
    "        model, \n",
    "        with_char_input,\n",
    "        sequence_vectorizer,\n",
    "        tag_vectorizer,\n",
    "        char_vectorizer,\n",
    "        index_to_word_table,\n",
    "        index_to_tag_table,\n",
    "        with_pos,\n",
    "        pos_vectorizer,\n",
    "        pos_encoder,\n",
    "        language,\n",
    "        pos_tagger\n",
    "    )\n",
    "    \n",
    "    inputs = tf.constant([\n",
    "        \"No podemos obligar a poner el mismo número de hombres y mujeres en todas las materias Existe la opinión de que las universidades y los colegios deberían matricular por igual a estudiantes hombres y mujeres en cada facultad . Personalmente , no estoy de acuerdo con el punto de vista , porque existen muchos caracteres diferentes entre estudiantes y estudiantes . Por un lado , los niños y niñas tienen diversidad en modos psicológicos e individualidad . La mayoría de los estudiantes varones tienden a utilizar el lado izquierdo del cerebro para pensar y actuar , y en muchos casos son más racionales y lógicos que las niñas . Por ejemplo , hay más científicos e ingenieros hombres en comparación con las mujeres en todo el mundo . Muchos niños están interesados ​​en la ciencia y la tecnología , mientras que a varias niñas les gusta aprender literatura , educación y artes . Además , es más probable que las chicas prefieran algunos trabajos relacionados con la emoción y la comunicación , como profesora , cantante e intérprete . Esto significa que las niñas difieren en gran medida de los niños en mente y comportamiento , y ambos tienen mejores habilidades en el aspecto específico . Además , puede tener un efecto negativo en estos estudiantes exigirles que elijan una materia en igual proporción de género , y que no se ajuste a los rasgos de personalidad y desarrollo mental de los estudiantes . Por ejemplo , una niña que está interesada en la literatura es asignada a un departamento de ingeniería , pero es poco probable que se concentre en su materia , y esto también puede bloquear el futuro desarrollo y la perspectiva profesional de la niña . Por otro lado , las universidades deberían animar a más chicas a elegir asignaturas de ciencias ya más chicos a estudiar humanidades , y esto podría evitar desequilibrios de género en algunas asignaturas . Afectaría la salud mental de los estudiantes estudiar en un ambiente de un solo género . En conclusión , es necesario que las universidades respeten la elección individual de asignaturas debido a la diversidad de chicos y chicas , y no podemos obligar a poner el mismo número de chicos y chicas en todas las asignaturas .\"\n",
    "    ])\n",
    "    result = full_model(inputs)\n",
    "    print(result)\n",
    "    \n",
    "    params[\"full_model\"] = full_model\n",
    "\n",
    "save_final_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import segmenter.models.tag_fixer as tag_fixer\n",
    "# import importlib\n",
    "# importlib.reload(tag_fixer)\n",
    "from segmenter.models.tag_fixer import fix_tags\n",
    "\n",
    "class ExportFixedTagsModel(ExportModel):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        result = super().call(inputs)\n",
    "        fixed_result = [fix_tags([s.numpy().decode() for s in x if s.numpy().decode()], verbose=False) for x in result]\n",
    "        return fixed_result\n",
    "\n",
    "def fixed_tags_model(params: dict):\n",
    "    model_name = params['model_name']\n",
    "    model = params[model_name]\n",
    "    with_char_input = params['with_cnn'] or params['with_lstm']\n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    tag_vectorizer = params['tag_vectorizer']\n",
    "    char_vectorizer = params['char_vectorizer']\n",
    "    index_to_word_table = params['index_to_word_table'] \n",
    "    index_to_tag_table = params['index_to_tag_table']\n",
    "    with_pos = params['with_pos']\n",
    "    pos_vectorizer = params['pos_vectorizer']\n",
    "    pos_encoder = params['pos_encoder']\n",
    "    language = params['language']\n",
    "    pos_tagger = NLTKPOSTagger() if params['pos_tagger'] == \"nltk\" else (SpacyPOSTagger() if params['pos_tagger'] == \"spacy\" else None)\n",
    "\n",
    "    full_model = ExportFixedTagsModel(\n",
    "        model, \n",
    "        with_char_input,\n",
    "        sequence_vectorizer,\n",
    "        tag_vectorizer,\n",
    "        char_vectorizer,\n",
    "        index_to_word_table,\n",
    "        index_to_tag_table,\n",
    "        with_pos,\n",
    "        pos_vectorizer,\n",
    "        pos_encoder,\n",
    "        language,\n",
    "        pos_tagger,\n",
    "    )\n",
    "    \n",
    "    inputs = tf.constant([\n",
    "#         \"No podemos obligar a poner el mismo número de hombres y mujeres en todas las materias Existe la opinión de que las universidades y los colegios deberían matricular por igual a estudiantes hombres y mujeres en cada facultad . Personalmente , no estoy de acuerdo con el punto de vista , porque existen muchos caracteres diferentes entre estudiantes y estudiantes . Por un lado , los niños y niñas tienen diversidad en modos psicológicos e individualidad . La mayoría de los estudiantes varones tienden a utilizar el lado izquierdo del cerebro para pensar y actuar , y en muchos casos son más racionales y lógicos que las niñas . Por ejemplo , hay más científicos e ingenieros hombres en comparación con las mujeres en todo el mundo . Muchos niños están interesados ​​en la ciencia y la tecnología , mientras que a varias niñas les gusta aprender literatura , educación y artes . Además , es más probable que las chicas prefieran algunos trabajos relacionados con la emoción y la comunicación , como profesora , cantante e intérprete . Esto significa que las niñas difieren en gran medida de los niños en mente y comportamiento , y ambos tienen mejores habilidades en el aspecto específico . Además , puede tener un efecto negativo en estos estudiantes exigirles que elijan una materia en igual proporción de género , y que no se ajuste a los rasgos de personalidad y desarrollo mental de los estudiantes . Por ejemplo , una niña que está interesada en la literatura es asignada a un departamento de ingeniería , pero es poco probable que se concentre en su materia , y esto también puede bloquear el futuro desarrollo y la perspectiva profesional de la niña . Por otro lado , las universidades deberían animar a más chicas a elegir asignaturas de ciencias ya más chicos a estudiar humanidades , y esto podría evitar desequilibrios de género en algunas asignaturas . Afectaría la salud mental de los estudiantes estudiar en un ambiente de un solo género . En conclusión , es necesario que las universidades respeten la elección individual de asignaturas debido a la diversidad de chicos y chicas , y no podemos obligar a poner el mismo número de chicos y chicas en todas las asignaturas .\",\n",
    "        \"¡Gracias por la sorpresa ! Siempre es útil reclamar ante lo mal hecho , pero soy de la opinión de que es mucho más útil y hermoso agradecer y reconocer . El miércoles 22 de junio , el ómnibus que transporta a los trabajadores de la Central Termoeléctrica Máximo Gómez Báez , de Mariel , fue detenido por la patrulla , a la salida de Guanajay . Nos pidieron que bajáramos , lo cual hicimos , aun sin saber de qué se trataba . Una vez fuera del vehículo , salieron , de no sabemos dónde , un grupo de compañeras de la FMC con banderas y fotos de la querida Vilma Espín . Su único propósito era darnos un reconocimiento , y agradecernos por ser trabajadores del sector eléctrico , en especial de la Termoeléctrica . Luego de invitarnos a tomar café y té , nos despidieron con un aplauso . Claro que debíamos haber dicho algo agradeciéndoles , pero fue tan sorpresivo y tan emocionante que nos quedamos sin palabras . Entonces , en nombre de los trabajadores y directivos de la CTE , quiero agradecer a esas compañeras que , pese a lo temprano y sin conocernos , nos dieron esa sorpresa tan linda y decirles : muchas gracias . Roger Puig Martínez , calle 37 , edificio 14 , Apto . 6 , e/ 34 y 40 , Artemisa .\"\n",
    "    ])\n",
    "    result = full_model(inputs)\n",
    "    print(result)\n",
    "    params[\"full_fixed_model\"] = full_model\n",
    "\n",
    "fixed_tags_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing statistics about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def models_statistics(params: dict):\n",
    "    testa_sequences_dataset = params[\"testa_sequences\"] \n",
    "    testa_labels_dataset = params[\"testa_labels\"]\n",
    "    batch_size = params['batch_size']\n",
    "    model = params['full_model']\n",
    "    fixed_model = params['full_fixed_model']\n",
    "    with_pos = params['with_pos']\n",
    "    \n",
    "    all_statistic = {\n",
    "        \"Word\": [],\n",
    "        \"TrueTag\": [],\n",
    "        \"InferedTag\": [],\n",
    "        \"FixedInferedTag\": []\n",
    "    }\n",
    "    \n",
    "    error_statistic = {\n",
    "        \"Word\": [],\n",
    "        \"TrueTag\": [],\n",
    "        \"InferedTag\": [],\n",
    "        \"FixedInferedTag\": []\n",
    "    }\n",
    "    \n",
    "    for test_seq_batch, test_lbl_batch in tf.data.Dataset.zip((testa_sequences_dataset, testa_labels_dataset)).batch(batch_size):\n",
    "        if with_pos:\n",
    "            test_seq_batch, _ = test_seq_batch\n",
    "        \n",
    "        infered_test_lbl_batch = model(test_seq_batch)\n",
    "        infered_test_lbl_fixed_batch = fixed_model(test_seq_batch)\n",
    "        \n",
    "        for words, true_labels, infered_labels, fixed_infered_labels in zip(test_seq_batch, test_lbl_batch, infered_test_lbl_batch, infered_test_lbl_fixed_batch):\n",
    "            words = words.numpy().decode().split()\n",
    "            true_labels = true_labels.numpy().decode().split()\n",
    "            \n",
    "            assert len(words) == len(true_labels)\n",
    "            if len(words) > len(infered_labels):\n",
    "                print(\"WARNING:\", f\"Sequence length ({len(words)}) exceed model max amount ({len(infered_labels)}):\\n{words}\\n\")\n",
    "            \n",
    "            for word, true_label, infered_label, fixed_infered_label in zip(words, true_labels, infered_labels, fixed_infered_labels):\n",
    "                infered_label = infered_label.numpy().decode()\n",
    "                \n",
    "                if true_label != fixed_infered_label:\n",
    "                    error_statistic[\"Word\"].append(word)\n",
    "                    error_statistic[\"TrueTag\"].append(true_label)\n",
    "                    error_statistic[\"InferedTag\"].append(infered_label)\n",
    "                    error_statistic[\"FixedInferedTag\"].append(fixed_infered_label)\n",
    "\n",
    "                all_statistic[\"Word\"].append(word)\n",
    "                all_statistic[\"TrueTag\"].append(true_label)\n",
    "                all_statistic[\"InferedTag\"].append(infered_label)\n",
    "                all_statistic[\"FixedInferedTag\"].append(fixed_infered_label)\n",
    "                    \n",
    "    error_statistic = pd.DataFrame(error_statistic)\n",
    "    all_statistic = pd.DataFrame(all_statistic)\n",
    "    params['general_statistic'] = (error_statistic, all_statistic)\n",
    "    \n",
    "models_statistics(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "Individual tag classification:\n",
    "\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "\n",
    "General tag classification:\n",
    "\n",
    "- F1 Macro\n",
    "\n",
    "Component classification:\n",
    "\n",
    "- F1 100%: If the component match is exact\n",
    "- F1 50%: If at least 50% of the component is in the match\n",
    "\n",
    "Tag confusion matrix\n",
    "\n",
    "- Error \n",
    "- Complete statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, f1_score\n",
    "\n",
    "def sk_f1(statistic):\n",
    "    print(classification_report(statistic['TrueTag'], statistic['FixedInferedTag'], zero_division=0))\n",
    "    return classification_report(statistic['TrueTag'], statistic['FixedInferedTag'], zero_division=0, output_dict=True)\n",
    "\n",
    "def component_level_statistic(true_intervals, infered_intervals, rate):\n",
    "    \n",
    "    def fill_segments(intervals, fix_tags = False):\n",
    "        b = []\n",
    "        e = []\n",
    "        s = []\n",
    "        \n",
    "        prev_e = True\n",
    "        for i,tag in enumerate(intervals):\n",
    "            bioes = tag[0]\n",
    "            meta = tag[2:]\n",
    "            if bioes == \"B\":\n",
    "                \n",
    "                if not prev_e and fix_tags:\n",
    "                    print(f\"WARNING: Fixing tag {tag} at {i}. Searching for Missing E tag.\")\n",
    "                    last_b, b_meta = b[-1]\n",
    "                    for j, j_tag in enumerate(intervals[last_b+1:i+1], start=last_b+1):\n",
    "                        if j_tag != f\"I-{b_meta}\":\n",
    "                            e.append((j-1, b_meta))\n",
    "                            break\n",
    "                        elif j == len(intervals) - 1:\n",
    "                            e.append((j, b_meta))\n",
    "                            break\n",
    "                    else:\n",
    "                        raise Exception(\"No E tag candidate found\")\n",
    "                \n",
    "                b.append((i, meta))\n",
    "                prev_e = False\n",
    "            elif bioes == \"E\":\n",
    "                \n",
    "                if prev_e and fix_tags:\n",
    "                    print(f\"WARNING: Fixing tag {tag} at {i}. Searching for Missing B tag.\")\n",
    "                    last_e, e_meta = e[-1]\n",
    "                    for j, j_tag in enumerate(intervals[last_e+1:i], start=last_e+1):\n",
    "                        j_tag_meta = j_tag[2:]\n",
    "                        if j_tag[0] != \"O\":\n",
    "                            b.append((j, j_tag_meta))\n",
    "                            break\n",
    "                    else:\n",
    "                        raise Exception(\"No B tag candidate found\")\n",
    "                \n",
    "                e.append((i, meta))\n",
    "                prev_e = True\n",
    "            elif bioes == \"S\":\n",
    "                s.append((i, meta))\n",
    "                \n",
    "                prev_e = True\n",
    "        \n",
    "        assert len(b) == len(e), \"Unbalanced B and E tags\"\n",
    "        \n",
    "        intervals = sorted([(b, e + 1, t) for (b, t),(e, t) in zip(b,e)] + [(s, s + 1, t) for (s, t) in s])\n",
    "        return intervals\n",
    "    \n",
    "    def compute_interval_complement(segments):\n",
    "        false_segments = []\n",
    "        last_end = 0\n",
    "        for start, end, _ in segments:\n",
    "            if last_end < start:\n",
    "                false_segments.append((last_end, start, \"O-NEGATIVE_CLASS\"))\n",
    "            last_end = end\n",
    "        return false_segments\n",
    "    \n",
    "    true_segments = fill_segments(true_intervals, fix_tags=True) # The segment might be too long for the model\n",
    "    false_segments = compute_interval_complement(true_segments)\n",
    "    all_true_segments = sorted(true_segments+false_segments)\n",
    "\n",
    "    \n",
    "    pred_segments = fill_segments(infered_intervals)\n",
    "    false_pred_segments = compute_interval_complement(pred_segments)\n",
    "    all_pred_segments = sorted(pred_segments+false_pred_segments)\n",
    "\n",
    "    \n",
    "    def return_true_positive_and_false_negative(true_segments, pred_segments):\n",
    "        true_positive = 0\n",
    "        false_negative = 0\n",
    "        intersections = []\n",
    "        \n",
    "        for current_true_start, current_true_end, current_true_tag in true_segments:\n",
    "            top_match = 0\n",
    "            top_interval = None\n",
    "            length = current_true_end - current_true_start\n",
    "            for current_pred_start, current_pred_end, current_pred_tag in pred_segments:\n",
    "                current_match = 0\n",
    "                if current_pred_end <= current_true_start or current_true_end <= current_pred_start:\n",
    "                    current_match = 0\n",
    "                else:\n",
    "                    max_start = max(current_true_start, current_pred_start)\n",
    "                    min_end = min(current_true_end, current_pred_end)\n",
    "                    intersection_length = min_end - max_start\n",
    "                    assert intersection_length > 0\n",
    "\n",
    "                    if intersection_length/length >= rate and intersection_length > top_match and current_pred_tag == current_true_tag:\n",
    "                        top_interval = min_end, max_start, current_pred_tag\n",
    "                        top_match = intersection_length\n",
    "\n",
    "            if top_interval is not None:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "                    \n",
    "        return true_positive, false_negative\n",
    "    \n",
    "    \n",
    "    true_positive, false_negative = return_true_positive_and_false_negative(all_true_segments, all_pred_segments)\n",
    "    _, false_positive = return_true_positive_and_false_negative(pred_segments, all_true_segments)\n",
    "    \n",
    "    try:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    except:\n",
    "        recall = 0\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except:\n",
    "        f1 = 0\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"rate\": rate}\n",
    "    \n",
    "def plot_confusion_matrix(params, true_y, pred_y, title, xticks_rotation=0):\n",
    "    ConfusionMatrixDisplay.from_predictions(true_y, pred_y, normalize=\"true\", values_format=\".2f\")\n",
    "    plt.xticks(rotation = xticks_rotation)\n",
    "    plt.title(title)\n",
    "    plt.savefig(Path(params['model_histories_path']) /  (title + \".png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_statistic(params: dict):\n",
    "    error_statistic, all_statistic = params['general_statistic']\n",
    "\n",
    "    print()\n",
    "    total_size = len(all_statistic)\n",
    "    \n",
    "    print()\n",
    "    print(\"Tag Accuracy:\", len(all_statistic[all_statistic[\"TrueTag\"] == all_statistic[\"FixedInferedTag\"]])/total_size)\n",
    "    print(\"BIOES-only Accuracy:\", len(all_statistic[all_statistic[\"TrueTag\"].map(lambda x: x[0]) == all_statistic[\"FixedInferedTag\"].map(lambda x: x[0])])/total_size)\n",
    "    print()\n",
    "    \n",
    "    infered_fixed_errors = error_statistic[error_statistic[\"TrueTag\"] != error_statistic[\"FixedInferedTag\"]]\n",
    "    bio_infered_fix_errors = error_statistic[error_statistic[\"TrueTag\"].map(lambda x: x[0]) != error_statistic[\"FixedInferedTag\"].map(lambda x: x[0])]\n",
    "    \n",
    "    headers = [\n",
    "        (\"Infered with Fixed Tags Error Rate\", infered_fixed_errors),\n",
    "        (\"BIOES-only Infered with Fixed Tags Error Rate\", bio_infered_fix_errors),\n",
    "    ]\n",
    "    for title, df in headers:\n",
    "        print(title)\n",
    "        print(len(df)/total_size)\n",
    "        print()\n",
    "    \n",
    "    print(\"All tags statistic\")\n",
    "    all_f1_report = sk_f1(all_statistic)\n",
    "    print()\n",
    "    \n",
    "    print(\"BIOES tags statistic\")\n",
    "    bio_statistic = all_statistic.applymap(lambda x: x[0] if len(x) > 0 else \"\")\n",
    "    bioes_f1_report = sk_f1(bio_statistic)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(\"Component statistic 100%: \")\n",
    "    all_100_report = component_level_statistic(all_statistic[\"TrueTag\"], all_statistic[\"FixedInferedTag\"], 1)\n",
    "    print()\n",
    "    print(\"Component statistic 50%: \")\n",
    "    all_50_report = component_level_statistic(all_statistic[\"TrueTag\"], all_statistic[\"FixedInferedTag\"], .5)\n",
    "    print()\n",
    "    \n",
    "    print(\"BIOES Component statistic 100%: \")\n",
    "    bioes_100_report = component_level_statistic(bio_statistic[\"TrueTag\"], bio_statistic[\"FixedInferedTag\"], 1)\n",
    "    print()\n",
    "    print(\"BIOES Component statistic 50%: \")\n",
    "    bioes_50_report = component_level_statistic(bio_statistic[\"TrueTag\"], bio_statistic[\"FixedInferedTag\"], .5)\n",
    "    print()\n",
    "    \n",
    "    plot_confusion_matrix(params, error_statistic[\"TrueTag\"], error_statistic[\"FixedInferedTag\"], \"Error Tags\", xticks_rotation=45)\n",
    "    plot_confusion_matrix(params, all_statistic[\"TrueTag\"], all_statistic[\"FixedInferedTag\"], \"All Tags\", xticks_rotation=45)\n",
    "    plot_confusion_matrix(params, all_statistic[\"TrueTag\"].map(lambda x: x[0]), all_statistic[\"FixedInferedTag\"].map(lambda x: x[0]), \"All Tags Only BIOES\", xticks_rotation=45)\n",
    "\n",
    "    statistic = {\n",
    "        \"all_report\": all_f1_report,\n",
    "        \"bioes_report\": bioes_f1_report,\n",
    "        \"all_100_component\": all_100_report,\n",
    "        \"all_50_component\": all_50_report,\n",
    "        \"bioes_100_component\": bioes_100_report,\n",
    "        \"bioes_50_component\": bioes_50_report,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    with (Path(params['model_histories_path']) / \"statistic.json\").open('w') as f:\n",
    "        json.dump(statistic, f)\n",
    "\n",
    "\n",
    "show_statistic(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Metrics\n",
    "\n",
    "Show the metrics for all models. The models should be in DATA_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def build_multiple_bar_plot(labels: list, values: dict, width=0.35, save: Path=None):\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects = []\n",
    "\n",
    "    for i, label in enumerate(values):\n",
    "        values_values = values[label]\n",
    "        diff = width / len(values) \n",
    "        label = label.replace(\"mean\", \"promedio\")\n",
    "        label = label.replace(\"source\", \"origen\")\n",
    "        label = label.replace(\"target\", \"objetivo\")\n",
    "        label = label.replace(\"relations\", \"relaciones\")\n",
    "        label = label.replace(\"relation\", \"relación\")\n",
    "        label = label.replace(\"loss\", \"pérdida\")\n",
    "        label = label.replace(\"linked\", \"enlazado\")\n",
    "        label = label.replace(\"_\", \" \")\n",
    "        rect = ax.bar(x + diff * i, [round(x,2) for x in values_values], diff, label=label)\n",
    "        rects.append(rect)\n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "#     ax.set_ylabel('Values')\n",
    "    ax.set_ylabel('Valor')\n",
    "#     ax.set_title('Metrics per Model')\n",
    "    ax.set_title('Métricas por modelos')\n",
    "    ax.set_xticks(x, labels, rotation=0)\n",
    "    ax.legend(loc=\"lower left\")\n",
    "\n",
    "    for rect in rects:\n",
    "        ax.bar_label(rect, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_joint_metrics(params: dict):\n",
    "    \n",
    "    base_path = Path(params['data_path'])\n",
    "    tags_path = Path(params['tags_path'])\n",
    "    \n",
    "    tags = sorted([x.strip() for x in tags_path.open().readlines()])\n",
    "    bioes_tags = sorted(set(tag[0] for tag in tags))\n",
    "    \n",
    "    def plot_history(history, model_name): \n",
    "        label = f'loss train {model_name}'\n",
    "        label = label.replace(\"loss\", \"pérdida\")\n",
    "        label = label.replace(\"train\", \"entrenamiento\")\n",
    "        plt.plot(history['crf_loss'], label=label)\n",
    "    \n",
    "        label = f'loss val {model_name}'\n",
    "        label = label.replace(\"loss\", \"pérdida\")\n",
    "        label = label.replace(\"val\",  \"validación\")\n",
    "        plt.plot(history['val_crf_loss'], label=label)\n",
    "\n",
    "    def show_plot():\n",
    "#         plt.title(\"All models\")\n",
    "        plt.title(\"Todos los modelos\")\n",
    "#         plt.ylabel('Cross entropy value')\n",
    "        plt.ylabel('Entropía cruzada')\n",
    "#         plt.xlabel('No. epoch')\n",
    "        plt.xlabel('No. de época')\n",
    "#         plt.legend(loc=\"upper right\")\n",
    "        plt.legend()\n",
    "        plt.savefig(base_path / \"crf_loss.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    statistic_df = {\n",
    "        \"model_name\": [],\n",
    "        \"component_bioes_100_precision\": [],\n",
    "        \"component_bioes_100_recall\": [],\n",
    "        \"component_bioes_100_f1\": [],\n",
    "        \"component_bioes_50_precision\": [],\n",
    "        \"component_bioes_50_recall\": [],\n",
    "        \"component_bioes_50_f1\": [],\n",
    "        \"component_100_precision\": [],\n",
    "        \"component_100_recall\": [],\n",
    "        \"component_100_f1\": [],\n",
    "        \"component_50_precision\": [],\n",
    "        \"component_50_recall\": [],\n",
    "        \"component_50_f1\": [],\n",
    "        **{f\"{tag}_precision\": [] for tag in tags},\n",
    "        **{f\"{tag}_recall\": [] for tag in tags},\n",
    "        **{f\"{tag}_f1\": [] for tag in tags},\n",
    "        **{f\"{tag}_bioes_precision\": [] for tag in bioes_tags},\n",
    "        **{f\"{tag}_bioes_recall\": [] for tag in bioes_tags},\n",
    "        **{f\"{tag}_bioes_f1\": [] for tag in bioes_tags},\n",
    "        \"macro_f1\": [],\n",
    "        \"wheight_f1\": [],\n",
    "        \"accuracy\": [],\n",
    "        \"bioes_macro_f1\": [],\n",
    "        \"bioes_wheight_f1\": [],\n",
    "        \"bioes_accuracy\": [],\n",
    "        \"evaluation_crf_loss\": []\n",
    "        \n",
    "    }\n",
    "    \n",
    "    name_map = {}\n",
    "    \n",
    "    for model_path in base_path.iterdir():\n",
    "        if model_path.name[0] == \"_\": continue\n",
    "        if model_path.is_dir():\n",
    "            model_name = model_path.name\n",
    "            statistic_df[\"model_name\"].append(model_name)\n",
    "            smaller_name = \"\".join([x[0] for x in model_name.split(\"_\")])\n",
    "#             smaller_name = f\"model{len(name_map)+1}\"\n",
    "            smaller_name = f\"modelo {len(name_map)+1}\"\n",
    "            name_map[model_name] = smaller_name\n",
    "            \n",
    "            history_path = model_path / f\"history.json\"\n",
    "            if history_path.exists():\n",
    "                history = json.load(history_path.open())\n",
    "                plot_history(history, smaller_name)\n",
    "            else:\n",
    "                print(f\"WARNING: History for {history_path} not found\" )\n",
    "            \n",
    "            statistic = model_path / \"statistic.json\"\n",
    "            statistic = json.load(statistic.open())\n",
    "            to_remove = []\n",
    "            for tag in tags:\n",
    "                try:\n",
    "                    statistic_df[f\"{tag}_precision\"].append(statistic[\"all_report\"][tag][\"precision\"])\n",
    "                    statistic_df[f\"{tag}_recall\"].append(statistic[\"all_report\"][tag][\"recall\"])\n",
    "                    statistic_df[f\"{tag}_f1\"].append(statistic[\"all_report\"][tag][\"f1-score\"])\n",
    "                except KeyError as e:\n",
    "                    to_remove.append(tag)\n",
    "                    pass\n",
    "            for tag in to_remove:\n",
    "                tags.remove(tag)\n",
    "            \n",
    "            evaluation_path = model_path / \"evaluation.json\"\n",
    "            if evaluation_path.exists():\n",
    "                evaluation = json.load(evaluation_path.open())\n",
    "                statistic_df[\"evaluation_crf_loss\"].append(evaluation['crf_loss'])\n",
    "            else:\n",
    "                print(f\"WARNING: Evaluation {evaluation_path} doesn't exist\")\n",
    "                statistic_df[\"evaluation_crf_loss\"].append(0)\n",
    "            \n",
    "            statistic_df[\"macro_f1\"].append(statistic[\"all_report\"][\"macro avg\"][\"f1-score\"])\n",
    "            statistic_df[\"wheight_f1\"].append(statistic[\"all_report\"][\"weighted avg\"][\"f1-score\"])\n",
    "            statistic_df[\"accuracy\"].append(statistic[\"all_report\"][\"accuracy\"])\n",
    "            statistic_df[\"component_100_precision\"].append(statistic['all_100_component']['precision'])\n",
    "            statistic_df[\"component_100_recall\"].append(statistic['all_100_component']['recall'])\n",
    "            statistic_df[\"component_100_f1\"].append(statistic['all_100_component']['f1'])\n",
    "            statistic_df[\"component_50_precision\"].append(statistic['all_50_component']['precision'])\n",
    "            statistic_df[\"component_50_recall\"].append(statistic['all_50_component']['recall'])\n",
    "            statistic_df[\"component_50_f1\"].append(statistic['all_50_component']['f1'])\n",
    "\n",
    "\n",
    "            to_remove = []\n",
    "            for tag in bioes_tags:\n",
    "                try:\n",
    "                    statistic_df[f\"{tag}_bioes_precision\"].append(statistic[\"bioes_report\"][tag][\"precision\"])\n",
    "                    statistic_df[f\"{tag}_bioes_recall\"].append(statistic[\"bioes_report\"][tag][\"recall\"])\n",
    "                    statistic_df[f\"{tag}_bioes_f1\"].append(statistic[\"bioes_report\"][tag][\"f1-score\"])\n",
    "                except KeyError as e:\n",
    "                    to_remove.append(tag)\n",
    "                    pass\n",
    "            for tag in to_remove:\n",
    "                bioes_tags.remove(tag)\n",
    "            \n",
    "            statistic_df[\"bioes_macro_f1\"].append(statistic[\"bioes_report\"][\"macro avg\"][\"f1-score\"])\n",
    "            statistic_df[\"bioes_wheight_f1\"].append(statistic[\"bioes_report\"][\"weighted avg\"][\"f1-score\"])\n",
    "            statistic_df[\"bioes_accuracy\"].append(statistic[\"bioes_report\"][\"accuracy\"])\n",
    "            statistic_df[\"component_bioes_100_precision\"].append(statistic['bioes_100_component']['precision'])\n",
    "            statistic_df[\"component_bioes_100_recall\"].append(statistic['bioes_100_component']['recall'])\n",
    "            statistic_df[\"component_bioes_100_f1\"].append(statistic['bioes_100_component']['f1'])\n",
    "            statistic_df[\"component_bioes_50_precision\"].append(statistic['bioes_50_component']['precision'])\n",
    "            statistic_df[\"component_bioes_50_recall\"].append(statistic['bioes_50_component']['recall'])\n",
    "            statistic_df[\"component_bioes_50_f1\"].append(statistic['bioes_50_component']['f1'])\n",
    "\n",
    "                \n",
    "    for key, value in name_map.items():\n",
    "        print(key, \"->\", value)\n",
    "    names_mapped = [name_map[name] for name in name_map]\n",
    "    \n",
    "    # Show train/loss \n",
    "    show_plot()\n",
    "    \n",
    "    \n",
    "    keys = [\n",
    "        \"component_100_f1\", \n",
    "        \"component_bioes_100_f1\", \n",
    "        \"component_50_f1\",\n",
    "        \"component_bioes_50_f1\", \n",
    "    ]\n",
    "    save = base_path / \"components.png\"\n",
    "    build_multiple_bar_plot(names_mapped, {key: statistic_df[key] for key in keys}, width=0.8, save=save)\n",
    "    \n",
    "    keys = [\n",
    "        \"macro_f1\", \n",
    "        \"bioes_macro_f1\", \n",
    "        \"accuracy\", \n",
    "        \"bioes_accuracy\",\n",
    "    ]\n",
    "    save = base_path / \"macro_micro_metrics.png\"\n",
    "    build_multiple_bar_plot(names_mapped, {key: statistic_df[key] for key in keys}, width=0.8, save=save)\n",
    "    \n",
    "    keys = [\n",
    "        \"wheight_f1\", \n",
    "        \"bioes_wheight_f1\", \n",
    "    ]\n",
    "    save = base_path / \"weight_macro_metrics.png\"\n",
    "    build_multiple_bar_plot(names_mapped, {key: statistic_df[key] for key in keys}, width=0.8, save=save)\n",
    "    \n",
    "    keys = [f\"{tag}_f1\" for tag in tags]\n",
    "    save = base_path / \"tags_f1.png\"\n",
    "    build_multiple_bar_plot(names_mapped, {key: statistic_df[key] for key in keys}, width=0.8, save=save)\n",
    "    \n",
    "    keys = [f\"{tag}_bioes_f1\" for tag in bioes_tags]\n",
    "    save = base_path / \"tags_bioes_f1.png\"\n",
    "    build_multiple_bar_plot(names_mapped, {key: statistic_df[key] for key in keys}, width=0.8, save=save)\n",
    "    \n",
    "    keys = [f\"evaluation_crf_loss\"]\n",
    "    save = base_path / \"evaluation.png\"\n",
    "    build_multiple_bar_plot(names_mapped, {key: statistic_df[key] for key in keys}, width=0.8, save=save)\n",
    "    \n",
    "print_joint_metrics(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model\n",
    "\n",
    "Perform the segmentation on text. The data to process directory must contain folders with **.txt** files, the tokens in this files must be separated by whitepaces. This files will be passed through the model and the output will be written in conll format in the processed directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_build_model_from_params(params: dict):\n",
    "    \n",
    "    add_datasets(params)\n",
    "    load_saved_model(params, 'model_path', params['model_name'])    \n",
    "    encode_datasets(params)\n",
    "    fixed_tags_model(params)\n",
    "    return params[\"full_fixed_model\"]\n",
    "\n",
    "load_and_build_model_from_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(params: dict, create_corpus: bool = False):\n",
    "    create_segmenter_corpus(params, create_corpus)\n",
    "    add_datasets(params)\n",
    "    add_embeddings(params)\n",
    "    create_model(params)\n",
    "    encode_datasets(params)\n",
    "    train_and_save_model(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def perform_segmentation(params: dict):\n",
    "    model = params['full_fixed_model']\n",
    "    batch_size = params['batch_size']\n",
    "    to_process_path = params['to_process_data_path']\n",
    "    to_save_path = params['processed_data_path']\n",
    "    \n",
    "    labels = sorted(list(os.walk(to_process_path))[0][1])\n",
    "    print(labels)\n",
    "    files = keras.utils.text_dataset_from_directory(\n",
    "        str(to_process_path), \n",
    "        class_names = labels,\n",
    "        shuffle = False,\n",
    "    )\n",
    "    \n",
    "    base_path = Path(to_save_path) / params['model_name']\n",
    "    base_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    file_number = 0\n",
    "    filenames = [file.name for file in Path(to_process_path).rglob('*.txt')]\n",
    "    \n",
    "    for text_batch, label_batch in files:\n",
    "        tag_batch = model(text_batch)\n",
    "        for text, label, tags in zip(text_batch, label_batch, tag_batch):\n",
    "            text = text.numpy().decode().split()\n",
    "            tags = [tag for tag in tags]\n",
    "            \n",
    "            current_file = base_path / labels[label]\n",
    "            current_file.mkdir(exist_ok=True)\n",
    "            current_file /= filenames[file_number] + \".conll\"\n",
    "            current_file.touch()\n",
    "            current_file.write_text(\"\\n\".join(f\"{word}\\t{tag}\" for word,tag in zip(text, tags)))\n",
    "            file_number += 1\n",
    "            \n",
    "\n",
    "perform_segmentation(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export jupyter as module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    try:\n",
    "        if Path(__file__).suffix == \".ipynb\":\n",
    "            raise NameError()\n",
    "    except NameError:\n",
    "        # In Jupyer Notebook\n",
    "        from utils.notebook_utils import export_notebook_as_module\n",
    "        export_notebook_as_module(Path(\"segmenter.ipynb\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
