{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts4UQMDNPRSK"
      },
      "source": [
        "# Argument Mining Repo\n",
        "\n",
        "This notebook presents an end-to-end cross language algorithm to perform the argumentative structure extraction from texts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBjBKCWt07YX"
      },
      "source": [
        "# Install Software\n",
        "\n",
        "This section will install and setup all you need to run the program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mao_0C_WPWHi",
        "outputId": "d360c44d-983d-4cc4-a912-c44e92d231e7"
      },
      "outputs": [],
      "source": [
        "# Delete repo\n",
        "# !rm -r argument-mining/\n",
        "\n",
        "# Clone repo\n",
        "!git clone https://github.com/luisoibarra/argument-mining.git\n",
        "\n",
        "# Update repo with master\n",
        "# !cd argument-mining/; git reset --hard && git clean -fd; git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok68HQqzjlme",
        "outputId": "db2713d4-f9bd-4a90-d552-eb8e5d32ccc8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Changing .sh permissions\n",
        "!cd argument-mining/ && chmod 777 install.sh\n",
        "for file in Path(\"argument-mining\", \"scripts\").iterdir():\n",
        "  if file.is_file() and file.suffix == \".sh\":\n",
        "    name = str(file)\n",
        "    !chmod 777 $name\n",
        "\n",
        "!cd argument-mining && ./install.sh\n",
        "!cd argument-mining/scripts && ./install_tools.sh nobrat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy8Zz3vA5sP8"
      },
      "source": [
        "# Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65wHhxUn56nm",
        "outputId": "eb394a6b-ac66-4fd2-c2a4-0536ed451124"
      },
      "outputs": [],
      "source": [
        "#@title Setting params: { display-mode: \"form\" }\n",
        "import os\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from IPython.utils import capture\n",
        "\n",
        "#@markdown ## Set the target language and the source language \n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "with capture.capture_output() as cap: \n",
        "  %cd /content/\n",
        "#@markdown Cropus language (can try with another language, although is not been tested with other):\n",
        "source_language = \"english\" #@param [\"english\", \"spanish\", \"french\"]  {allow-input: true}\n",
        "\n",
        "#@markdown Target language (same as source language):\n",
        "target_language = \"spanish\" #@param [\"english\", \"spanish\", \"french\"]  {allow-input: true}\n",
        "\n",
        "#@markdown ## Set the training corpus\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown This tag must exist in argument-mining/data/corpus/ with train, test and dev subfolders.\n",
        "corpus = \"persuasive_essays_paragraph_all_linked\" #@param [\"abstrct\", \"cdcp\", \"persuasive_essays_paragraph\", \"persuassive_essays_paragraph_all_linked\"] {allow-input: true}\n",
        "\n",
        "#@markdown ## Set the process tag.\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown This tag must exist in argument-mining/data/to_process/\n",
        "process = \"testing\" #@param [\"testing\", \"granma_letters\", \"response_responded_granma_letters\", \"selected_response_responded_granma_letters\"] {allow-input: true}\n",
        "\n",
        "#@markdown ## Path of the saved models, if any.\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Segmenter path\n",
        "segmenter_saved_path = \"/content/drive/MyDrive/argument-mining/segmenter\" #@param {type:\"string\"}\n",
        "segmenter_saved_path = Path(segmenter_saved_path, corpus)\n",
        "\n",
        "#@markdown Link Predictor path\n",
        "link_predictor_saved_path = \"/content/drive/MyDrive/argument-mining/link_prediction\" #@param {type:\"string\"}\n",
        "link_predictor_saved_path = Path(link_predictor_saved_path, corpus)\n",
        "\n",
        "#@markdown Segmenter path\n",
        "segmenter_to_save_path = \"/content/segmenter\" #@param {type:\"string\"}\n",
        "segmenter_to_save_path = Path(segmenter_to_save_path, corpus)\n",
        "\n",
        "#@markdown Link Predictor path\n",
        "link_predictor_to_save_path = \"/content/link_prediction\" #@param {type:\"string\"}\n",
        "link_predictor_to_save_path = Path(link_predictor_to_save_path, corpus)\n",
        "\n",
        "if segmenter_saved_path.exists():\n",
        "  path = str(segmenter_saved_path)\n",
        "  target = Path(f\"/content/argument-mining/data/segmenter_corpus/\")\n",
        "  target.mkdir(parents=True, exist_ok=True)\n",
        "  target_path = str(target)\n",
        "  !cp -r \"$path/\" \"$target_path\"\n",
        "  print(\"Model segmenter copied.\")\n",
        "\n",
        "if link_predictor_saved_path.exists():\n",
        "  path = str(link_predictor_saved_path)\n",
        "  target = Path(f\"/content/argument-mining/data/link_prediction/\")\n",
        "  target.mkdir(parents=True, exist_ok=True)\n",
        "  target_path = str(target)\n",
        "  !cp -r \"$path/\" \"$target_path\"\n",
        "  print(\"Model link predictor copied.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaeY02zD1qdM"
      },
      "source": [
        "# Corpus Projection\n",
        "\n",
        "This section will create the corpus in the target language by projecting the labels from the corpus in the source language. Can be skipped if abstrct, cdcp or persuasive_essays_paragraph_all_linked were selected, or if you already have the projections in the folder /content/argument-mining/data/projection/$corpus_name.\n",
        "\n",
        "A corpus can be added to the /content/argument-mining/data/corpus/$corpus_name with train, dev and test subfolders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3j_pydCodIm",
        "outputId": "96e0efab-b5a2-4f95-f5ae-a668dc2da45f"
      },
      "outputs": [],
      "source": [
        "# Projecting corpus\n",
        "for split in [\"dev\", \"test\", \"train\"]:\n",
        "  !cd argument-mining/ python3 project_corpus.py \\\n",
        "  \"data/corpus/$corpus/$split\" \\\n",
        "  \"data/parsed_to_conll/$corpus/$split\" \\\n",
        "  \"data/sentence_alignment/$corpus/$split\" \\\n",
        "  \"data/bidirectional_alignment/$corpus/$split\" \\\n",
        "  \"data/projection/$corpus/$split\"\\\n",
        "  --source_language $source_language\\\n",
        "  --target_language $target_language\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoDbJK_x2Swl"
      },
      "source": [
        "# Train Models\n",
        "\n",
        "This section will train the segmenter and the link predictor. If the models were loaded can skip this section.\n",
        "\n",
        "## Segmenter\n",
        "\n",
        "The segmenter model will perform the split of the argumentative discourse units (ADU) and its classification.\n",
        "\n",
        "## Link Predictor\n",
        "\n",
        "The link predictor will perform the prediction and classification of the links between the extracted ADUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxBWtKacqmu9",
        "outputId": "9f2065a0-8b89-4ceb-cd34-5cee60b4e03c"
      },
      "outputs": [],
      "source": [
        "# Training segmenter\n",
        "!cd argument-mining/scripts && ./train_segmenter.sh $corpus $target_language --epochs 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo9dIU3gfxTb",
        "outputId": "0f103920-d78a-4b49-c8f8-470c438c5b24"
      },
      "outputs": [],
      "source": [
        "# Training link projection\n",
        "!cd argument-mining/scripts && ./train_link_predictor.sh $corpus $target_language --epochs 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFAb8PW_TbZp"
      },
      "outputs": [],
      "source": [
        "segmenter_to_save_path.mkdir(parents=True, exist_ok=True)\n",
        "link_predictor_to_save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "segmenter_to_save_path_str = str(segmenter_to_save_path)\n",
        "link_predictor_to_save_path_str = str(link_predictor_to_save_path)\n",
        "\n",
        "!cp -r /content/argument-mining/data/segmenter_corpus/$corpus/* $segmenter_to_save_path_str\n",
        "!cp -r /content/argument-mining/data/link_prediction/$corpus/* $link_predictor_to_save_path_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y4r8ZTo27ba"
      },
      "source": [
        "# Extract Argumentative Structures\n",
        "\n",
        "This section will extract the argumentative structure using the trained models.\n",
        "\n",
        "You can copy the text to process into /content/argument-mining/data/to_process/$name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFcV8KnDkNTu",
        "outputId": "12b84145-53de-4dcd-c84e-d33ec6970a03"
      },
      "outputs": [],
      "source": [
        "processed_data_path = Path(f\"data/link_prediction_processed/{corpus}/{process}\")\n",
        "processed_data_path_str = str(processed_data_path)\n",
        "\n",
        "# Applying segmenter and link projector\n",
        "!cd argument-mining/ && python3 predict_relations.py --corpus_tag $corpus --source_language $target_language \\\n",
        "\"data/to_process/$process\" \\\n",
        "\"data/segmenter_processed/$corpus/$process\" \\\n",
        "$processed_data_path_str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh7bijWE3SMF"
      },
      "source": [
        "# View Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A5OrxOYxpQ4X"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "import re\n",
        "import logging as log\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor, Future, wait\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "ArgumentationInfo = Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
        "AnnotatedRawTextInfo = Tuple[str,str]\n",
        "\n",
        "class Parser():\n",
        "    \n",
        "    def __init__(self, accepted_files: Iterable[str], suffix: str) -> None:\n",
        "        \"\"\"\n",
        "        accepted_files: List of files extensions to be read\n",
        "        \"\"\"\n",
        "        self.accepted_files = tuple(accepted_files)\n",
        "        self.suffix = suffix\n",
        "        \n",
        "    def _should_read_file(self, file: Path):\n",
        "        \"\"\"\n",
        "        Returns if a file should be read as a corpus file\n",
        "        \"\"\"\n",
        "        return file.is_file() and file.name.endswith(self.accepted_files)\n",
        "\n",
        "    def parse_dir(self, corpus_path: Path, **kwargs) -> Dict[str, ArgumentationInfo]:\n",
        "        \"\"\"\n",
        "        Parse the file\n",
        "        \n",
        "        corpus_path: Base corpus address\n",
        "        \n",
        "        return: A dictionary mapping file address to its information\n",
        "        \"\"\"\n",
        "        \n",
        "        results = {}\n",
        "        futures: List[Future] = []\n",
        "        max_worker = 20\n",
        "        \n",
        "        files = [file for file in corpus_path.iterdir()]\n",
        "        batch = len(files)//max_worker + 1\n",
        "        \n",
        "        def read(slice):\n",
        "            for file in files[batch*slice:batch*(slice+1)]:\n",
        "                if self._should_read_file(file):\n",
        "                    result = self.parse_file(file, **kwargs)\n",
        "                    results[str(file)] = result\n",
        "        \n",
        "        with ThreadPoolExecutor(max_workers=max_worker) as exe:\n",
        "            for i in range(max_worker):\n",
        "                futures.append(exe.submit(read, i))\n",
        "                # read(i)\n",
        "        wait(futures)\n",
        "        exceptions = [future.exception() for future in futures if future.exception()]\n",
        "        \n",
        "        if exceptions:\n",
        "            raise Exception(exceptions)\n",
        "        \n",
        "        return results\n",
        "\n",
        "    def parse_file(self, file: Path, **kwargs) -> ArgumentationInfo:\n",
        "        \"\"\"\n",
        "        Parse the content of `file` returning two DataFrames containing\n",
        "        the argumentative unit and the relation information.\n",
        "        \n",
        "        argumentative_units columns: \n",
        "          - `prop_id` Proposition ID inside the document\n",
        "          - `prop_type` Proposition type\n",
        "          - `prop_init` When the proposition starts in the original text\n",
        "          - `prop_end` When the proposition ends in the original text\n",
        "          - `prop_text` Proposition text\n",
        "          \n",
        "        relations columns:\n",
        "          - `relation_id` Relation ID inside the document\n",
        "          - `relation_type` Relation type\n",
        "          - `prop_id_source` Relation's source proposition id \n",
        "          - `prop_id_target` Relation's target proposition id\n",
        "          \n",
        "        non_argumentative_units columns:\n",
        "          - `prop_init` When the proposition starts in the original text\n",
        "          - `prop_end` When the proposition ends in the original text\n",
        "          - `prop_text` Proposition text\n",
        "          \n",
        "        return: (argumentative_units, relations, non_argumentative_units)\n",
        "        \"\"\"\n",
        "        return self.parse(file.read_text(), file, **kwargs)\n",
        "    \n",
        "    def parse(self, content:str, file: Optional[Path] = None, **kwargs) -> ArgumentationInfo:\n",
        "        \"\"\"\n",
        "        Parse `content` returning DataFrames containing\n",
        "        the argumentative unit and the relation information.\n",
        "        \n",
        "        content: text containing the content to parse\n",
        "        file: Optional, content's original file\n",
        "        \n",
        "        argumentative_units columns: \n",
        "          - `prop_id` Proposition ID inside the document\n",
        "          - `prop_type` Proposition type\n",
        "          - `prop_init` When the proposition starts in the original text\n",
        "          - `prop_end` When the proposition ends in the original text\n",
        "          - `prop_text` Proposition text\n",
        "          \n",
        "        relations columns:\n",
        "          - `relation_id` Relation ID inside the document\n",
        "          - `relation_type` Relation type\n",
        "          - `prop_id_source` Relation's source proposition id \n",
        "          - `prop_id_target` Relation's target proposition id\n",
        "          \n",
        "        non_argumentative_units columns:\n",
        "          - `prop_init` When the proposition starts in the original text\n",
        "          - `prop_end` When the proposition ends in the original text\n",
        "          - `prop_text` Proposition text\n",
        "          \n",
        "        return: (argumentative_units, relations, non_argumentative_units)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def from_dataframes(self, dataframes: Dict[str, ArgumentationInfo], source_language=\"english\", **kwargs) -> Dict[str, AnnotatedRawTextInfo]:\n",
        "        \"\"\"\n",
        "        Creates file with annotated corpus representing the received DataFrames. \n",
        "        \n",
        "        dataframes: The result from calling a parse function in any Parser class\n",
        "        the keys aren't important, so a mock key can be passed.\n",
        "        language: Language for tokenization process\n",
        "        \n",
        "        returns: Annotated string, Raw entire text\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def export_from_dataframes(self, dest_address: Path, dataframes: Dict[str, ArgumentationInfo], **kwargs):\n",
        "        \"\"\"\n",
        "        Saves the corpus to into dest_address, converting the dataframe version into the corresponding representation.\n",
        "        \n",
        "        dest_address: Path where to save the corpus. May not exist\n",
        "        dataframes: DataFrame representation of the corpus\n",
        "        \"\"\"\n",
        "        representation = self.from_dataframes(dataframes, **kwargs)\n",
        "        self.export_corpus_from_files(dest_address, representation, **kwargs)\n",
        "        self.create_conf_files_from_argumentation_dict(dest_address, dataframes)\n",
        "    \n",
        "    def create_conf_files_from_argumentation_dict(self, corpus_path: Path, argumentation_dict: Dict[str, ArgumentationInfo]):\n",
        "        \"\"\"\n",
        "        Create any extra requirement file for the corpus\n",
        "        \n",
        "        corpus_path: Path to save the generated files\n",
        "        argumentation_dict: Result of calling the `parse_dir` function\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def export_corpus_from_files(self, dest_address: Path, files: Dict[str,AnnotatedRawTextInfo], **kwargs):\n",
        "        \"\"\"\n",
        "        Saves the corpus into dest_address. The files will be named after its key.\n",
        "        \n",
        "        dest_addres: Path where to save the corpus. May not exist\n",
        "        files: Maps file address or file name to its corpus representation and its full text.\n",
        "        \"\"\"\n",
        "        if not dest_address.is_dir():\n",
        "            dest_address.mkdir(exist_ok=True, parents=True)\n",
        "            \n",
        "        for filedir, (annotated_text, raw_text) in files.items():\n",
        "            name = Path(filedir).name\n",
        "            name += self.suffix\n",
        "            dest = dest_address / name\n",
        "            dest.write_text(annotated_text)\n",
        "            dest = dest_address / (name + \".txt\")\n",
        "            dest.write_text(raw_text)\n",
        "\n",
        "ConllTagInfo = Dict[str, Union[str,int]]\n",
        "\n",
        "class ConllParser(Parser):\n",
        "    \n",
        "    ANNOTATION_REGEX = r\"^(?P<tok>[^\\s]+)\\s(?P<bio_tag>[{TAGS}])(-(?P<prop_type>\\w+))?(?P<relations>(-\\w+--?\\d+)*)\\s*$\"\n",
        "    TAG_FORMAT = \"{bio_tag}-{prop_type}-{relations_string}\"\n",
        "    ANNOTATION_FORMAT = f\"{{tok}}\\t{TAG_FORMAT}\\n\"\n",
        "    \n",
        "    def __init__(self, *additional_supported_formats, bioes=False, use_spacy=False, **kwargs) -> None:\n",
        "        super().__init__((\".conll\", *additional_supported_formats), \".conll\")\n",
        "        tags = \"BIO\" if not bioes else \"BIOES\"\n",
        "        self.bioes = bioes\n",
        "        self.use_spacy = use_spacy\n",
        "        self.ANNOTATION_REGEX = self.ANNOTATION_REGEX.format_map({\"TAGS\": tags})\n",
        "        self.annotation_regex = re.compile(self.ANNOTATION_REGEX)\n",
        "        self.__sent_separator = {\"tok\":\"\\n\", \"bio_tag\":\"\"}\n",
        "        self.__relation_regex = re.compile(r\"-(?P<relation_tag>\\w+)-(?P<distance>-?\\d+)\")\n",
        "    \n",
        "    def __split_sentences(self, line_infos: list, language: str) -> list:\n",
        "        \"\"\"\n",
        "        Create a new list and adds a sentence separator to the `line_infos`'s content\n",
        "        the separator is the dtctionary `{\"tok\":\"\", \"bio_tag\":\"\"}`\n",
        "        \n",
        "        line_infos: Original information list\n",
        "        language: Language content\n",
        "        \n",
        "        returns: A new list containing a sentence separator \n",
        "        \"\"\"\n",
        "        new_line_infos = []\n",
        "        previous_splitted = [i for i, tok in enumerate(line_infos) if tok[\"bio_tag\"] == \"\"]\n",
        "        if len(previous_splitted) == 0 or line_infos[-1] != self.__sent_separator:\n",
        "             previous_splitted.append(len(line_infos))\n",
        "        index = 0\n",
        "        \n",
        "        for end in previous_splitted:\n",
        "            content = \" \".join(tok[\"tok\"] for tok in line_infos[index:end])\n",
        "            if self.use_spacy:\n",
        "                nlp = get_spacy_model(language)\n",
        "                sentences = [x.text for x in nlp(content).sents]\n",
        "            else:\n",
        "                sentences = sent_tokenize(content, language=language)\n",
        "\n",
        "            prev_word_sentence_split = None\n",
        "            for i, sentence in enumerate(sentences):\n",
        "                words = sentence.strip().split(\" \")\n",
        "                if prev_word_sentence_split:\n",
        "                    words[0] = prev_word_sentence_split + words[0]\n",
        "                    prev_word_sentence_split = None\n",
        "                for j, word in enumerate(words):\n",
        "                    if word != line_infos[index][\"tok\"]:\n",
        "                        if j == len(words) - 1 and i < len(sentences) - 1 and \\\n",
        "                           word + sentences[i+1].strip().split(\" \")[0] == line_infos[index][\"tok\"]:\n",
        "                            # Sentece splited in middle of a token and continues next\n",
        "                            prev_word_sentence_split = word\n",
        "                        else:\n",
        "                            # Raise exception\n",
        "                            assert word == line_infos[index][\"tok\"]\n",
        "                    else:\n",
        "                        new_line_infos.append(line_infos[index])\n",
        "                        index += 1\n",
        "\n",
        "                if prev_word_sentence_split is not None:\n",
        "                    continue\n",
        "\n",
        "                # Sentence separator\n",
        "                new_line_infos.append(self.__sent_separator)\n",
        "            assert index == end\n",
        "            index += 1\n",
        "        return new_line_infos\n",
        "        \n",
        "    def __extract_relations(self, relations_string: Optional[str]) -> Optional[List[Tuple[str,int]]]:\n",
        "        \"\"\"\n",
        "        Extracts the relations from `relations_string`\n",
        "        \n",
        "        Example \"-RelA--1-RelB-2-RelA--3-RelC-4\" -> [(\"RelA\", -1), (\"RelB\", 2), (\"RelA\", -3), (\"RelC\", 4)]\n",
        "        \"\"\"\n",
        "\n",
        "        if relations_string is None:\n",
        "            return None\n",
        "        relations = []\n",
        "        for match in self.__relation_regex.finditer(relations_string):\n",
        "            g_dict = match.groupdict()\n",
        "            relations.append((g_dict['relation_tag'], int(g_dict['distance'])))\n",
        "        return relations\n",
        "       \n",
        "    def __get_relations_string(self, relations: List[Tuple[str, int]]) -> str:\n",
        "        \n",
        "        if not relations:\n",
        "            return 'none'\n",
        "        \n",
        "        return \"-\".join(f\"{tag}-{distance}\" for tag, distance in relations)\n",
        "        \n",
        "    def parse(self, content:str, file: Optional[Path] = None, get_tags=False, **kwargs) -> ArgumentationInfo:\n",
        "        \"\"\"\n",
        "        Parse `content` returning DataFrames containing\n",
        "        the argumentative unit and the relation information.\n",
        "        \n",
        "        content: text containing the content to parse\n",
        "        file: Optional, content's original file\n",
        "        get_tags: If a List of tags info is returned instead a dataframe representation\n",
        "        \n",
        "        argumentative_units columns: \n",
        "          - `prop_id` Proposition ID inside the document\n",
        "          - `prop_type` Proposition type\n",
        "          - `prop_init` When the proposition starts in the original text\n",
        "          - `prop_end` When the proposition ends in the original text\n",
        "          - `prop_text` Proposition text\n",
        "          \n",
        "        relations columns:\n",
        "          - `relation_id` Relation ID inside the document\n",
        "          - `relation_type` Relation type\n",
        "          - `prop_id_source` Relation's source proposition id \n",
        "          - `prop_id_target` Relation's target proposition id\n",
        "          \n",
        "        non_argumentative_units columns:\n",
        "          - `prop_init` When the proposition starts in the original text\n",
        "          - `prop_end` When the proposition ends in the original text\n",
        "          - `prop_text` Proposition text\n",
        "          \n",
        "        return: (argumentative_units, relations, non_argumentative_units)\n",
        "        \"\"\"\n",
        "        \n",
        "        content = content.splitlines()\n",
        "        \n",
        "        line_parse = []\n",
        "        \n",
        "        for i,line in enumerate(content):\n",
        "            match = self.annotation_regex.match(line)\n",
        "            \n",
        "            if match:\n",
        "                g_dict = match.groupdict()\n",
        "                g_dict['relations'] = self.__extract_relations(g_dict['relations'])\n",
        "                line_parse.append(g_dict)\n",
        "            elif line == \"\":\n",
        "                line_parse.append(self.__sent_separator)\n",
        "            else:\n",
        "                if file:\n",
        "                    log.warning(f\"Line {i} file {file.name}. Match not found: {line}\")\n",
        "                else:\n",
        "                    log.warning(f\"Line {i}. Match not found: {line}\")\n",
        "\n",
        "        if get_tags:\n",
        "            return line_parse\n",
        "\n",
        "        def extract_proposition(propositions: List[dict], start_index=0):\n",
        "            \"\"\"\n",
        "            Extracts the first proposition in `propositions`.\n",
        "            \n",
        "            Raise: IndexOutOfRange if no proposition is found\n",
        "            \"\"\"\n",
        "            current = start_index\n",
        "            \n",
        "            def extract_language_tag(word):\n",
        "                \"\"\"\n",
        "                Check if the word is annotated with a language tag i.e. [_es, _en, _de]\n",
        "                and return the unannotated word.\n",
        "                \"\"\"\n",
        "                if len(word) > 3 and word[-3] == \"_\":\n",
        "                    return word[:-3]\n",
        "                return word\n",
        "\n",
        "            if propositions[current][\"bio_tag\"] == \"\": # Sentence separator\n",
        "                proposition_text = extract_language_tag(propositions[current][\"tok\"])\n",
        "                current += 1\n",
        "            elif propositions[current][\"bio_tag\"] == \"O\":\n",
        "                proposition_text = extract_language_tag(propositions[current][\"tok\"])\n",
        "                current += 1\n",
        "\n",
        "                # Join all tokens\n",
        "                while current < len(propositions) and propositions[current][\"bio_tag\"] == \"O\":\n",
        "                    proposition_text += \" \" + extract_language_tag(propositions[current][\"tok\"])\n",
        "                    current += 1\n",
        "            else:\n",
        "                if self.bioes and propositions[current][\"bio_tag\"] == \"S\":\n",
        "                    proposition_text = extract_language_tag(propositions[current][\"tok\"])\n",
        "                    current += 1\n",
        "                    return proposition_text, current\n",
        "                \n",
        "                if propositions[current][\"bio_tag\"] != \"B\":\n",
        "                    proposition = propositions[current]\n",
        "                    if file:\n",
        "                        log.warning(f\"File {file.name}. Proposition '{proposition['tok']}' at index {current} doesn't start with a B\")\n",
        "                    else:\n",
        "                        log.warning(f\"Proposition '{proposition['tok']}' at index {current} doesn't start with a B\")\n",
        "                \n",
        "                # Current should be B\n",
        "                proposition_text = extract_language_tag(propositions[current][\"tok\"])\n",
        "                current += 1\n",
        "                \n",
        "                # Join all tokens\n",
        "                while current < len(propositions) and propositions[current][\"bio_tag\"] == \"I\":\n",
        "                    proposition_text += \" \" + extract_language_tag(propositions[current][\"tok\"])\n",
        "                    current += 1\n",
        "                    \n",
        "                if self.bioes:\n",
        "                    proposition = propositions[current]\n",
        "                    if current >= len(propositions) or propositions[current][\"bio_tag\"] != \"E\":\n",
        "                        if file:\n",
        "                            log.warning(f\"File {file.name}. Proposition '{proposition['tok']}' at index {current} doesn't end with an E\")\n",
        "                        else:\n",
        "                            log.warning(f\"Proposition '{proposition['tok']}' at index {current} doesn't end with an E\")\n",
        "                    elif current < len(propositions):\n",
        "                        proposition_text += \" \" + extract_language_tag(propositions[current][\"tok\"])\n",
        "                        current += 1\n",
        "                        \n",
        "            return proposition_text, current\n",
        "        \n",
        "        argumentative_units = {\n",
        "            \"prop_id\": [], \n",
        "            \"prop_type\": [], \n",
        "            \"prop_init\": [], \n",
        "            \"prop_end\": [], \n",
        "            \"prop_text\": [],\n",
        "        }\n",
        "        \n",
        "        non_argumentative_units = {\n",
        "            \"prop_init\": [], \n",
        "            \"prop_end\": [], \n",
        "            \"prop_text\": [],\n",
        "        }\n",
        "        \n",
        "        relations = {\n",
        "            \"relation_id\": [], \n",
        "            \"relation_type\": [], \n",
        "            \"prop_id_source\": [], \n",
        "            \"prop_id_target\": [],            \n",
        "        }\n",
        "\n",
        "        current = 0\n",
        "        accumulative_offset = 0\n",
        "        while current < len(line_parse):\n",
        "            \n",
        "            proposition, current = extract_proposition(line_parse, current)\n",
        "            prop_info = line_parse[current-1] # All annotations of the argument are equal \n",
        "            prop_id = len(argumentative_units['prop_id']) + 1 # 0 is the root node\n",
        "            \n",
        "            if prop_info[\"bio_tag\"] in [\"O\", \"\"]:\n",
        "                non_argumentative_units['prop_init'].append(accumulative_offset)\n",
        "                non_argumentative_units['prop_end'].append(accumulative_offset + len(proposition))\n",
        "                non_argumentative_units['prop_text'].append(proposition)\n",
        "            else:\n",
        "                argumentative_units['prop_id'].append(prop_id)\n",
        "                argumentative_units['prop_type'].append(prop_info[\"prop_type\"])\n",
        "                argumentative_units['prop_init'].append(accumulative_offset)\n",
        "                argumentative_units['prop_end'].append(accumulative_offset + len(proposition))\n",
        "                argumentative_units['prop_text'].append(proposition)\n",
        "\n",
        "                if prop_info[\"relations\"] is not None:\n",
        "                    for relation_tag, distance in prop_info['relations']:\n",
        "                        relations['relation_id'].append(len(relations['relation_id']))\n",
        "                        relations['relation_type'].append(relation_tag)\n",
        "                        relations['prop_id_source'].append(prop_id)\n",
        "                        relations['prop_id_target'].append(prop_id + int(distance))\n",
        "            \n",
        "            accumulative_offset += len(proposition)\n",
        "            if prop_info[\"bio_tag\"] != \"\":\n",
        "                accumulative_offset += 1 # Extra separator when rebuilding text\n",
        "        \n",
        "        return pd.DataFrame(argumentative_units), pd.DataFrame(relations), pd.DataFrame(non_argumentative_units)\n",
        "        \n",
        "    def fix_annotations(self, annotations: List[ConllTagInfo]) -> List[ConllTagInfo]:\n",
        "        \"\"\"\n",
        "        Fix posible errors found in `annotations` returning a new list without them.\n",
        "        \n",
        "        annotations: Original list of conll annotations\n",
        "        \"\"\"\n",
        "        fixed_annotations = []\n",
        "        for i, annotation in enumerate(annotations):\n",
        "            # The next annotation can go after the previous annotation\n",
        "            # But a sentence separator is in the middle\n",
        "            if annotation == self.__sent_separator \\\n",
        "               and i < len(annotations) - 1 \\\n",
        "               and i > 0 \\\n",
        "               and annotations[i+1][\"bio_tag\"] == \"I\" \\\n",
        "               and annotations[i-1][\"bio_tag\"] in [\"B\", \"I\"] \\\n",
        "               and annotations[i-1][\"prop_type\"] == annotations[i+1][\"prop_type\"] \\\n",
        "               and annotations[i-1][\"relations\"] == annotations[i+1][\"relations\"]:\n",
        "                # Skip sentence separator\n",
        "                continue\n",
        "            fixed_annotations.append(annotation)\n",
        "        return fixed_annotations\n",
        "\n",
        "    def from_dataframes(self, dataframes: Dict[str, ArgumentationInfo], source_language=\"english\", get_tags=False, exact_text=True, split_sentences=True, **kwargs) -> Dict[str, Union[AnnotatedRawTextInfo, Tuple[List[ConllTagInfo], str]]]:\n",
        "        \"\"\"\n",
        "        Creates a CONLL annotated corpus representing the received DataFrames. \n",
        "        \n",
        "        dataframes: The result from calling a parse function in any Parser class\n",
        "        the keys aren't important, so a mock key can be passed.\n",
        "        source_language: Language for tokenization process\n",
        "        get_tags: If true, returns the tags instead of the annotated text\n",
        "        exact_text: If true, returns the exact text representation else will \n",
        "        be returned the tokens separated by whitespaces\n",
        "        \n",
        "        returns: CONLL annotated string or CONLL annotations, Raw text\n",
        "        \"\"\"\n",
        "        \n",
        "        results = {}\n",
        "        default_gap = \" \"\n",
        "                \n",
        "        for file_path_str, (argumentative_units, relations, non_argumentative_units) in dataframes.items():\n",
        "\n",
        "            tags_info = []\n",
        "            all_units = pd.concat([argumentative_units, non_argumentative_units], sort=True)\n",
        "            all_units.sort_values(by=\"prop_init\", inplace=True)\n",
        "            all_units = all_units.reindex(columns=[\"prop_id\", \"prop_type\", \"prop_init\", \"prop_end\", \"prop_text\"])\n",
        "            all_units['prop_init'] = all_units['prop_init'].apply(np.int64)\n",
        "            all_units['prop_end'] = all_units['prop_end'].apply(np.int64)\n",
        "            max_length = all_units[\"prop_end\"].max()\n",
        "            \n",
        "            text = default_gap*max_length if exact_text else \"\"\n",
        "            \n",
        "            for index, (prop_id, prop_type, prop_init, prop_end, prop_text) in all_units.iterrows():\n",
        "                prop_id = int(prop_id) if pd.notna(prop_id) else prop_id\n",
        "                if self.use_spacy:\n",
        "                    nlp = get_spacy_model(source_language)\n",
        "                    doc = nlp(prop_text)\n",
        "                    prop_tokens = [x.text for x in doc if \"\\n\" not in x.text and x.text.strip()]\n",
        "                else:\n",
        "                    prop_tokens = word_tokenize(prop_text, language=source_language)\n",
        "                \n",
        "                if exact_text:\n",
        "                    text = text[:prop_init] + prop_text + text[prop_end:]\n",
        "                else:\n",
        "                    text += default_gap.join(prop_tokens) + default_gap\n",
        "                \n",
        "                if pd.notna(prop_type):\n",
        "                    # It's the begining of a proposition\n",
        "                    for i,tok in enumerate(prop_tokens):\n",
        "                        current_relations = []\n",
        "                        if i == 0:\n",
        "                            if len(prop_tokens) == 1 and self.bioes:\n",
        "                                bio_tag = \"S\"\n",
        "                            else:    \n",
        "                                bio_tag = \"B\"  \n",
        "                        elif i == len(prop_tokens) - 1 and self.bioes:\n",
        "                            bio_tag = \"E\"\n",
        "                        else:\n",
        "                            bio_tag = \"I\"\n",
        "                        relation = relations[relations[\"prop_id_source\"] == prop_id]\n",
        "                        for _, relation in relation.iterrows():\n",
        "                            relation_type = relation[\"relation_type\"]\n",
        "                            relation_distance = relation[\"prop_id_target\"] - relation[\"prop_id_source\"]\n",
        "                            current_relations.append((relation_type, relation_distance))\n",
        "                        \n",
        "                        tags_info.append({\n",
        "                                \"tok\": tok,\n",
        "                                \"bio_tag\": bio_tag,\n",
        "                                \"prop_type\": prop_type,\n",
        "                                \"relations\": current_relations,\n",
        "                                \"relations_string\": self.__get_relations_string(current_relations)\n",
        "                        })\n",
        "                        tags_info[-1][\"full_tag\"] = self.TAG_FORMAT.format_map(tags_info[-1]).replace(\"-none\", \"\")\n",
        "\n",
        "                else:\n",
        "                    if all(x == \"\\n\" for x in prop_text): # Sentence and Paragraph separators\n",
        "                        for x in prop_text:\n",
        "                            tags_info.append(self.__sent_separator)\n",
        "                    else:\n",
        "                        # Out of proposition\n",
        "                        for tok in prop_tokens:\n",
        "                            # Fill the gap with O until the proposition is found\n",
        "                            tags_info.append({\n",
        "                                \"tok\": tok,\n",
        "                                \"bio_tag\": \"O\",\n",
        "                                \"prop_type\": \"none\",\n",
        "                                \"relations\": \"none\",\n",
        "                                \"relations_string\": \"none\",\n",
        "                            })\n",
        "                            tags_info[-1][\"full_tag\"] = \"O\"\n",
        "                \n",
        "            if split_sentences:\n",
        "                tags_info = self.__split_sentences(tags_info, source_language)\n",
        "            \n",
        "            tags_info = self.fix_annotations(tags_info)\n",
        "            \n",
        "            if get_tags:\n",
        "                results[file_path_str] = tags_info, text\n",
        "            else:\n",
        "                result = self.get_conll_text_from_annotation_dicts(tags_info)\n",
        "                results[file_path_str] = result, text\n",
        "        \n",
        "        return results\n",
        "\n",
        "    def get_conll_text_from_annotation(self, annotations: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Maps the anotation to its associated conll text representation.\n",
        "        \n",
        "        annotations: List containig the dictionary that holds the information about the tag\n",
        "        \n",
        "        returns: The annotated conll text representation\n",
        "        \"\"\"\n",
        "        text = \"\"\n",
        "        for annotation in annotations:\n",
        "            if annotation == \"\":\n",
        "                to_write = \"\\n\"\n",
        "            else:\n",
        "                match = self.annotation_regex.match(annotation)\n",
        "                assert match\n",
        "                annotation = match.groupdict()\n",
        "                to_write = self.ANNOTATION_FORMAT.format_map(annotation)\n",
        "                to_write = to_write.replace(\"-none\", \"\") # Remove unnecesary labels\n",
        "                to_write = to_write.replace(\"-None\", \"\") # Remove unnecesary labels\n",
        "            text += to_write\n",
        "        return text\n",
        "\n",
        "    def get_text_from_annotation(self, annotations: List[ConllTagInfo]) -> str:\n",
        "        \"\"\"\n",
        "        Returns the text associated with `annotations`. All tokens are placed in\n",
        "        a single line separated by a whitespace. \n",
        "        \n",
        "        annotations: List containig the dictionary that holds the information about the tag\n",
        "        \n",
        "        returns: The text representation\n",
        "        \"\"\"\n",
        "        return \" \".join([x[\"tok\"] for x in annotations])\n",
        "    \n",
        "    def get_conll_text_from_annotation_dicts(self, annotations: List[ConllTagInfo]) -> str:\n",
        "        \"\"\"\n",
        "        Returns the conll text associated with `annotations`.\n",
        "        \n",
        "        annotations: List containig the dictionary that holds the information about the tag\n",
        "        \n",
        "        returns: The annotated conll text representation\n",
        "        \"\"\"\n",
        "        # Create text\n",
        "        result = \"\"\n",
        "        for tag_info in annotations:\n",
        "            if tag_info == self.__sent_separator:\n",
        "                to_write = \"\\n\"\n",
        "            else:\n",
        "                to_write = self.ANNOTATION_FORMAT.format_map(tag_info)\n",
        "                to_write = to_write.replace(\"-none\", \"\") # Remove unnecesary labels\n",
        "            result += to_write\n",
        "        return result\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ub9nE9U3XW6",
        "outputId": "24a47b63-0df7-44e7-da65-51e5b8897b3c"
      },
      "outputs": [],
      "source": [
        "conll_content = []\n",
        "global_processed_data_path = Path(\"/content/argument-mining\", processed_data_path)\n",
        "\n",
        "info = ConllParser().parse_dir(global_processed_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "Ds_PTHqBrbBL",
        "outputId": "b905102e-3729-4287-e23c-1ccfafd62d10"
      },
      "outputs": [],
      "source": [
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for file in info:\n",
        "  print(\"Argumenttative structure of\", Path(file).name)\n",
        "  print()\n",
        "  arguments, relations, non_arguments = info[file]\n",
        "\n",
        "  nodes = {}\n",
        "  relations_list = []\n",
        "  arg_types = []\n",
        "  rel_types = []\n",
        "  \n",
        "  print(\"ARGUMENTS:\")\n",
        "  print()\n",
        "  for i,a in arguments.iterrows():\n",
        "    nodes[a['prop_id']] = a['prop_text']\n",
        "    arg_types.append(a['prop_type'])\n",
        "    print(f\"{i}: {a['prop_type']}:  {a['prop_text']}\")\n",
        "  print()\n",
        "  print(\"RELATIONS:\")\n",
        "  print()\n",
        "  for i,a in relations.iterrows():\n",
        "    # relations_list.append((nodes[a['prop_id_source']], nodes[a['prop_id_target']]))\n",
        "    rel_types.append(a['relation_type'])\n",
        "    relations_list.append((a['prop_id_source'], a['prop_id_target']))\n",
        "    print(f\"{i}:  {a['prop_id_source']}-{a['relation_type']}->{a['prop_id_target']}\")\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  colors = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"orange\"]\n",
        "\n",
        "  arg_colors = list(set(arg_types))\n",
        "  arg_colors = { arg_type: colors[i % len(colors)] for i, arg_type in enumerate(arg_colors, 3) }\n",
        "  rel_colors = list(set(rel_types))\n",
        "  rel_colors = { rel_type: colors[i % len(colors)] for i, rel_type in enumerate(rel_colors) }\n",
        "\n",
        "  G = nx.DiGraph()\n",
        "  G.add_nodes_from(nodes)\n",
        "  G.add_edges_from(relations_list)\n",
        "\n",
        "  node_color = [arg_colors[arg] for arg in arg_types]\n",
        "  edge_color = [rel_colors[rel] for rel in rel_types]\n",
        "  options = {\n",
        "      \"font_size\": 36,\n",
        "      \"node_size\": 3000,\n",
        "      \"node_color\": \"white\",\n",
        "      \"edge_color\": edge_color,\n",
        "      \"linewidths\": 5,\n",
        "      \"width\": 5,\n",
        "      \"edgecolors\": node_color, \n",
        "  }\n",
        "\n",
        "  nx.draw_networkx(G, **options)\n",
        "\n",
        "  # Set margins for the axes so that nodes aren't clipped\n",
        "  ax = plt.gca()\n",
        "  ax.margins(0.20)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxrMO_H4J4Wa"
      },
      "source": [
        "# Export models and files processed\n",
        "\n",
        "This section will create zip files with the contet you wish to export. Once created you can download the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lC4gBhZ2JzGg"
      },
      "outputs": [],
      "source": [
        "#@title Export options: { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ## Export options\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Segmenter path\n",
        "export_segmenter = True #@param {type: \"boolean\"}\n",
        "export_link_predictor = True #@param {type: \"boolean\"}\n",
        "export_processed_files = True #@param {type: \"boolean\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_p6mNIuLohS",
        "outputId": "b481d63f-cf86-4675-9ab2-de5d203e5085"
      },
      "outputs": [],
      "source": [
        "segmenter_target = Path(f\"/content/argument-mining/data/segmenter_corpus/\")\n",
        "link_prediction_target = Path(f\"/content/argument-mining/data/link_prediction/\")\n",
        "processed_data_target = Path(\"argument-mining\") / processed_data_path\n",
        "\n",
        "processed_data_path_str = str(processed_data_target.resolve())\n",
        "segmenter_to_save_path_str = str(segmenter_target.resolve())\n",
        "link_predictor_to_save_path_str = str(link_prediction_target.resolve())\n",
        "\n",
        "processed_name = processed_data_path.name\n",
        "segmenter_name = segmenter_to_save_path.name\n",
        "link_predictor_name = link_predictor_to_save_path.name\n",
        "\n",
        "if export_segmenter:\n",
        "  !zip -r \"$segmenter_name segmenter.zip\" $segmenter_to_save_path_str\n",
        "if export_link_predictor:\n",
        "  !zip -r \"$link_predictor_name link predictor.zip\" $link_predictor_to_save_path_str\n",
        "if export_processed_files:\n",
        "  !zip -r \"$processed_name data.zip\" $processed_data_path_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnPHv94ERGVG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QBjBKCWt07YX",
        "wy8Zz3vA5sP8",
        "WaeY02zD1qdM",
        "FoDbJK_x2Swl",
        "8Y4r8ZTo27ba",
        "Hh7bijWE3SMF",
        "RxrMO_H4J4Wa"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
