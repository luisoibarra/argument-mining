{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from corpus_parser.brat_parser import BratParser\n",
    "from corpus_parser.conll_parser import ConllParser\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "BASE_DATA = Path(\"data\")\n",
    "PROCESS_TAG = \"selected_response_responded_granma_letters\"\n",
    "\n",
    "Indexable = Union[List, str]\n",
    "\n",
    "def jaccard_sim(s1, s2):\n",
    "    \"\"\"\n",
    "    Jaccard similarity\n",
    "    \"\"\"\n",
    "    a = set(s1)\n",
    "    b = set(s2)\n",
    "    return len(a.intersection(b))/len(a.union(b))\n",
    "\n",
    "def stredit(s1:Indexable, s2:Indexable):\n",
    "    \"\"\"\n",
    "    Levenshtein distance\n",
    "    \"\"\"\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "    \n",
    "    table = [[0 for _ in range(len2 + 1)] for _ in range(len1 + 1)]\n",
    "    for i in range(1, len1 + 1):\n",
    "        table[i][0] = i\n",
    "    for j in range(1, len2 + 1):\n",
    "        table[0][j] = j\n",
    "\n",
    "\n",
    "    for i in range(1, len1+1):\n",
    "        for j in range(1,len2+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1\n",
    "            \n",
    "            copy_sub_value = table[i-1][j-1] + d\n",
    "            insert_value = table[i][j-1]+1\n",
    "            delete_value = table[i-1][j]+1\n",
    "\n",
    "            table[i][j] = value = min(copy_sub_value,\n",
    "                              delete_value,\n",
    "                              insert_value)\n",
    "\n",
    "    return table[len1][len2]\n",
    "\n",
    "\n",
    "def data_augmentation_metrics(corpus_path: Path):\n",
    "    \n",
    "    def process_dir(path: Path):\n",
    "        originals = {}\n",
    "        augmented = {}\n",
    "        \n",
    "        info = {\n",
    "            \"file\": [],\n",
    "            \"augmented_language\": [],\n",
    "            \"original_word_len\": [],\n",
    "            \"augmented_word_len\": [],\n",
    "            \"edit_word_distance\": [],\n",
    "            \"jaccard_similarity\": [],\n",
    "        }\n",
    "        \n",
    "        for file in path.iterdir():\n",
    "            if file.is_file() and file.suffix == \".conll\":\n",
    "                if all(x in file.name for x in [\"from\", \"augmented\"]):\n",
    "                    # Is aguemnted\n",
    "                    name = file.name.split(\"_\")[3].split(\".\")[0]\n",
    "                    if name in augmented:\n",
    "                        augmented[name].append(file)\n",
    "                    else:\n",
    "                        augmented[name] = [file]\n",
    "                else:\n",
    "                    # Is original\n",
    "                    name = file.name.split(\".\")[0]\n",
    "                    if name in originals:\n",
    "                        print(\"WARNING: Repeated original file:\", name)\n",
    "                    else:\n",
    "                        originals[name] = file\n",
    "        \n",
    "        for orig in originals:\n",
    "            orig_file = originals[orig]\n",
    "            orig_text = orig_file.read_text()\n",
    "            augmented_files = augmented[orig] if orig in augmented else []\n",
    "            for augmented_file in augmented_files:\n",
    "                augm_text = augmented_file.read_text()\n",
    "                \n",
    "                language = augmented_file.name.split(\"_\")[1]\n",
    "                \n",
    "                orig_tokens = [x.split(\"\\t\")[0] for x in orig_text.split(\"\\n\") if x]\n",
    "                augm_tokens = [x.split(\"\\t\")[0] for x in augm_text.split(\"\\n\") if x]\n",
    "                \n",
    "                orig_leng = len(orig_tokens)\n",
    "                augm_leng = len(augm_tokens)\n",
    "                \n",
    "                edit = stredit(orig_tokens, augm_tokens)\n",
    "                jaccard = jaccard_sim(orig_tokens, augm_tokens)\n",
    "                \n",
    "                info[\"file\"].append(orig)\n",
    "                info[\"augmented_language\"].append(language)\n",
    "                info[\"original_word_len\"].append(orig_leng)\n",
    "                info[\"augmented_word_len\"].append(augm_leng)\n",
    "                info[\"edit_word_distance\"].append(edit)\n",
    "                info[\"jaccard_similarity\"].append(jaccard)\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    data = None\n",
    "    for path in corpus_path.iterdir():\n",
    "        result = process_dir(path)\n",
    "        if data:\n",
    "            for key, value in result.items():\n",
    "                data[key].extend(value)\n",
    "        else:\n",
    "            data = result\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def get_corpus_info(path: Path, is_corpus: bool = False):\n",
    "    \n",
    "    if is_corpus:\n",
    "        tags_info = ConllParser(bioes=False).parse_dir(path / \"dev\", get_tags=True)\n",
    "        file_info = ConllParser(bioes=False).parse_dir(path / \"dev\")\n",
    "        \n",
    "        for t in [\"test\", \"train\"]:\n",
    "            tags_info_ = ConllParser(bioes=False).parse_dir(path / t, get_tags=True)\n",
    "            file_info_ = ConllParser(bioes=False).parse_dir(path / t)\n",
    "            \n",
    "            tags_info.update(tags_info_)\n",
    "            file_info.update(file_info_)\n",
    "    else:\n",
    "        tags_info = ConllParser(bioes=True).parse_dir(path, get_tags=True)\n",
    "        file_info = ConllParser(bioes=True).parse_dir(path)\n",
    "    \n",
    "    statistic = {\n",
    "        \"key\": [],\n",
    "        \"token_amount\": [],\n",
    "        \"inside_token_amount\": [],\n",
    "        \"relation_amount\": [],\n",
    "        \"argumentative_units_amount\": [],\n",
    "        \"non_argumentative_units_amount\": [],\n",
    "    }\n",
    "\n",
    "    arg_type_counter = Counter([x for _, (df, _, _) in file_info.items() for x in df['prop_type']])\n",
    "    relation_type_counter = Counter([x for _, (_, df, _) in file_info.items() for x in df['relation_type']])\n",
    "    \n",
    "    for key in tags_info:\n",
    "        token_amount = 0\n",
    "        inside_token_amount = 0\n",
    "        for tag_info in tags_info[key]:\n",
    "            token_amount += 1\n",
    "            inside_token_amount += 0 if tag_info['bio_tag'] == \"O\" else 1\n",
    "\n",
    "        arg, rel, non_arg = file_info[key]\n",
    "\n",
    "        arg_amount = len(arg)\n",
    "        non_arg_amount = len(non_arg[non_arg['prop_text'] != \"\\n\"])\n",
    "        rel_amount = len(rel)\n",
    "\n",
    "        statistic['key'].append(key)\n",
    "        statistic['token_amount'].append(token_amount)\n",
    "        statistic['inside_token_amount'].append(inside_token_amount)\n",
    "        statistic['relation_amount'].append(rel_amount)\n",
    "        statistic['argumentative_units_amount'].append(arg_amount)\n",
    "        statistic['non_argumentative_units_amount'].append(non_arg_amount)\n",
    "\n",
    "    statistic_df = pd.DataFrame(statistic)    \n",
    "\n",
    "    print(path)\n",
    "    print(arg_type_counter)\n",
    "    print(relation_type_counter)\n",
    "    \n",
    "    return statistic_df\n",
    "\n",
    "def plot_histogram(serie, title: str, bin_factor: int = 1, xlabel: str = None, ylabel: str = None):\n",
    "    bins = sorted(set(serie))\n",
    "    serie.hist(bins=len(bins)//bin_factor)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "\n",
    "def plot_increasing_values(serie: pd.Series, title: str, xlabel: str = None, ylabel: str = None):\n",
    "    serie = serie.sort_values()\n",
    "    plt.plot([i for i in range(len(serie))], serie)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "def plot_and_save(save = None):\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    plt.show()\n",
    "        \n",
    "def plot_results(statistic_dfs: dict, normalize_document_amount: bool=True):\n",
    "    \n",
    "    statistic_dfs = statistic_dfs.copy()\n",
    "    \n",
    "    keys = list(statistic_dfs.keys())\n",
    "    minim_amount = min([len(x) for x in statistic_dfs.values()])\n",
    "    \n",
    "    if normalize_document_amount:\n",
    "        # Remove rows random from datasets until all have the same length\n",
    "        for key, value in statistic_dfs.items():\n",
    "            statistic_dfs[key] = value.sample(minim_amount)\n",
    "    \n",
    "    for key, statistic_df in statistic_dfs.items():\n",
    "        plot_histogram(statistic_df['token_amount'], f\"Cantidad de tokens\", bin_factor=5, xlabel=\"Cantidad de tokens\", ylabel=\"Cantidad de documentos\")\n",
    "    plt.legend(keys)\n",
    "    plot_and_save()\n",
    "   \n",
    "    print(\"Percentage of tokens annotated with BIES\")\n",
    "    for key, statistic_df in statistic_dfs.items():\n",
    "        inside_percentage = (statistic_df['inside_token_amount'] / statistic_df['token_amount'])\n",
    "        plot_increasing_values(inside_percentage, f\"Porciento tokens argumentativos\", xlabel=\"√çndice de documentos\", ylabel=\"Porciento de tokens argumentativos\")\n",
    "    plt.legend(keys)\n",
    "    plot_and_save()\n",
    "        \n",
    "    print(\"Percentage of argumentative units\")\n",
    "    for key, statistic_df in statistic_dfs.items():\n",
    "        total_components = statistic_df['argumentative_units_amount'] + statistic_df['non_argumentative_units_amount']\n",
    "        argumentative_percentage = statistic_df['argumentative_units_amount'] / total_components\n",
    "        plot_increasing_values(argumentative_percentage, f\"Porciento componentes argumentativas\", xlabel=\"√çndice de documentos\", ylabel=\"Porciento de componentes argumentativas\")\n",
    "    plt.legend(keys)\n",
    "    plot_and_save()\n",
    "\n",
    "    print(\"Relations per argumentative component\")\n",
    "    for key, statistic_df in statistic_dfs.items():\n",
    "        normalized_relation = statistic_df['relation_amount'] / statistic_df['argumentative_units_amount']\n",
    "        plot_histogram(normalized_relation, f\"Relaciones\", bin_factor=3, xlabel=\"Cantidad de relaciones normalizada\", ylabel=\"Cantidad de documentos\")\n",
    "    plt.legend(keys)\n",
    "    plot_and_save()\n",
    "\n",
    "\n",
    "def print_tabular_info(statistics_df: dict):\n",
    "    \n",
    "    data = {\n",
    "        \"% promedio de UDA\": [],\n",
    "        \"% promedio de tokens argumentatvos\": [],\n",
    "        \"Promedio de relaciones por UDA\": [],\n",
    "    }\n",
    "    index = list(statistics_df.keys())\n",
    "    \n",
    "    for key, table in statistics_df.items():\n",
    "        total_components = table['argumentative_units_amount'] + table['non_argumentative_units_amount']\n",
    "        argumentative_percentage = table['argumentative_units_amount'] / total_components\n",
    "        argumentative_percentage = argumentative_percentage.describe()\n",
    "        data[\"% promedio de UDA\"].append(argumentative_percentage[\"mean\"])\n",
    "        \n",
    "        token_argumentative_percentage = table['inside_token_amount'] / table[\"token_amount\"]\n",
    "        token_argumentative_percentage = token_argumentative_percentage.describe()\n",
    "        data[\"% promedio de tokens argumentatvos\"].append(token_argumentative_percentage[\"mean\"])\n",
    "        \n",
    "        normalized_relation = table['relation_amount'] / table['argumentative_units_amount']\n",
    "        normalized_relation = normalized_relation.describe()\n",
    "        data[\"Promedio de relaciones por UDA\"].append(normalized_relation[\"mean\"])\n",
    "    \n",
    "    return pd.DataFrame(data, index=index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_TO_MEASURE = [\"cdcp\", \"abstrct\", \"persuasive_essays_paragraph_all_linked\"]\n",
    "\n",
    "data_augmentation_dict = { key: data_augmentation_metrics(BASE_DATA / \"parsed_to_conll\" / key) for key in CORPUS_TO_MEASURE }\n",
    "\n",
    "for key, df in data_augmentation_dict.items():\n",
    "    df['word_len_relation'] = df[\"original_word_len\"] / df[\"augmented_word_len\"]\n",
    "    # print(key)\n",
    "    # display(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_info_dict = {f\"{key}_{PROCESS_TAG}\": get_corpus_info(BASE_DATA / 'link_prediction_processed' / key / PROCESS_TAG) for key in CORPUS_TO_MEASURE }\n",
    "corpus_info_dict.update({f\"{key}_corpus\": get_corpus_info(BASE_DATA / 'parsed_to_conll' / key, is_corpus=True) for key in CORPUS_TO_MEASURE })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Similarity between corpus and processed\n",
    "for key in CORPUS_TO_MEASURE:\n",
    "    plot_results({x: corpus_info_dict[x] for x in [f\"{key}_{PROCESS_TAG}\", f\"{key}_corpus\"]})\n",
    "\n",
    "# Similarity between processed\n",
    "plot_results({x: corpus_info_dict[x] for x in [f\"{key}_{PROCESS_TAG}\" for key in CORPUS_TO_MEASURE]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular info\n",
    "\n",
    "\n",
    "info = print_tabular_info({x: corpus_info_dict[x] for x in corpus_info_dict})\n",
    "display(info)\n",
    "\n",
    "for table_name in corpus_info_dict:\n",
    "    print(table_name)\n",
    "    display(corpus_info_dict[table_name].describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75a89b5e31e4c74bfba9f9e3683726b0b64c86df91deff717b657bf4e96fae57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
